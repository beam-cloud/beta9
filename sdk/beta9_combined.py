# Combined Beta9 SDK Files
# This file contains all Python files from src/beta9
# Generated by combine_beta9_files.py


================================================================================
# File: __init__.py
# Path: src/beta9/__init__.py
================================================================================

from . import env, schema
from .abstractions import experimental, integrations
from .abstractions.base.container import Container
from .abstractions.endpoint import ASGI as asgi
from .abstractions.endpoint import Endpoint as endpoint
from .abstractions.endpoint import RealtimeASGI as realtime
from .abstractions.experimental.bot.bot import Bot, BotEventType, BotLocation
from .abstractions.experimental.bot.types import BotContext
from .abstractions.function import Function as function
from .abstractions.function import Schedule as schedule
from .abstractions.image import Image
from .abstractions.map import Map
from .abstractions.output import Output
from .abstractions.pod import Pod
from .abstractions.queue import SimpleQueue as Queue
from .abstractions.sandbox import Sandbox
from .abstractions.taskqueue import TaskQueue as task_queue
from .abstractions.volume import CloudBucket, CloudBucketConfig, Volume
from .client.client import Client
from .client.deployment import Deployment
from .client.task import Task
from .type import (
    GpuType,
    PricingPolicy,
    PricingPolicyCostModel,
    PythonVersion,
    QueueDepthAutoscaler,
    TaskPolicy,
)

__all__ = [
    "Map",
    "Image",
    "Queue",
    "Volume",
    "CloudBucket",
    "CloudBucketConfig",
    "task_queue",
    "function",
    "endpoint",
    "asgi",
    "realtime",
    "Container",
    "env",
    "GpuType",
    "PythonVersion",
    "Output",
    "QueueDepthAutoscaler",
    "experimental",
    "integrations",
    "schedule",
    "TaskPolicy",
    "Bot",
    "BotLocation",
    "BotEventType",
    "BotContext",
    "Pod",
    "PricingPolicy",
    "PricingPolicyCostModel",
    "Client",
    "Task",
    "Deployment",
    "schema",
    "Sandbox",
]

================================================================================
# File: __main__.py
# Path: src/beta9/__main__.py
================================================================================

from .cli import main

main.start()

================================================================================
# File: abstractions/__init__.py
# Path: src/beta9/abstractions/__init__.py
================================================================================


================================================================================
# File: abstractions/base/__init__.py
# Path: src/beta9/abstractions/base/__init__.py
================================================================================

import os

os.environ["GRPC_VERBOSITY"] = os.getenv("GRPC_VERBOSITY") or "NONE"

import sys
from abc import ABC
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

from ... import config
from ...channel import Channel
from ...channel import get_channel as _get_channel
from ...config import ConfigContext, get_config_context, set_settings

# Global channel
_channel: Optional[Channel] = None


def set_channel(
    channel: Optional[Channel] = None,
    context: Optional[ConfigContext] = None,
) -> None:
    """
    Sets the channel globally for the SDK.

    Use this before importing any abstraction to control which
    gateway to connect to. When you provide a channel, it should already be
    authenticated. When you provide a context, this will authenticate for you.
    If neithe are provided, this uses the default context and will create a
    channel. If there is no default context (or config file), then we will
    prompt the user for it.

    Args:
        channel: gRPC channel. Defaults to None.
        context: Config context that defines the channel credentials. Defaults to None.
    """
    global _channel

    if channel:
        _channel = channel
        return

    if context:
        _channel = _get_channel(context)
        return

    context = get_config_context()
    _channel = _get_channel(context)


def get_channel() -> Channel:
    global _channel

    if not _channel:
        set_channel()

    return _channel  # type: ignore


class BaseAbstraction(ABC):
    @property
    def channel(self) -> Channel:
        return get_channel()

    def __init_subclass__(cls, /, **kwargs):
        """
        Dynamically load settings depending on if this library is being used
        by beta9 or beam. This is done by inspecting the first frame loaded
        onto the stack.
        """

        if "beam" in sys.modules:

            @dataclass
            class SDKSettings(config.SDKSettings):
                realtime_host: str = os.getenv("REALTIME_HOST", "wss://rt.beam.cloud")

            settings = SDKSettings(
                name="Beam",
                api_host=os.getenv("API_HOST", "api.beam.cloud"),
                gateway_host=os.getenv("GATEWAY_HOST", "gateway.beam.cloud"),
                gateway_port=int(os.getenv("GATEWAY_PORT", 443)),
                config_path=Path("~/.beam/config.ini").expanduser(),
                use_defaults_in_prompt=True,
            )
            set_settings(settings)

        super().__init_subclass__(**kwargs)

================================================================================
# File: abstractions/base/container.py
# Path: src/beta9/abstractions/base/container.py
================================================================================

import os
import time
from queue import Empty, Queue
from typing import Callable, Optional

from watchdog.observers import Observer

from ... import terminal
from ...clients.gateway import (
    AttachToContainerRequest,
    ContainerStreamMessage,
    GatewayServiceStub,
    SyncContainerWorkspaceOperation,
    SyncContainerWorkspaceRequest,
)
from ...sync import SyncEventHandler
from .runner import BaseAbstraction

DEFAULT_SYNC_INTERVAL = 0.1


class Container(BaseAbstraction):
    def __init__(
        self,
        container_id: str,
    ) -> None:
        super().__init__()
        self.gateway_stub = GatewayServiceStub(self.channel)
        self.container_id = container_id

    def attach(self, *, container_id: str, sync_dir: Optional[str] = None, hide_logs: bool = False):
        """
        Attach to a running container and stream messages back and forth. Also, optionally sync a directory to the container workspace.
        """

        terminal.header(f"Connecting to {container_id}...")

        def _container_stream_generator():
            yield ContainerStreamMessage(
                attach_request=AttachToContainerRequest(container_id=container_id)
            )

            if sync_dir:
                yield from self._sync_dir_to_workspace(dir=sync_dir, container_id=container_id)
            else:
                while True:
                    time.sleep(DEFAULT_SYNC_INTERVAL)
                    yield ContainerStreamMessage()

        # Connect to the remote container and stream messages back and forth
        stream = self.gateway_stub.attach_to_container(_container_stream_generator())

        r = None
        for r in stream:
            if r.output and not hide_logs:
                terminal.detail(r.output, end="")

            if r.done or r.exit_code != 0:
                break

        if r is None:
            return terminal.error("Container failed ❌")

        if not r.done:
            return terminal.error(f"\n{r.output} ❌")

        if not hide_logs:
            terminal.header(r.output)

    def _sync_dir_to_workspace(
        self, *, dir: str, container_id: str, on_event: Optional[Callable] = None
    ):
        file_update_queue = Queue()
        event_handler = SyncEventHandler(file_update_queue)

        observer = Observer()
        observer.schedule(event_handler, dir, recursive=True)
        observer.start()

        terminal.header(f"Watching '{dir}' for changes...")
        while True:
            try:
                operation, path, new_path = file_update_queue.get_nowait()

                if on_event:
                    on_event(operation, path, new_path)

                req = SyncContainerWorkspaceRequest(
                    container_id=container_id,
                    path=os.path.relpath(path, start=dir),
                    is_dir=os.path.isdir(path),
                    op=operation,
                )

                if operation == SyncContainerWorkspaceOperation.WRITE:
                    if not req.is_dir:
                        with open(path, "rb") as f:
                            req.data = f.read()

                elif operation == SyncContainerWorkspaceOperation.DELETE:
                    pass

                elif operation == SyncContainerWorkspaceOperation.MOVED:
                    req.new_path = os.path.relpath(new_path, start=dir)

                yield ContainerStreamMessage(sync_container_workspace=req)

                file_update_queue.task_done()
            except Empty:
                time.sleep(DEFAULT_SYNC_INTERVAL)
            except BaseException as e:
                terminal.warn(str(e))

================================================================================
# File: abstractions/base/runner.py
# Path: src/beta9/abstractions/base/runner.py
================================================================================

import inspect
import json
import os
import threading
from typing import Callable, Dict, List, Optional, Union

import cloudpickle

from ... import terminal
from ...abstractions.base import BaseAbstraction
from ...abstractions.image import Image, ImageBuildResult
from ...abstractions.volume import Volume
from ...clients.gateway import Autoscaler as AutoscalerProto
from ...clients.gateway import (
    GatewayServiceStub,
    GetOrCreateStubRequest,
    GetOrCreateStubResponse,
    GetUrlRequest,
    GetUrlResponse,
    SecretVar,
)
from ...clients.gateway import (
    Schema as SchemaProto,
)
from ...clients.gateway import (
    SchemaField as SchemaFieldProto,
)
from ...clients.gateway import TaskPolicy as TaskPolicyProto
from ...clients.shell import ShellServiceStub
from ...clients.types import PricingPolicy as PricingPolicyProto
from ...config import ConfigContext, SDKSettings, get_config_context, get_settings
from ...env import called_on_import, is_notebook_env
from ...schema import Schema
from ...sync import FileSyncer
from ...type import (
    _AUTOSCALER_TYPES,
    Autoscaler,
    GpuType,
    GpuTypeAlias,
    PricingPolicy,
    QueueDepthAutoscaler,
    TaskPolicy,
)
from ...utils import TempFile

CONTAINER_STUB_TYPE = "container"
FUNCTION_STUB_TYPE = "function"
TASKQUEUE_STUB_TYPE = "taskqueue"
ENDPOINT_STUB_TYPE = "endpoint"
ASGI_STUB_TYPE = "asgi"
SCHEDULE_STUB_TYPE = "schedule"
BOT_STUB_TYPE = "bot"
SHELL_STUB_TYPE = "shell"

TASKQUEUE_DEPLOYMENT_STUB_TYPE = "taskqueue/deployment"
ENDPOINT_DEPLOYMENT_STUB_TYPE = "endpoint/deployment"
ASGI_DEPLOYMENT_STUB_TYPE = "asgi/deployment"
FUNCTION_DEPLOYMENT_STUB_TYPE = "function/deployment"
SCHEDULE_DEPLOYMENT_STUB_TYPE = "schedule/deployment"
BOT_DEPLOYMENT_STUB_TYPE = "bot/deployment"

TASKQUEUE_SERVE_STUB_TYPE = "taskqueue/serve"
ENDPOINT_SERVE_STUB_TYPE = "endpoint/serve"
ASGI_SERVE_STUB_TYPE = "asgi/serve"
FUNCTION_SERVE_STUB_TYPE = "function/serve"
BOT_SERVE_STUB_TYPE = "bot/serve"

POD_DEPLOYMENT_STUB_TYPE = "pod/deployment"
POD_RUN_STUB_TYPE = "pod/run"
SANDBOX_STUB_TYPE = "sandbox"

_stub_creation_lock = threading.Lock()
_stub_created_for_workspace = False


def _is_stub_created_for_workspace() -> bool:
    global _stub_created_for_workspace
    _stub_created_for_workspace = False
    return _stub_created_for_workspace


def _set_stub_created_for_workspace(value: bool) -> None:
    global _stub_created_for_workspace
    _stub_created_for_workspace = value


class RunnerAbstraction(BaseAbstraction):
    def __init__(
        self,
        app: str = "",
        cpu: Union[int, float, str] = 1.0,
        memory: Union[int, str] = 128,
        gpu: Union[GpuTypeAlias, List[GpuTypeAlias]] = GpuType.NoGPU,
        gpu_count: int = 0,
        image: Image = Image(),
        workers: int = 1,
        concurrent_requests: int = 1,
        keep_warm_seconds: float = 10.0,
        max_pending_tasks: int = 100,
        retries: int = 3,
        timeout: int = 3600,
        volumes: Optional[List[Volume]] = None,
        secrets: Optional[List[str]] = None,
        env: Optional[Dict[str, str]] = {},
        on_start: Optional[Callable] = None,
        on_deploy: Optional["AbstractCallableWrapper"] = None,
        callback_url: Optional[str] = None,
        authorized: bool = True,
        name: Optional[str] = None,
        autoscaler: Autoscaler = QueueDepthAutoscaler(),
        task_policy: TaskPolicy = TaskPolicy(),
        checkpoint_enabled: bool = False,
        entrypoint: Optional[List[str]] = None,
        ports: Optional[List[int]] = [],
        pricing: Optional[PricingPolicy] = None,
        inputs: Optional[Schema] = None,
        outputs: Optional[Schema] = None,
    ) -> None:
        super().__init__()

        if image is None:
            image = Image()

        formatted_env = []
        if env:
            formatted_env = [f"{k}={v}" for k, v in env.items()]

        self.name = name
        self.app = app
        self.authorized = authorized
        self.image: Image = image
        self.image_available: bool = False
        self.files_synced: bool = False
        self.stub_created: bool = False
        self.runtime_ready: bool = False
        self.object_id: str = ""
        self.image_id: str = ""
        self.stub_id: str = ""
        self.handler: str = ""
        self.on_start: str = ""
        self.on_deploy: "AbstractCallableWrapper" = self.parse_on_deploy(on_deploy)
        self.callback_url = callback_url or ""
        self.cpu = self.parse_cpu(cpu)
        self.memory = self.parse_memory(memory)
        self.gpu = gpu
        self.gpu_count = gpu_count
        self.volumes = volumes or []
        self.secrets = [SecretVar(name=s) for s in (secrets or [])]
        self.env: List[str] = formatted_env
        self.workers = workers
        self.concurrent_requests = concurrent_requests
        self.keep_warm_seconds = keep_warm_seconds
        self.max_pending_tasks = max_pending_tasks
        self.autoscaler = autoscaler
        self.task_policy = TaskPolicy(
            max_retries=task_policy.max_retries or retries,
            timeout=task_policy.timeout or timeout,
            ttl=task_policy.ttl,
        )
        self.checkpoint_enabled = checkpoint_enabled
        self.extra: dict = {}
        self.entrypoint: Optional[List[str]] = entrypoint

        if (self.gpu != "" or len(self.gpu) > 0) and self.gpu_count == 0:
            self.gpu_count = 1

        if on_start is not None:
            self._map_callable_to_attr(attr="on_start", func=on_start)

        self._gateway_stub: Optional[GatewayServiceStub] = None
        self._shell_stub: Optional[ShellServiceStub] = None
        self.syncer: FileSyncer = FileSyncer(self.gateway_stub)
        self.settings: SDKSettings = get_settings()
        self.config_context: ConfigContext = get_config_context()
        self.tmp_files: List[TempFile] = []
        self.is_websocket: bool = False
        self.ports: List[int] = ports or []
        self.pricing: Optional[PricingPolicy] = pricing
        self.inputs: Optional[Schema] = inputs
        self.outputs: Optional[Schema] = outputs

    def print_invocation_snippet(self, url_type: str = "") -> GetUrlResponse:
        """Print curl request to call deployed container URL"""

        res = self.gateway_stub.get_url(
            GetUrlRequest(
                stub_id=self.stub_id,
                deployment_id=getattr(self, "deployment_id", ""),
                url_type=url_type,
            )
        )
        if not res.ok:
            return terminal.error("Failed to get invocation URL", exit=False)

        if "<PORT>" in res.url:
            terminal.header("Exposed endpoints\n")

            for port in self.ports:
                terminal.print(f"\tPort {port}: {res.url.replace('<PORT>', str(port))}")

            terminal.print("")
            return res

        terminal.header("Invocation details")
        commands = [
            f"curl -X POST '{res.url}' \\",
            "-H 'Connection: keep-alive' \\",
            "-H 'Content-Type: application/json' \\",
            *(
                [f"-H 'Authorization: Bearer {self.config_context.token}' \\"]
                if self.authorized
                else []
            ),
            "-d '{}'",
        ]

        if self.is_websocket:
            res.url = res.url.replace("http://", "ws://").replace("https://", "wss://")
            commands = [
                f"websocat '{res.url}' \\",
                *(
                    [f"-H 'Authorization: Bearer {self.config_context.token}'"]
                    if self.authorized
                    else []
                ),
            ]

        terminal.print("\n".join(commands), crop=False, overflow="ignore")

        return res

    def parse_memory(self, memory_str: str) -> int:
        if not isinstance(memory_str, str):
            return memory_str

        """Parse memory str (with units) to megabytes."""

        if memory_str.lower().endswith("mi"):
            return int(memory_str[:-2])
        elif memory_str.lower().endswith("gb"):
            return int(memory_str[:-2]) * 1000
        elif memory_str.lower().endswith("gi"):
            return int(memory_str[:-2]) * 1024
        else:
            raise ValueError("Unsupported memory format")

    @property
    def gateway_stub(self) -> GatewayServiceStub:
        if not self._gateway_stub:
            self._gateway_stub = GatewayServiceStub(self.channel)
        return self._gateway_stub

    @gateway_stub.setter
    def gateway_stub(self, value) -> None:
        self._gateway_stub = value

    @property
    def shell_stub(self) -> ShellServiceStub:
        if not self._shell_stub:
            self._shell_stub = ShellServiceStub(self.channel)
        return self._shell_stub

    @shell_stub.setter
    def shell_stub(self, value) -> None:
        self._shell_stub = value

    def parse_cpu(self, cpu: Union[float, str]) -> int:
        """
        Parse the cpu argument to an integer value in millicores.

        Args:
        cpu (Union[int, float, str]): The CPU requirement specified as a float (cores) or string (millicores).

        Returns:
        int: The CPU requirement in millicores.

        Raises:
        ValueError: If the input is invalid or out of the specified range.
        """
        min_cores = 0.1
        max_cores = 64.0

        if isinstance(cpu, float) or isinstance(cpu, int):
            if min_cores <= cpu <= max_cores:
                return int(cpu * 1000)  # convert cores to millicores
            else:
                raise ValueError("CPU value out of range. Must be between 0.1 and 64 cores.")

        elif isinstance(cpu, str):
            if cpu.endswith("m") and cpu[:-1].isdigit():
                millicores = int(cpu[:-1])
                if min_cores * 1000 <= millicores <= max_cores * 1000:
                    return millicores
                else:
                    raise ValueError("CPU value out of range. Must be between 100m and 64000m.")
            else:
                raise ValueError(
                    "Invalid CPU string format. Must be a digit followed by 'm' (e.g., '1000m')."
                )

        else:
            raise TypeError("CPU must be a float or a string.")

    def _map_callable_to_attr(self, *, attr: str, func: Callable):
        """
        Determine the module and function name of a callable function, and cache on the class.
        For Jupyter notebooks, serialize everything using cloudpickle.
        """
        if getattr(self, attr):
            return

        module = inspect.getmodule(func)

        # We check for the notebook environment before a normal module (.py file) because
        # marimo notebooks are normal python files.
        if is_notebook_env():
            tmp_file = TempFile(name=f"{func.__name__}.pkl", mode="wb")
            try:
                cloudpickle.dump(func, tmp_file)
                tmp_file.flush()
                module_name = os.path.basename(tmp_file.name)
                self.tmp_files.append(tmp_file)

                setattr(self, attr, f"{module_name}:{func.__name__}")
            except Exception as e:
                os.unlink(tmp_file.name)
                raise ValueError(f"Failed to pickle function: {str(e)}")
        elif hasattr(module, "__file__"):
            # Normal module case - use relative path
            module_file = os.path.relpath(module.__file__, start=os.getcwd()).replace("/", ".")
            module_name = os.path.splitext(module_file)[0]
            setattr(self, attr, f"{module_name}:{func.__name__}")
        else:
            module_name = "__main__"
            setattr(self, attr, f"{module_name}:{func.__name__}")

    def _remove_tmp_files(self) -> None:
        for tmp_file in self.tmp_files:
            tmp_file.close()

    def parse_gpu(self, gpu: Union[GpuTypeAlias, List[GpuTypeAlias]]) -> str:
        if not isinstance(gpu, str) and not isinstance(gpu, list):
            raise ValueError("Invalid GPU type")

        if isinstance(gpu, list):
            return ",".join([GpuType(g).value for g in gpu])
        else:
            return GpuType(gpu).value

    def parse_on_deploy(self, func: Callable):
        if func is None:
            return None

        if not callable(func) or not hasattr(func, "parent") or not hasattr(func, "func"):
            raise terminal.error(
                "Build failed: on_deploy must be a callable function with a function decorator"
            )

        return func

    def _schema_to_proto(self, py_schema):
        if py_schema is None:
            return None

        def _field_to_proto(field_dict):
            # Handle nested object fields
            if field_dict["type"] == "Object":
                # Recursively convert nested fields
                nested_fields = field_dict.get("fields", {}).get("fields", {})
                return SchemaFieldProto(
                    type="object",
                    fields=SchemaProto(
                        fields={k: _field_to_proto(v) for k, v in nested_fields.items()}
                    ),
                )
            else:
                return SchemaFieldProto(type=field_dict["type"])

        fields_dict = py_schema.to_dict()["fields"]
        return SchemaProto(fields={k: _field_to_proto(v) for k, v in fields_dict.items()})

    def prepare_runtime(
        self,
        *,
        func: Optional[Callable] = None,
        stub_type: str,
        force_create_stub: bool = False,
    ) -> bool:
        if called_on_import():
            return False

        if func is not None:
            self._map_callable_to_attr(attr="handler", func=func)

        stub_name = f"{stub_type}/{self.handler}" if self.handler else stub_type

        if self.runtime_ready:
            return True

        if not self.image_available:
            image_build_result: ImageBuildResult = self.image.build()

            if image_build_result and image_build_result.success:
                self.image_available = True
                self.image_id = image_build_result.image_id
                self.image.python_version = image_build_result.python_version
            else:
                terminal.error("Image build failed ❌", exit=False)
                return False

        if not self.files_synced:
            sync_result = self.syncer.sync()
            self._remove_tmp_files()

            if sync_result.success:
                self.files_synced = True
                self.object_id = sync_result.object_id
            else:
                terminal.error("File sync failed", exit=False)
                return False

        for v in self.volumes:
            if not v.ready and not v.get_or_create():
                terminal.error(f"Volume is not ready: {v.name}", exit=False)
                return False

        try:
            self.gpu = self.parse_gpu(self.gpu)
        except ValueError:
            terminal.error(f"Invalid GPU type: {self.gpu}", exit=False)
            return False

        autoscaler_type = _AUTOSCALER_TYPES.get(type(self.autoscaler), "")
        if not autoscaler_type:
            terminal.error(
                f"Invalid Autoscaler class: {type(self.autoscaler).__name__}",
                exit=False,
            )
            return False

        if not self.app:
            self.app = self.name or os.path.basename(os.getcwd())

        inputs = None
        if self.inputs:
            inputs = self._schema_to_proto(self.inputs)

        outputs = None
        if self.outputs:
            outputs = self._schema_to_proto(self.outputs)

        if not self.stub_created:
            stub_request = GetOrCreateStubRequest(
                object_id=self.object_id,
                image_id=self.image_id,
                stub_type=stub_type,
                name=stub_name,
                app_name=self.app,
                python_version=self.image.python_version,
                cpu=self.cpu,
                memory=self.memory,
                gpu=self.gpu,
                gpu_count=self.gpu_count,
                handler=self.handler,
                on_start=self.on_start,
                on_deploy=self.on_deploy.parent.handler if self.on_deploy else "",
                on_deploy_stub_id=self.on_deploy.parent.stub_id if self.on_deploy else "",
                callback_url=self.callback_url,
                keep_warm_seconds=self.keep_warm_seconds,
                workers=self.workers,
                max_pending_tasks=self.max_pending_tasks,
                volumes=[v.export() for v in self.volumes],
                secrets=self.secrets,
                env=self.env,
                force_create=force_create_stub,
                authorized=self.authorized,
                autoscaler=AutoscalerProto(
                    type=autoscaler_type,
                    max_containers=self.autoscaler.max_containers,
                    tasks_per_container=self.autoscaler.tasks_per_container,
                    min_containers=self.autoscaler.min_containers,
                ),
                task_policy=TaskPolicyProto(
                    max_retries=self.task_policy.max_retries,
                    timeout=self.task_policy.timeout,
                    ttl=self.task_policy.ttl,
                ),
                concurrent_requests=self.concurrent_requests,
                checkpoint_enabled=self.checkpoint_enabled,
                extra=json.dumps(self.extra),
                entrypoint=self.entrypoint,
                ports=self.ports,
                pricing=PricingPolicyProto(
                    cost_per_task=self.pricing.cost_per_task,
                    cost_per_task_duration_ms=self.pricing.cost_per_task_duration_ms,
                    cost_model=self.pricing.cost_model,
                    max_in_flight=self.pricing.max_in_flight,
                )
                if self.pricing
                else None,
                inputs=inputs,
                outputs=outputs,
            )
            if _is_stub_created_for_workspace():
                stub_response: GetOrCreateStubResponse = self.gateway_stub.get_or_create_stub(
                    stub_request
                )
            else:
                with _stub_creation_lock:
                    stub_response: GetOrCreateStubResponse = self.gateway_stub.get_or_create_stub(
                        stub_request
                    )
                    _set_stub_created_for_workspace(True)

            if stub_response.ok:
                self.stub_created = True
                self.stub_id = stub_response.stub_id
                if stub_response.warn_msg:
                    terminal.warn(stub_response.warn_msg)
            else:
                if err := stub_response.err_msg:
                    terminal.error(err, exit=False)
                else:
                    terminal.error("Failed to get or create stub", exit=False)
                return False

        self.runtime_ready = True
        return True


class AbstractCallableWrapper:
    def __init__(self, func: Callable, parent: RunnerAbstraction):
        self.func = func
        self.parent = parent

    def __call__(self, *args, **kwargs):
        raise NotImplementedError

================================================================================
# File: abstractions/endpoint.py
# Path: src/beta9/abstractions/endpoint.py
================================================================================

import os
import traceback
import types
from typing import Any, Callable, Dict, List, Optional, Union

from uvicorn.protocols.utils import ClientDisconnected

from .. import terminal
from ..abstractions.base.container import Container
from ..abstractions.base.runner import (
    ASGI_DEPLOYMENT_STUB_TYPE,
    ASGI_SERVE_STUB_TYPE,
    ASGI_STUB_TYPE,
    ENDPOINT_DEPLOYMENT_STUB_TYPE,
    ENDPOINT_SERVE_STUB_TYPE,
    ENDPOINT_STUB_TYPE,
    AbstractCallableWrapper,
    RunnerAbstraction,
)
from ..abstractions.image import Image
from ..abstractions.volume import CloudBucket, Volume
from ..channel import with_grpc_error_handling
from ..clients.endpoint import (
    EndpointServiceStub,
    StartEndpointServeRequest,
    StartEndpointServeResponse,
)
from ..env import is_local
from ..schema import Schema
from ..type import (
    Autoscaler,
    GpuType,
    GpuTypeAlias,
    PricingPolicy,
    QueueDepthAutoscaler,
    TaskPolicy,
)
from .mixins import DeployableMixin


class Endpoint(RunnerAbstraction):
    """
    Decorator which allows you to create a web endpoint out of the decorated function.
    Tasks are invoked synchronously as HTTP requests.

    Parameters:
        app (str):
            Assign the endpoint to an app. If the app does not exist, it will be created with the given name.
            An app is a group of resources (endpoints, task queues, functions, etc).
        cpu (Union[int, float, str]):
            The number of CPU cores allocated to the container. Default is 1.0.
        memory (Union[int, str]):
            The amount of memory allocated to the container. It should be specified in
            MiB, or as a string with units (e.g. "1Gi"). Default is 128 MiB.
        gpu (Union[GpuTypeAlias, List[GpuTypeAlias]]):
            The type or name of the GPU device to be used for GPU-accelerated tasks. If not
            applicable or no GPU required, leave it empty.
            You can specify multiple GPUs by providing a list of GpuTypeAlias. If you specify several GPUs,
            the scheduler prioritizes their selection based on their order in the list.
        gpu_count (int):
            The number of GPUs allocated to the container. Default is 0. If a GPU is
            specified but this value is set to 0, it will be automatically updated to 1.
        image (Union[Image, dict]):
            The container image used for the task execution. Default is [Image](#image).
        volumes (Optional[List[Union[Volume, CloudBucket]]]):
            A list of volumes and/or cloud buckets to be mounted to the endpoint. Default is None.
        timeout (Optional[int]):
            The maximum number of seconds a task can run before it times out.
            Default is 180. Set it to -1 to disable the timeout.
        workers (Optional[int]):
            The number of processes handling tasks per container.
            Modifying this parameter can improve throughput for certain workloads.
            Workers will share the CPU, Memory, and GPU defined.
            You may need to increase these values to increase concurrency.
            Default is 1.
        keep_warm_seconds (int):
            The duration in seconds to keep the task queue warm even if there are no pending
            tasks. Keeping the queue warm helps to reduce the latency when new tasks arrive.
            Default is 10s.
        max_pending_tasks (int):
            The maximum number of tasks that can be pending in the queue. If the number of
            pending tasks exceeds this value, the task queue will stop accepting new tasks.
            Default is 100.
        secrets (Optional[List[str]):
            A list of secrets that are injected into the container as environment variables. Default is [].
        env (Optional[Dict[str, str]]):
            A dictionary of environment variables to be injected into the container. Default is {}.
        name (Optional[str]):
            An optional app name for this endpoint. If not specified, it will be the name of the
            working directory containing the python file with the decorated function.
        authorized (bool):
            If false, allows the endpoint to be invoked without an auth token.
            Default is True.
        autoscaler (Optional[Autoscaler]):
            Configure a deployment autoscaler - if specified, you can use scale your function horizontally using
            various autoscaling strategies (Defaults to QueueDepthAutoscaler())
        callback_url (Optional[str]):
            An optional URL to send a callback to when a task is completed, timed out, or cancelled.
        task_policy (TaskPolicy):
            The task policy for the function. This helps manage the lifecycle of an individual task.
            Setting values here will override timeout and retries.
        checkpoint_enabled (bool):
            (experimental) Whether to enable checkpointing for the endpoint. Default is False.
            If enabled, the app will be checkpointed after the on_start function has completed.
            On next invocation, each container will restore from a checkpoint and resume execution instead of
            booting up from cold.
        inputs (Optional[Schema]):
            The input schema for the endpoint. Default is None.
        outputs (Optional[Schema]):
            The output model for the endpoint. Default is None.
    Example:
        ```python
        from beta9 import endpoint, Image

        @endpoint(
            cpu=1.0,
            memory=128,
            gpu="T4",
            image=Image(python_packages=["torch"]),
            keep_warm_seconds=1000,
            name="my-app",
        )
        def multiply(**inputs):
            result = inputs["x"] * 2
            return {"result": result}
        ```
    """

    concurrent_requests = 1

    def __init__(
        self,
        cpu: Union[int, float, str] = 1.0,
        memory: Union[int, str] = 128,
        app: str = "",
        gpu: Union[GpuTypeAlias, List[GpuTypeAlias]] = GpuType.NoGPU,
        gpu_count: int = 0,
        image: Image = Image(),
        timeout: int = 180,
        workers: int = 1,
        keep_warm_seconds: int = 180,
        max_pending_tasks: int = 100,
        on_start: Optional[Callable] = None,
        on_deploy: Optional[AbstractCallableWrapper] = None,
        volumes: Optional[List[Union[Volume, CloudBucket]]] = None,
        secrets: Optional[List[str]] = None,
        env: Optional[Dict[str, str]] = {},
        name: Optional[str] = None,
        authorized: bool = True,
        autoscaler: Autoscaler = QueueDepthAutoscaler(),
        callback_url: Optional[str] = None,
        task_policy: TaskPolicy = TaskPolicy(),
        checkpoint_enabled: bool = False,
        pricing: Optional[PricingPolicy] = None,
        inputs: Optional[Schema] = None,
        outputs: Optional[Schema] = None,
    ):
        super().__init__(
            cpu=cpu,
            memory=memory,
            gpu=gpu,
            gpu_count=gpu_count,
            image=image,
            workers=workers,
            timeout=timeout,
            retries=0,
            keep_warm_seconds=keep_warm_seconds,
            max_pending_tasks=max_pending_tasks,
            on_start=on_start,
            on_deploy=on_deploy,
            volumes=volumes,
            secrets=secrets,
            env=env,
            name=name,
            authorized=authorized,
            autoscaler=autoscaler,
            callback_url=callback_url,
            task_policy=task_policy,
            concurrent_requests=self.concurrent_requests,
            checkpoint_enabled=checkpoint_enabled,
            app=app,
            pricing=pricing,
            inputs=inputs,
            outputs=outputs,
        )

        self._endpoint_stub: Optional[EndpointServiceStub] = None

    @property
    def endpoint_stub(self) -> EndpointServiceStub:
        if not self._endpoint_stub:
            self._endpoint_stub = EndpointServiceStub(self.channel)
        return self._endpoint_stub

    def __call__(self, func):
        return _CallableWrapper(func, self)


class ASGI(Endpoint):
    """
    Decorator which allows you to create an ASGI application.

    Parameters:
        app (str):
            Assign the ASGI endpoint to an app. If the app does not exist, it will be created with the given name.
            An app is a group of resources (endpoints, task queues, functions, etc).
        cpu (Union[int, float, str]):
            The number of CPU cores allocated to the container. Default is 1.0.
        memory (Union[int, str]):
            The amount of memory allocated to the container. It should be specified in
            MiB, or as a string with units (e.g. "1Gi"). Default is 128 MiB.
        gpu (Union[GpuType, str]):
            The type or name of the GPU device to be used for GPU-accelerated tasks. If not
            applicable or no GPU required, leave it empty. Default is [GpuType.NoGPU](#gputype).
        gpu_count (int):
            The number of GPUs allocated to the container. Default is 0. If a GPU is
            specified but this value is set to 0, it will be automatically updated to 1.
        image (Union[Image, dict]):
            The container image used for the task execution. Default is [Image](#image).
        volumes (Optional[List[Union[Volume, CloudBucket]]]):
            A list of volumes and/or cloud buckets to be mounted to the ASGI application. Default is None.
        timeout (Optional[int]):
            The maximum number of seconds a task can run before it times out.
            Default is 3600. Set it to -1 to disable the timeout.
        retries (Optional[int]):
            The maximum number of times a task will be retried if the container crashes. Default is 3.
        workers (Optional[int]):
            The number of processes handling tasks per container.
            Modifying this parameter can improve throughput for certain workloads.
            Workers will share the CPU, Memory, and GPU defined.
            You may need to increase these values to increase concurrency.
            Default is 1.
        concurrent_requests (int):
            The maximum number of concurrent requests the ASGI application can handle.
            Unlike regular endpoints that process requests synchronously, ASGI applications
            can handle multiple requests concurrently. This parameter allows you to specify
            the level of concurrency. For applications with blocking operations, this can
            improve throughput by allowing the application to process other requests while
            waiting for blocking operations to complete. Default is 1.
        keep_warm_seconds (int):
            The duration in seconds to keep the task queue warm even if there are no pending
            tasks. Keeping the queue warm helps to reduce the latency when new tasks arrive.
            Default is 10s.
        max_pending_tasks (int):
            The maximum number of tasks that can be pending in the queue. If the number of
            pending tasks exceeds this value, the task queue will stop accepting new tasks.
            Default is 100.
        secrets (Optional[List[str]):
            A list of secrets that are injected into the container as environment variables. Default is [].
        env (Optional[Dict[str, str]]):
            A dictionary of environment variables to be injected into the container. Default is {}.
        name (Optional[str]):
            An optional app name for this ASGI application. If not specified, it will be the name of the
            working directory containing the python file with the decorated function.
        authorized (bool):
            If false, allows the ASGI application to be invoked without an auth token.
            Default is True.
        autoscaler (Optional[Autoscaler]):
            Configure a deployment autoscaler - if specified, you can use scale your function horizontally using
            various autoscaling strategies (Defaults to QueueDepthAutoscaler())
        callback_url (Optional[str]):
            An optional URL to send a callback to when a task is completed, timed out, or cancelled.
        task_policy (TaskPolicy):
            The task policy for the function. This helps manage the lifecycle of an individual task.
            Setting values here will override timeout and retries.
        checkpoint_enabled (bool):
            (experimental) Whether to enable checkpointing for the endpoint. Default is False.
            If enabled, the app will be checkpointed after the on_start function has completed.
            On next invocation, each container will restore from a checkpoint and resume execution instead of
            booting up from cold.
    Example:
        ```python
        from beta9 import asgi, Image

        @asgi(
            cpu=1.0,
            memory=128,
            gpu="T4"
        )
        def web_server(context):
            from fastapi import FastAPI

            app = FastAPI()

            @app.post("/hello")
            async def something():
                return {"hello": True}

            @app.post("/warmup")
            async def warmup():
                return {"status": "warm"}

            return app
        ```
    """

    def __init__(
        self,
        app: str = "",
        cpu: Union[int, float, str] = 1.0,
        memory: Union[int, str] = 128,
        gpu: GpuTypeAlias = GpuType.NoGPU,
        gpu_count: int = 0,
        image: Image = Image(),
        timeout: int = 180,
        workers: int = 1,
        concurrent_requests: int = 1,
        keep_warm_seconds: int = 180,
        max_pending_tasks: int = 100,
        on_start: Optional[Callable] = None,
        on_deploy: Optional[AbstractCallableWrapper] = None,
        volumes: Optional[List[Union[Volume, CloudBucket]]] = None,
        secrets: Optional[List[str]] = None,
        env: Optional[Dict[str, str]] = {},
        name: Optional[str] = None,
        authorized: bool = True,
        autoscaler: Autoscaler = QueueDepthAutoscaler(),
        callback_url: Optional[str] = None,
        checkpoint_enabled: bool = False,
        pricing: Optional[PricingPolicy] = None,
    ):
        self.concurrent_requests = concurrent_requests
        super().__init__(
            cpu=cpu,
            memory=memory,
            gpu=gpu,
            gpu_count=gpu_count,
            image=image,
            timeout=timeout,
            workers=workers,
            keep_warm_seconds=keep_warm_seconds,
            max_pending_tasks=max_pending_tasks,
            on_start=on_start,
            on_deploy=on_deploy,
            volumes=volumes,
            secrets=secrets,
            env=env,
            name=name,
            authorized=authorized,
            autoscaler=autoscaler,
            callback_url=callback_url,
            checkpoint_enabled=checkpoint_enabled,
            app=app,
            pricing=pricing,
        )

        self.is_asgi = True


REALTIME_ASGI_SLEEP_INTERVAL_SECONDS = 0.1
REALTIME_ASGI_HEALTH_CHECK_INTERVAL_SECONDS = 5


class RealtimeASGI(ASGI):
    """
    Decorator which allows you to create a realtime application built on top of ASGI / websockets.
    Your handler function will run every time a message is received over the websocket.

    Parameters:
        cpu (Union[int, float, str]):
            The number of CPU cores allocated to the container. Default is 1.0.
        memory (Union[int, str]):
            The amount of memory allocated to the container. It should be specified in
            MiB, or as a string with units (e.g. "1Gi"). Default is 128 MiB.
        gpu (Union[GpuType, str]):
            The type or name of the GPU device to be used for GPU-accelerated tasks. If not
            applicable or no GPU required, leave it empty. Default is [GpuType.NoGPU](#gputype).
        gpu_count (int):
            The number of GPUs allocated to the container. Default is 0. If a GPU is
            specified but this value is set to 0, it will be automatically updated to 1.
        image (Union[Image, dict]):
            The container image used for the task execution. Default is [Image](#image).
        volumes (Optional[List[Union[Volume, CloudBucket]]]):
            A list of volumes and/or cloud buckets to be mounted to the ASGI application. Default is None.
        timeout (Optional[int]):
            The maximum number of seconds a task can run before it times out.
            Default is 3600. Set it to -1 to disable the timeout.
        workers (Optional[int]):
            The number of processes handling tasks per container.
            Modifying this parameter can improve throughput for certain workloads.
            Workers will share the CPU, Memory, and GPU defined.
            You may need to increase these values to increase concurrency.
            Default is 1.
        concurrent_requests (int):
            The maximum number of concurrent requests the ASGI application can handle.
            Unlike regular endpoints that process requests synchronously, ASGI applications
            can handle multiple requests concurrently. This parameter allows you to specify
            the level of concurrency. For applications with blocking operations, this can
            improve throughput by allowing the application to process other requests while
            waiting for blocking operations to complete. Default is 1.
        keep_warm_seconds (int):
            The duration in seconds to keep the task queue warm even if there are no pending
            tasks. Keeping the queue warm helps to reduce the latency when new tasks arrive.
            Default is 10s.
        max_pending_tasks (int):
            The maximum number of tasks that can be pending in the queue. If the number of
            pending tasks exceeds this value, the task queue will stop accepting new tasks.
            Default is 100.
        secrets (Optional[List[str]):
            A list of secrets that are injected into the container as environment variables. Default is [].
        env (Optional[Dict[str, str]]):
            A dictionary of environment variables to be injected into the container. Default is {}.
        name (Optional[str]):
            An optional app name for this ASGI application. If not specified, it will be the name of the
            working directory containing the python file with the decorated function.
        authorized (bool):
            If false, allows the ASGI application to be invoked without an auth token.
            Default is True.
        autoscaler (Optional[Autoscaler]):
            Configure a deployment autoscaler - if specified, you can use scale your function horizontally using
            various autoscaling strategies (Defaults to QueueDepthAutoscaler())
        callback_url (Optional[str]):
            An optional URL to send a callback to when a task is completed, timed out, or cancelled.
        checkpoint_enabled (bool):
            (experimental) Whether to enable checkpointing for the endpoint. Default is False.
            If enabled, the app will be checkpointed after the on_start function has completed.
            On next invocation, each container will restore from a checkpoint and resume execution instead of
            booting up from cold.
    Example:
        ```python
        from beta9 import realtime

        def generate_text():
            return ["this", "could", "be", "anything"]

        @realtime(
            cpu=1.0,
            memory=128,
            gpu="T4"
        )
        def handler(context):
            return generate_text()
        ```
    """

    def __init__(
        self,
        cpu: Union[int, float, str] = 1.0,
        memory: Union[int, str] = 128,
        gpu: GpuTypeAlias = GpuType.NoGPU,
        gpu_count: int = 0,
        image: Image = Image(),
        timeout: int = 180,
        workers: int = 1,
        concurrent_requests: int = 1,
        keep_warm_seconds: int = 180,
        max_pending_tasks: int = 100,
        on_start: Optional[Callable] = None,
        on_deploy: Optional[AbstractCallableWrapper] = None,
        volumes: Optional[List[Union[Volume, CloudBucket]]] = None,
        secrets: Optional[List[str]] = None,
        env: Optional[Dict[str, str]] = {},
        name: Optional[str] = None,
        authorized: bool = True,
        autoscaler: Autoscaler = QueueDepthAutoscaler(),
        callback_url: Optional[str] = None,
        checkpoint_enabled: bool = False,
        pricing: Optional[PricingPolicy] = None,
    ):
        super().__init__(
            cpu=cpu,
            memory=memory,
            gpu=gpu,
            gpu_count=gpu_count,
            image=image,
            timeout=timeout,
            workers=workers,
            keep_warm_seconds=keep_warm_seconds,
            max_pending_tasks=max_pending_tasks,
            on_start=on_start,
            on_deploy=on_deploy,
            volumes=volumes,
            secrets=secrets,
            env=env,
            name=name,
            authorized=authorized,
            autoscaler=autoscaler,
            callback_url=callback_url,
            concurrent_requests=concurrent_requests,
            checkpoint_enabled=checkpoint_enabled,
            pricing=pricing,
        )
        self.is_websocket = True

    def __call__(self, func):
        import asyncio
        from queue import Queue

        from fastapi import FastAPI, WebSocket, WebSocketDisconnect, WebSocketException

        internal_asgi_app = FastAPI()
        internal_asgi_app.input_queue = Queue()

        @internal_asgi_app.websocket("/")
        async def stream(websocket: WebSocket):
            async def _heartbeat():
                while True:
                    try:
                        await websocket.send_json({"type": "ping"})
                        await asyncio.sleep(REALTIME_ASGI_HEALTH_CHECK_INTERVAL_SECONDS)
                    except (WebSocketDisconnect, WebSocketException, RuntimeError):
                        break

            await websocket.accept()

            heartbeat_task = asyncio.create_task(_heartbeat())
            try:
                while True:
                    try:
                        message = await websocket.receive()

                        if "text" in message:
                            data = message["text"]
                        elif "bytes" in message:
                            data = message["bytes"]
                        elif "json" in message:
                            data = message["json"]
                        elif message.get("type") == "websocket.disconnect":
                            return
                        else:
                            print(f"WS stream error - unknown message type: {message}")
                            continue

                        internal_asgi_app.input_queue.put(data)

                        async def _handle_output(output):
                            if isinstance(output, str):
                                await websocket.send_text(output)
                            elif isinstance(output, dict) or isinstance(output, list):
                                await websocket.send_json(output)
                            else:
                                await websocket.send(output)

                        while not internal_asgi_app.input_queue.empty():
                            output = internal_asgi_app.handler(
                                context=internal_asgi_app.context,
                                event=internal_asgi_app.input_queue.get(),
                            )

                            if isinstance(output, types.GeneratorType):
                                for o in output:
                                    await _handle_output(o)
                            else:
                                await _handle_output(output)

                        await asyncio.sleep(REALTIME_ASGI_SLEEP_INTERVAL_SECONDS)
                    except (
                        WebSocketDisconnect,
                        WebSocketException,
                        RuntimeError,
                        ClientDisconnected,
                    ):
                        return
                    except BaseException:
                        print(f"Unhandled exception in websocket stream: {traceback.format_exc()}")
            finally:
                heartbeat_task.cancel()

        func.internal_asgi_app = internal_asgi_app
        return _CallableWrapper(func, self)


class _CallableWrapper(DeployableMixin):
    deployment_stub_type = ENDPOINT_DEPLOYMENT_STUB_TYPE
    base_stub_type = ENDPOINT_STUB_TYPE

    def __init__(self, func: Callable, parent: Union[Endpoint, ASGI]):
        self.func: Callable = func
        self.parent: Union[Endpoint, ASGI] = parent

        if getattr(self.parent, "is_asgi", None):
            self.deployment_stub_type = ASGI_DEPLOYMENT_STUB_TYPE
            self.base_stub_type = ASGI_STUB_TYPE

    def __call__(self, *args, **kwargs) -> Any:
        if not is_local():
            return self.local(*args, **kwargs)

        raise NotImplementedError("Direct calls to Endpoints are not supported.")

    def local(self, *args, **kwargs) -> Any:
        return self.func(*args, **kwargs)

    @with_grpc_error_handling
    def serve(self, timeout: int = 0, url_type: str = ""):
        stub_type = ENDPOINT_SERVE_STUB_TYPE

        if getattr(self.parent, "is_asgi", None):
            stub_type = ASGI_SERVE_STUB_TYPE

        if not self.parent.prepare_runtime(
            func=self.func, stub_type=stub_type, force_create_stub=True
        ):
            return False

        try:
            with terminal.progress("Serving endpoint..."):
                self.parent.print_invocation_snippet(url_type=url_type)

                return self._serve(dir=os.getcwd(), timeout=timeout)
        except KeyboardInterrupt:
            terminal.header("Stopping serve container")
            terminal.print("Goodbye 👋")
            os._exit(0)  # kills all threads immediately

    def _serve(self, *, dir: str, timeout: int = 0):
        r: StartEndpointServeResponse = self.parent.endpoint_stub.start_endpoint_serve(
            StartEndpointServeRequest(
                stub_id=self.parent.stub_id,
                timeout=timeout,
            )
        )
        if not r.ok:
            return terminal.error(r.error_msg)

        container = Container(container_id=r.container_id)
        container.attach(container_id=r.container_id, sync_dir=dir)

================================================================================
# File: abstractions/experimental/__init__.py
# Path: src/beta9/abstractions/experimental/__init__.py
================================================================================

from .bot.bot import Bot, BotTransition
from .bot.marker import BotLocation
from .signal import Signal

__all__ = ["Signal", "Bot", "BotTransition", "BotLocation"]

================================================================================
# File: abstractions/experimental/bot/__init__.py
# Path: src/beta9/abstractions/experimental/bot/__init__.py
================================================================================


================================================================================
# File: abstractions/experimental/bot/bot.py
# Path: src/beta9/abstractions/experimental/bot/bot.py
================================================================================

import inspect
import json
import os
import threading
import uuid
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Union

from pydantic import BaseModel

from .... import env, terminal
from ....abstractions.base.runner import (
    BOT_DEPLOYMENT_STUB_TYPE,
    BOT_SERVE_STUB_TYPE,
    BOT_STUB_TYPE,
    RunnerAbstraction,
)
from ....abstractions.image import Image, ImageBuildResult
from ....abstractions.volume import Volume
from ....channel import with_grpc_error_handling
from ....clients.bot import (
    BotServiceStub,
)
from ....sync import FileSyncer
from ....type import GpuType, GpuTypeAlias, PythonVersion
from ...mixins import DeployableMixin
from .marker import BotLocation


class BotEventType(str, Enum):
    AGENT_MESSAGE = "agent_message"
    USER_MESSAGE = "user_message"
    TRANSITION_MESSAGE = "transition_message"
    MEMORY_MESSAGE = "memory_message"
    MEMORY_UPDATED = "memory_updated"
    SESSION_CREATED = "session_created"
    TRANSITION_FIRED = "transition_fired"
    TRANSITION_STARTED = "transition_started"
    TRANSITION_COMPLETED = "transition_completed"
    TRANSITION_FAILED = "transition_failed"
    NETWORK_STATE = "network_state"
    CONFIRM_TRANSITION = "confirm_transition"
    ACCEPT_TRANSITION = "accept_transition"
    REJECT_TRANSITION = "reject_transition"
    OUTPUT_FILE = "output_file"
    INPUT_FILE_REQUEST = "input_file_request"
    INPUT_FILE_RESPONSE = "input_file_response"
    CONFIRM_REQUEST = "confirm_request"
    CONFIRM_RESPONSE = "confirm_response"


class BotEvent(BaseModel):
    type: BotEventType
    value: str
    metadata: dict = {}


class BotTransition:
    """
    Parameters:
        cpu (Union[int, float, str]):
            The number of CPUs to allocate to the transition. Default is 1.0.
        memory (Union[int, str]):
            The amount of memory to allocate to the transition. Default is 128.
        gpu (GpuTypeAlias):
            The type of GPU to allocate to the transition. Default is GpuType.NoGPU.
        image (Image):
            The image to use for the transition. Default is an empty Image.
        secrets (Optional[List[str]]):
            A list of secrets to pass to the transition. Default is None.
        callback_url (Optional[str]):
            The URL to send callback events to. Default is None.
        inputs (dict):
            A dictionary of inputs to pass to the transition. Default is {}.
        outputs (list):
            A list of outputs the transition can return. Default is [].
        description (Optional[str]):
            A description of the transition. Default is None.
        expose (bool):
            Whether or not to give the model awareness of this transition. Default is True.
        confirm (bool):
            Whether or not to ask the user for confirmation before running the transition. Default is False.
        task_policy (Optional[str]):
            The task policy to use for the transition. Default is None.
    """

    def __init__(
        self,
        cpu: Union[int, float, str] = 1.0,
        memory: Union[int, str] = 128,
        gpu: GpuTypeAlias = GpuType.NoGPU,
        image: Image = Image(),
        secrets: Optional[List[str]] = None,
        callback_url: Optional[str] = None,
        inputs: dict = {},
        outputs: list = [],
        description: Optional[str] = None,
        expose: bool = True,
        confirm: bool = False,
        bot_instance: Optional["Bot"] = None,  # Reference to parent Bot instance
        task_policy: Optional[str] = None,
        handler: Optional[str] = None,
    ):
        self.handler: str = handler
        self.image: Image = image
        self.image_available: bool = False
        self.image_id: str = ""

        if not isinstance(inputs, dict):
            raise ValueError("Inputs must be a dictionary.")

        if not inputs:
            raise ValueError("Inputs cannot be empty.")

        for key, value in inputs.items():
            if not (isinstance(key, type) and issubclass(key, BaseModel)):
                raise ValueError(
                    f"Invalid key in inputs: {key}. All keys must be classes inherited from a Pydantic BaseModel."
                )

            if not isinstance(value, int):
                raise ValueError(
                    f"Invalid value in inputs for key {key}: {value}. All values must be integers."
                )
        for v in outputs:
            if not (isinstance(v, type) and issubclass(v, BaseModel)):
                raise ValueError(
                    f"Invalid value in outputs: {v}. All values must be classes inherited from a Pydantic BaseModel."
                )

        self.config = {
            "cpu": cpu,
            "memory": memory,
            "gpu": gpu,
            "secrets": secrets or [],
            "callback_url": callback_url or "",
            "task_policy": task_policy or "",
            "handler": handler or "",
            "python_version": self.image.python_version or PythonVersion.Python310,
            "inputs": {
                k.__name__ if isinstance(k, type) and env.is_local() else k: v
                for k, v in inputs.items()
            },
            "outputs": [
                k.__name__ if isinstance(k, type) and env.is_local() else k for k in outputs
            ],
            "description": description or "",
            "expose": expose,
            "confirm": confirm,
        }

        self.bot_instance: Optional["Bot"] = bot_instance

        if self.bot_instance.image != self.image or self.bot_instance.image_id == "":
            if env.is_remote():
                return

            if not self._build_image_for_transition():
                return

        self.config["image_id"] = self.image_id

    def _build_image_for_transition(self) -> Optional[bool]:
        if not self.image_available:
            terminal.detail(f"Building image for transition: {self.image}")

            image_build_result: ImageBuildResult = self.image.build()
            if image_build_result and image_build_result.success:
                self.image_available = True
                self.image_id = image_build_result.image_id
                return True
            else:
                terminal.error("Image build failed", exit=False)
                return False

    def _map_callable_to_attr(self, *, attr: str, func: Callable):
        """
        Determine the module and function name of a callable function, and cache on the class.
        This is used for passing callables to stub config.
        """
        if getattr(self, attr):
            return

        module = inspect.getmodule(func)  # Determine module / function name
        if module:
            module_file = os.path.relpath(module.__file__, start=os.getcwd()).replace("/", ".")
            module_name = os.path.splitext(module_file)[0]
        else:
            module_name = "__main__"

        function_name = func.__name__
        setattr(self, attr, f"{module_name}:{function_name}")

    def local(self, *args, **kwargs) -> Any:
        return self.func(*args, **kwargs)

    def __call__(self, *args, **kwargs) -> None:
        if env.called_on_import():
            self.func = args[0]
            return self

        if not env.is_local():
            return self.local(*args, **kwargs)

        self.func = args[0]
        self._map_callable_to_attr(attr="handler", func=self.func)
        self.config["handler"] = getattr(self, "handler")

        transition_data = self.config.copy()
        transition_data["name"] = self.func.__name__

        if not hasattr(self.bot_instance, "extra"):
            self.bot_instance.extra = {}

        if "transitions" not in self.bot_instance.extra:
            self.bot_instance.extra["transitions"] = {}

        self.bot_instance.extra["transitions"][self.func.__name__] = transition_data
        return self


class Bot(RunnerAbstraction, DeployableMixin):
    """
    Parameters:
        model (Optional[str]):
            Which model to use for the bot. Default is "gpt-4o".
        api_key (str):
            OpenAI API key to use for the bot. In the future this will support other LLM providers.
        locations (Optional[List[BotLocation]]):
            A list of locations where the bot can store markers. Default is [].
        description (Optional[str]):
            A description of the bot. Default is None.
        volumes (Optional[List[Volume]]):
            A list of volumes to mount in bot transitions. Default is None.
        authorized (bool):
            If false, allows the bot to be invoked without an auth token.
            Default is True.
        welcome_message (Optional[str]):
            A welcome message to display to a user when a new session with the bot is started. Default is None.
    """

    deployment_stub_type = BOT_DEPLOYMENT_STUB_TYPE
    base_stub_type = BOT_STUB_TYPE

    VALID_MODELS = [
        "gpt-4o",
        "gpt-4o-mini",
        "gpt-4",
        "gpt-3.5-turbo",
        "gpt-3.5-turbo-instruct",
        "gpt-3.5-turbo-16k",
        "gpt-3.5-turbo-0613",
        "gpt-4-0613",
    ]

    def __init__(
        self,
        model: str = "gpt-4o",
        api_key: str = "",
        locations: List[BotLocation] = [],
        description: Optional[str] = None,
        volumes: Optional[List[Volume]] = None,
        authorized: bool = True,
        welcome_message: Optional[str] = None,
    ) -> None:
        super().__init__(volumes=volumes)

        if not api_key or api_key == "":
            raise ValueError("API key is required")

        if model not in self.VALID_MODELS:
            raise ValueError(
                f"Invalid model name: {model}. We currently only support: {', '.join(self.VALID_MODELS)}"
            )

        self.is_websocket = True
        self._bot_stub: Optional[BotServiceStub] = None
        self.syncer: FileSyncer = FileSyncer(self.gateway_stub)
        self.locations: List[BotLocation] = locations
        self.parent: "Bot" = self

        self.extra: Dict[str, Dict[str, dict]] = {}
        self.extra["model"] = model
        self.extra["locations"] = {}
        self.extra["description"] = description
        self.extra["api_key"] = api_key
        self.extra["authorized"] = authorized
        self.extra["welcome_message"] = welcome_message

        for location in self.locations:
            location_config = location.to_dict()
            self.extra["locations"][location.name] = location_config

    @property
    def bot_stub(self) -> BotServiceStub:
        if not self._bot_stub:
            self._bot_stub = BotServiceStub(self.channel)
        return self._bot_stub

    @bot_stub.setter
    def bot_stub(self, value: BotServiceStub) -> None:
        self._bot_stub = value

    def transition(self, *args, **kwargs) -> BotTransition:
        return BotTransition(*args, **kwargs, bot_instance=self)

    def set_handler(self, handler: str):
        self.handler = handler

    def func(self, *args: Any, **kwargs: Any):
        pass

    @with_grpc_error_handling
    def serve(self, timeout: int = 0, url_type: str = ""):
        if not self.prepare_runtime(
            func=None, stub_type=BOT_SERVE_STUB_TYPE, force_create_stub=True
        ):
            return False

        try:
            res = self.print_invocation_snippet(url_type=url_type)
            return self._serve(url=res.url, timeout=timeout)
        except KeyboardInterrupt:
            self._handle_serve_interrupt()

    def _serve(self, *, url: str, timeout: int = 0):
        def _connect_to_session():
            session_event = threading.Event()

            import websocket
            from prompt_toolkit import PromptSession

            def on_message(ws, message):
                event = BotEvent(**json.loads(message))
                event_type = event.type
                event_value = event.value

                if event_type == BotEventType.SESSION_CREATED:
                    session_id = event_value
                    terminal.header(f"Session started: {session_id}")
                    terminal.header("💬 Chat with your bot below...")
                    session_event.set()  # Signal that session is ready
                elif event_type == BotEventType.NETWORK_STATE:
                    pass
                else:
                    terminal.print(f"\n{json.dumps(event.model_dump(), indent=2)}")

            def on_error(ws, error):
                terminal.error(f"Error: {error}")

            def on_close(ws, close_status_code, close_msg):
                pass

            def on_open(ws):
                def _send_user_input():
                    with terminal.progress("Waiting for session to start..."):
                        session_event.wait()  # Wait until session is ready

                    session = PromptSession()
                    while True:
                        try:
                            msg = session.prompt("# ")
                            if msg:
                                ws.send(
                                    BotEvent(
                                        type=BotEventType.USER_MESSAGE,
                                        value=msg,
                                        metadata={"request_id": str(uuid.uuid4())},
                                    ).model_dump_json()
                                )
                        except KeyboardInterrupt:
                            confirm = session.prompt("# Exit chat session (y/n) ")
                            if confirm.strip().lower() == "y":
                                ws.close()
                                break
                            else:
                                continue  # Return to the prompt
                        except BaseException as e:
                            terminal.error(f"An error occurred: {e}")
                            continue

                threading.Thread(target=_send_user_input, daemon=True).start()

            ws = websocket.WebSocketApp(
                url,
                on_open=on_open,
                on_message=on_message,
                on_error=on_error,
                on_close=on_close,
                header={"Authorization": f"Bearer {self.config_context.token}"},
            )
            ws.run_forever()

        _connect_to_session()

        terminal.warn("Bot serve session exited. All containers have been stopped.")

    def _handle_serve_interrupt(self) -> None:
        terminal.header("Stopping all bot containers")
        terminal.print("Goodbye 👋")
        os._exit(0)

================================================================================
# File: abstractions/experimental/bot/marker.py
# Path: src/beta9/abstractions/experimental/bot/marker.py
================================================================================

import json
from typing import Any, Type, Union

from pydantic import BaseModel


class BotLocation:
    def __init__(self, *, marker: Union[Type[BaseModel], Any], expose: bool = True) -> None:
        if not issubclass(marker, BaseModel):
            raise TypeError("marker must be a subclass of a pydantic BaseModel")

        self.marker: Type[BaseModel] = marker
        self.expose: bool = expose
        self.name: str = marker.__name__

    def to_dict(self) -> dict:
        """
        Convert BotLocation instance to a dictionary.
        """
        marker_spec = {
            str(name): str(field.annotation) for name, field in self.marker.__fields__.items()
        }

        return {"name": self.name, "marker": marker_spec, "expose": self.expose}

    def to_json(self) -> str:
        """
        Convert BotLocation instance to a JSON string.
        """
        return json.dumps(self.to_dict(), indent=2)

    def __str__(self):
        """
        Return the class name as a string representation.
        """
        return self.marker.__name__

    def __hash__(self):
        """
        Use the class name for hashing.
        """
        return hash(self.marker.__name__)

    def __eq__(self, other):
        """
        Compare based on class name.
        """
        if isinstance(other, BotLocation):
            return self.marker.__name__ == other.marker.__name__
        return False

================================================================================
# File: abstractions/experimental/bot/types.py
# Path: src/beta9/abstractions/experimental/bot/types.py
================================================================================

import json
import mimetypes
from typing import Any, Optional, Union
from uuid import uuid4

from ....clients.bot import (
    BotServiceStub,
    PushBotEventBlockingRequest,
    PushBotEventBlockingResponse,
    PushBotEventRequest,
)
from ....runner.common import FunctionContext
from .bot import BotEvent, BotEventType

BOT_VOLUME_NAME = "beta9-bot-inputs"


class BotContext(FunctionContext):
    session_id: str = ""
    transition_name: str = ""
    bot_stub: BotServiceStub = None

    @classmethod
    def new(
        cls,
        *,
        config: Any,
        task_id: Optional[str],
        on_start_value: Optional[Any] = None,
        session_id: str = "",
        transition_name: str = "",
        bot_stub: BotServiceStub = None,
    ) -> "BotContext":
        """
        Create a new instance of BotContext, to be passed directly into a bot transition handler
        """

        instance = cls(
            container_id=config.container_id,
            stub_id=config.stub_id,
            stub_type=config.stub_type,
            callback_url=config.callback_url,
            python_version=config.python_version,
            task_id=task_id,
            bind_port=config.bind_port,
            timeout=config.timeout,
            on_start_value=on_start_value,
        )

        instance.session_id = session_id
        instance.transition_name = transition_name
        instance.bot_stub = bot_stub
        return instance

    def push_event(cls, *, event_type: BotEventType, event_value: str):
        """Send an event to the bot (supports all event types)"""

        print(f"Sending bot event<{event_type}> {event_value}")
        cls.bot_stub.push_bot_event(
            PushBotEventRequest(
                stub_id=cls.stub_id,
                session_id=cls.session_id,
                event_type=event_type,
                event_value=event_value,
                metadata={
                    "task_id": cls.task_id,
                    "session_id": cls.session_id,
                    "transition_name": cls.transition_name,
                },
            )
        )

    def prompt(
        cls, msg: str, timeout_seconds: int = 10, wait_for_response=True
    ) -> Union[BotEvent, None]:
        """Send a raw prompt to your model. By default, this will wait for a response for up to timeout_seconds."""

        if not wait_for_response:
            cls.bot_stub.push_bot_event(
                PushBotEventRequest(
                    stub_id=cls.stub_id,
                    session_id=cls.session_id,
                    event_type=BotEventType.TRANSITION_MESSAGE,
                    event_value=msg,
                    metadata={
                        "task_id": cls.task_id,
                        "session_id": cls.session_id,
                        "transition_name": cls.transition_name,
                    },
                )
            )
            return None

        r: PushBotEventBlockingResponse = cls.bot_stub.push_bot_event_blocking(
            PushBotEventBlockingRequest(
                stub_id=cls.stub_id,
                session_id=cls.session_id,
                event_type=BotEventType.TRANSITION_MESSAGE,
                event_value=msg,
                metadata={
                    "task_id": cls.task_id,
                    "session_id": cls.session_id,
                    "transition_name": cls.transition_name,
                },
                timeout_seconds=timeout_seconds,
            )
        )

        if not r.ok:
            return None

        return BotEvent(
            type=r.event.type,
            value=r.event.value,
            metadata=r.event.metadata,
        )

    def say(cls, msg: str):
        """Send a message to the user from the bot"""

        cls.bot_stub.push_bot_event(
            PushBotEventRequest(
                stub_id=cls.stub_id,
                session_id=cls.session_id,
                event_type=BotEventType.AGENT_MESSAGE,
                event_value=msg,
                metadata={
                    "task_id": cls.task_id,
                    "session_id": cls.session_id,
                    "transition_name": cls.transition_name,
                },
            )
        )

    def confirm(cls, *, description: str, timeout_seconds: int = 120) -> bool:
        r: PushBotEventBlockingResponse = cls.bot_stub.push_bot_event_blocking(
            PushBotEventBlockingRequest(
                stub_id=cls.stub_id,
                session_id=cls.session_id,
                event_type=BotEventType.CONFIRM_REQUEST,
                event_value=json.dumps(
                    {
                        "description": description,
                        "timeout_seconds": str(timeout_seconds),
                    }
                ),
                metadata={
                    "task_id": cls.task_id,
                    "session_id": cls.session_id,
                    "transition_name": cls.transition_name,
                },
                timeout_seconds=timeout_seconds,
            )
        )
        if not r.ok:
            return False

        return r.event.value == "true"

    def remember(cls, obj: Any):
        """Store an arbitrary object in the bot's memory (must be JSON serializable)"""

        cls.bot_stub.push_bot_event(
            PushBotEventRequest(
                stub_id=cls.stub_id,
                session_id=cls.session_id,
                event_type=BotEventType.MEMORY_MESSAGE,
                event_value=json.dumps(obj),
                metadata={
                    "task_id": cls.task_id,
                    "session_id": cls.session_id,
                    "transition_name": cls.transition_name,
                },
            )
        )

    def send_file(cls, *, path: str, description: str):
        """Capture a file and send it to the user"""

        from beta9 import Output

        o = Output(path=path)
        o.save()

        filetype, _ = mimetypes.guess_type(path)
        file_msg = {
            "url": o.public_url(),
            "description": description,
            "filetype": filetype,
        }

        cls.bot_stub.push_bot_event(
            PushBotEventRequest(
                stub_id=cls.stub_id,
                session_id=cls.session_id,
                event_type=BotEventType.OUTPUT_FILE,
                event_value=json.dumps(file_msg),
                metadata={
                    "task_id": cls.task_id,
                    "session_id": cls.session_id,
                    "transition_name": cls.transition_name,
                },
            )
        )

    def get_file(cls, *, description: str, timeout_seconds: Optional[int] = -1) -> Union[str, None]:
        """
        Request a file from the user.

        Args:
            description (str): Description of the requested file to be displayed to the user.
            timeout_seconds (int): Time to wait for the file in seconds. -1 for no timeout.

        Returns:
            str or None: Path to the file in the container.
        """

        file_id = uuid4().hex
        r: PushBotEventBlockingResponse = cls.bot_stub.push_bot_event_blocking(
            PushBotEventBlockingRequest(
                stub_id=cls.stub_id,
                session_id=cls.session_id,
                event_type=BotEventType.INPUT_FILE_REQUEST,
                event_value=json.dumps(
                    {
                        "description": description,
                        "file_id": file_id,
                        "timeout_seconds": str(timeout_seconds),
                        "volume_path": f"{BOT_VOLUME_NAME}/{cls.session_id}/{file_id}",
                    }
                ),
                metadata={
                    "task_id": cls.task_id,
                    "session_id": cls.session_id,
                    "transition_name": cls.transition_name,
                },
                timeout_seconds=timeout_seconds,
            )
        )
        if not r.ok:
            return None

        return r.event.value

================================================================================
# File: abstractions/experimental/signal.py
# Path: src/beta9/abstractions/experimental/signal.py
================================================================================

import threading
import time
from typing import Callable, Optional, Union

from ...clients.signal import (
    SignalClearRequest,
    SignalClearResponse,
    SignalMonitorRequest,
    SignalMonitorResponse,
    SignalServiceStub,
    SignalSetRequest,
    SignalSetResponse,
)
from ...env import called_on_import
from ..base import BaseAbstraction


class Signal(BaseAbstraction):
    def __init__(
        self,
        *,
        name: str,
        handler: Optional[Callable] = None,
        clear_after_interval: Optional[int] = -1,
    ) -> None:
        """

        Creates a Signal Instance. Signals can be used to notify a container to do something using a flag.
        For example, you may want to reload some global state, send a webhook, or exit the container.
        Signals are an experimental feature.

        Parameters:
        name (str):
            The name of the signal.
        handler (Optional[Callable]):
            If set, this function will be called when the signal is set. Default is None.
        clear_after_interval (Optional[int]):
            If both handler and clear_after_interval are set, the signal will be automatically cleared after
            'clear_after_interval' seconds.

        Example usage:
            ```

            # This is how you would set up a consumer of a signal:

            s = Signal(name="reload-model", handler=reload_model, clear_after_interval=5)
            some_global_model = None

            def load_model():
                global some_global_model
                some_global_model = LoadIt()

            @endpoint(on_start=load_model)
            def handler(**kwargs):
                global some_global_model
                return some_global_model(kwargs["param1"])

            # To trigger load_model to happen again while the container is still running, another process can run:
            s = Signal(name="reload-model")
            s.set(ttl=60)

            ```
        """
        super().__init__()

        self.name: str = name
        self.handler: Union[Callable, None] = handler
        self._stub: Optional[SignalServiceStub] = None
        self.clear_after_interval: Optional[int] = clear_after_interval

        if self.handler is not None and called_on_import():
            threading.Thread(
                target=self._monitor,
                daemon=True,
            ).start()

    @property
    def stub(self) -> SignalServiceStub:
        if not self._stub:
            self._stub = SignalServiceStub(self.channel)
        return self._stub

    @stub.setter
    def stub(self, value: SignalServiceStub):
        self._stub = value

    def set(self, ttl: Optional[int] = 600) -> bool:
        """Fires an event to another container to notify the container that something has occurred"""
        r: SignalSetResponse = self.stub.signal_set(SignalSetRequest(name=self.name, ttl=ttl))
        return r.ok

    def clear(self) -> bool:
        """Removes the signal flag that has been set. This supersedes clear_after_interval, if set"""
        r: SignalClearResponse = self.stub.signal_clear(SignalClearRequest(name=self.name))
        return r.ok

    def _monitor(self) -> None:
        for response in self.stub.signal_monitor(
            SignalMonitorRequest(
                name=self.name,
            )
        ):
            response: SignalMonitorResponse
            if response.set:
                print(f"Signal '{self.name}' received, running handler: {self.handler}")
                self.handler()

            if self.clear_after_interval > 0:
                time.sleep(self.clear_after_interval)
                self.clear()

================================================================================
# File: abstractions/function.py
# Path: src/beta9/abstractions/function.py
================================================================================

import concurrent.futures
import inspect
import os
from datetime import datetime, timezone
from typing import Any, Callable, Dict, Iterator, List, Optional, Sequence, Union

import cloudpickle

from .. import terminal
from ..abstractions.base.runner import (
    FUNCTION_DEPLOYMENT_STUB_TYPE,
    FUNCTION_STUB_TYPE,
    SCHEDULE_DEPLOYMENT_STUB_TYPE,
    SCHEDULE_STUB_TYPE,
    AbstractCallableWrapper,
    RunnerAbstraction,
)
from ..abstractions.image import Image
from ..abstractions.volume import CloudBucket, Volume
from ..channel import with_grpc_error_handling
from ..clients.function import (
    FunctionInvokeRequest,
    FunctionInvokeResponse,
    FunctionScheduleRequest,
    FunctionServiceStub,
)
from ..env import called_on_import, is_local
from ..schema import Schema
from ..sync import FileSyncer
from ..type import GpuType, GpuTypeAlias, PricingPolicy, TaskPolicy
from .mixins import DeployableMixin


class Function(RunnerAbstraction):
    """
    Decorator which allows you to run the decorated function in a remote container.

    Parameters:
        app (str):
            Assign the function to an app. If the app does not exist, it will be created with the given name.
            An app is a group of resources (endpoints, task queues, functions, etc).
        cpu (Union[int, float, str]):
            The number of CPU cores allocated to the container. Default is 1.0.
        memory (Union[int, str]):
            The amount of memory allocated to the container. It should be specified in
            MiB, or as a string with units (e.g. "1Gi"). Default is 128 MiB.
        gpu (Union[GpuTypeAlias, List[GpuTypeAlias]]):
            The type or name of the GPU device to be used for GPU-accelerated tasks. If not
            applicable or no GPU required, leave it empty.
            You can specify multiple GPUs by providing a list of GpuTypeAlias. If you specify several GPUs,
            the scheduler prioritizes their selection based on their order in the list.
        gpu_count (int):
            The number of GPUs allocated to the container. Default is 0. If a GPU is
            specified but this value is set to 0, it will be automatically updated to 1.
        image (Union[Image, dict]):
            The container image used for the task execution. Default is [Image](#image).
        timeout (Optional[int]):
            The maximum number of seconds a task can run before it times out.
            Default is 3600. Set it to -1 to disable the timeout.
        retries (Optional[int]):
            The maximum number of times a task will be retried if the container crashes. Default is 3.
        callback_url (Optional[str]):
            An optional URL to send a callback to when a task is completed, timed out, or cancelled.
        volumes (Optional[List[Union[Volume, CloudBucket]]]):
            A list of storage volumes and/or cloud buckets to be associated with the function. Default is [].
        secrets (Optional[List[str]):
            A list of secrets that are injected into the container as environment variables. Default is [].
        env (Optional[Dict[str, str]]):
            A dictionary of environment variables to be injected into the container. Default is {}.
        name (Optional[str]):
            An optional app name for this function. If not specified, it will be the name of the
            working directory containing the python file with the decorated function.
        task_policy (TaskPolicy):
            The task policy for the function. This helps manage the lifecycle of an individual task.
            Setting values here will override timeout and retries.
        headless (bool):
            Determines whether the function continues running in the background after the client disconnects. Default: False.
        inputs (Optional[Schema]):
            The input schema for the function. Default is None.
        outputs (Optional[Schema]):
            The output model for the function. Default is None.
    Example:
        ```python
        from beta9 import function, Image

        @function(cpu=1.0, memory=128, gpu="T4", image=Image(python_packages=["torch"]), keep_warm_seconds=1000)
        def transcribe(filename: str):
            print(filename)
            return "some_result"

        # Call a function in a remote container
        function.remote("some_file.mp4")

        # Map the function over several inputs
        # Each of these inputs will be routed to remote containers
        for result in function.map(["file1.mp4", "file2.mp4"]):
            print(result)
        ```
    """

    def __init__(
        self,
        app: str = "",
        cpu: Union[int, float, str] = 1.0,
        memory: Union[int, str] = 128,
        gpu: Union[GpuTypeAlias, List[GpuTypeAlias]] = GpuType.NoGPU,
        gpu_count: int = 0,
        image: Image = Image(),
        timeout: int = 3600,
        retries: int = 3,
        callback_url: Optional[str] = None,
        volumes: Optional[List[Union[Volume, CloudBucket]]] = None,
        secrets: Optional[List[str]] = None,
        env: Optional[Dict[str, str]] = {},
        name: Optional[str] = None,
        task_policy: TaskPolicy = TaskPolicy(),
        on_deploy: Optional[AbstractCallableWrapper] = None,
        headless: bool = False,
        pricing: Optional[PricingPolicy] = None,
        inputs: Optional[Schema] = None,
        outputs: Optional[Schema] = None,
    ) -> None:
        super().__init__(
            cpu=cpu,
            memory=memory,
            gpu=gpu,
            gpu_count=gpu_count,
            image=image,
            timeout=timeout,
            retries=retries,
            callback_url=callback_url,
            volumes=volumes,
            secrets=secrets,
            env=env,
            name=name,
            task_policy=task_policy,
            on_deploy=on_deploy,
            app=app,
            pricing=pricing,
            inputs=inputs,
            outputs=outputs,
        )

        self._function_stub: Optional[FunctionServiceStub] = None
        self.syncer: FileSyncer = FileSyncer(self.gateway_stub)
        self.headless = headless

    def __call__(self, func):
        return _CallableWrapper(func, self)

    @property
    def function_stub(self) -> FunctionServiceStub:
        if not self._function_stub:
            self._function_stub = FunctionServiceStub(self.channel)
        return self._function_stub

    @function_stub.setter
    def function_stub(self, value: FunctionServiceStub) -> None:
        self._function_stub = value


class _CallableWrapper(DeployableMixin):
    base_stub_type = FUNCTION_STUB_TYPE
    deployment_stub_type = FUNCTION_DEPLOYMENT_STUB_TYPE

    def __init__(self, func: Callable, parent: Function) -> None:
        self.func: Callable = func
        self.parent: Function = parent

    @with_grpc_error_handling
    def __call__(self, *args, **kwargs) -> Any:
        if called_on_import():
            return

        if not is_local():
            return self.local(*args, **kwargs)

        try:
            if not self.parent.prepare_runtime(
                func=self.func,
                stub_type=self.base_stub_type,
            ):
                return
        except KeyboardInterrupt:
            terminal.error("Exiting shell. Your build was stopped.")

        try:
            with terminal.progress("Working..."):
                return self._call_remote(*args, **kwargs)
        except KeyboardInterrupt:
            if self.parent.headless:
                terminal.error("Exiting shell. Your function will continue running remotely.")
            else:
                terminal.error("Exiting shell. Your function will be terminated.")

    @with_grpc_error_handling
    def _call_remote(self, *args, **kwargs) -> Any:
        args = cloudpickle.dumps(
            {
                "args": args,
                "kwargs": kwargs,
            },
        )

        terminal.header(f"Running function: <{self.parent.handler}>")
        last_response: Optional[FunctionInvokeResponse] = None

        for r in self.parent.function_stub.function_invoke(
            FunctionInvokeRequest(
                stub_id=self.parent.stub_id,
                args=args,
                headless=self.parent.headless,
            )
        ):
            if r.output != "":
                terminal.detail(r.output, end="")

            if r.done or r.exit_code != 0:
                last_response = r
                break

        if last_response is None or not last_response.done or last_response.exit_code != 0:
            terminal.error(f"Function failed <{last_response.task_id}> ❌", exit=False)
            return

        terminal.header(f"Function complete <{last_response.task_id}>")
        # Sometimes the result is empty (task timed out)
        if not last_response.result:
            return None
        return cloudpickle.loads(last_response.result)

    def local(self, *args, **kwargs) -> Any:
        return self.func(*args, **kwargs)

    def remote(self, *args, **kwargs) -> Any:
        return self(*args, **kwargs)

    def serve(self, **kwargs):
        terminal.error("Serve has not been implemented for functions.")

    def _format_args(self, args):
        if isinstance(args, tuple):
            return list(args)
        elif not isinstance(args, list):
            return [args]
        return args

    def _threaded_map(self, inputs: Sequence[Any]) -> Iterator[Any]:
        with terminal.progress(f"Running {len(inputs)} container(s)..."):
            with concurrent.futures.ThreadPoolExecutor(max_workers=len(inputs)) as pool:
                futures = [
                    pool.submit(self._call_remote, *self._format_args(args)) for args in inputs
                ]
                try:
                    for future in concurrent.futures.as_completed(futures):
                        try:
                            yield future.result()
                        except Exception as e:
                            terminal.error(f"Task failed during map: {e}", exit=False)
                            yield None
                except KeyboardInterrupt:
                    pool.shutdown(wait=False, cancel_futures=True)
                    terminal.error(
                        f"Exiting shell. Mapped functions will {'be terminated.' if not self.parent.headless else 'continue running.'}",
                        exit=False,
                    )
                    os._exit(1)

    def map(self, inputs: Sequence[Any]) -> Iterator[Any]:
        if not self.parent.prepare_runtime(
            func=self.func,
            stub_type=self.base_stub_type,
        ):
            terminal.error("Function failed to prepare runtime ❌")

        iterator = self._threaded_map(inputs)
        yield from iterator


class ScheduleWrapper(_CallableWrapper):
    base_stub_type = SCHEDULE_STUB_TYPE
    deployment_stub_type = SCHEDULE_DEPLOYMENT_STUB_TYPE

    def deploy(self, *args: Any, **kwargs: Any) -> bool:
        deployed = super().deploy(invocation_details_func=self.invocation_details, *args, **kwargs)
        if deployed:
            res = self.parent.function_stub.function_schedule(
                FunctionScheduleRequest(
                    stub_id=self.parent.stub_id,
                    when=self.parent.when,
                    deployment_id=self.parent.deployment_id,
                )
            )
            if not res.ok:
                terminal.error(res.err_msg, exit=False)
                return False
        return deployed

    def invocation_details(self, **kwargs) -> None:
        """
        Print the schedule details.

        Used as an alternative view when deploying a scheduled function.
        """
        from croniter import croniter

        terminal.header("Schedule details")
        terminal.print(f"Schedule: {self.parent.when}")
        terminal.print("Upcoming:")

        local_tz = datetime.now().astimezone().tzinfo

        cron = croniter(self.parent.when, datetime.now(local_tz))
        cron_utc = croniter(self.parent.when, datetime.now(timezone.utc))
        for i in range(3):
            next_run = cron.get_next(datetime)
            next_run_utc = cron_utc.get_next(datetime)
            terminal.print(
                (
                    f"  [bright_white]{i + 1}.[/bright_white] {next_run_utc:%Y-%m-%d %H:%M:%S %Z} "
                    f"({next_run:%Y-%m-%d %H:%M:%S} {local_tz})"
                )
            )


class Schedule(Function):
    """
    Decorator which allows you to run the decorated function as a scheduled job.

    Parameters:
        app (str):
            Assign the scheduled function to an app. If the app does not exist, it will be created with the given name.
            An app is a group of resources (endpoints, task queues, functions, etc).
        when (str):
            A cron expression that specifies when the task should be run. For example "*/5 * * * *".
            The timezone is always UTC.
        cpu (Union[int, float, str]):
            The number of CPU cores allocated to the container. Default is 1.0.
        memory (Union[int, str]):
            The amount of memory allocated to the container. It should be specified in
            MiB, or as a string with units (e.g. "1Gi"). Default is 128 MiB.
        gpu (Union[GpuType, str]):
            The type or name of the GPU device to be used for GPU-accelerated tasks. If not
            applicable or no GPU required, leave it empty. Default is [GpuType.NoGPU](#gputype).
        gpu_count (int):
            The number of GPUs allocated to the container. Default is 0. If a GPU is
            specified but this value is set to 0, it will be automatically updated to 1.
        image (Union[Image, dict]):
            The container image used for the task execution. Default is [Image](#image).
        timeout (Optional[int]):
            The maximum number of seconds a task can run before it times out.
            Default is 3600. Set it to -1 to disable the timeout.
        retries (Optional[int]):
            The maximum number of times a task will be retried if the container crashes. Default is 3.
        callback_url (Optional[str]):
            An optional URL to send a callback to when a task is completed, timed out, or cancelled.
        volumes (Optional[List[Union[Volume, CloudBucket]]]):
            A list of storage volumes and/or cloud buckets to be associated with the function. Default is [].
        secrets (Optional[List[str]):
            A list of secrets that are injected into the container as environment variables. Default is [].
        env (Optional[Dict[str, str]]):
            A dictionary of environment variables to be injected into the container. Default is {}.
        name (Optional[str]):
            An optional app name for this function. If not specified, it will be the name of the
            working directory containing the python file with the decorated function.

    Example:
        ```python
        from beta9 import schedule

        @schedule(when="*/5 * * * *")
        def task():
            print("Hi, from scheduled task!")

        ```

    Predefined schedules:
        These can be used in the `when` parameter.

        @yearly (or @annually)  Run once a year at midnight of 1 January                    0 0 1 1 *
        @monthly                Run once a month at midnight of the first day of the month  0 0 1 * *
        @weekly                 Run once a week at midnight on Sunday                       0 0 * * 0
        @daily (or @midnight)   Run once a day at midnight                                  0 0 * * *
        @hourly                 Run once an hour at the beginning of the hour               0 * * * *
    """

    def __init__(
        self,
        when: str,
        app: str = "",
        cpu: Union[int, float, str] = 1.0,
        memory: Union[int, str] = 128,
        gpu: GpuTypeAlias = GpuType.NoGPU,
        gpu_count: int = 0,
        image: Image = Image(),
        timeout: int = 3600,
        retries: int = 3,
        callback_url: Optional[str] = None,
        volumes: Optional[List[Union[Volume, CloudBucket]]] = None,
        secrets: Optional[List[str]] = None,
        env: Optional[Dict[str, str]] = {},
        name: Optional[str] = None,
        on_deploy: Optional[AbstractCallableWrapper] = None,
    ) -> None:
        params = inspect.signature(Function.__init__).parameters
        kwargs = {k: v for k, v in locals().items() if k in params and k != "self"}
        super().__init__(**kwargs)

        self.when = when

    def __call__(self, func) -> ScheduleWrapper:
        return ScheduleWrapper(func, self)

================================================================================
# File: abstractions/image.py
# Path: src/beta9/abstractions/image.py
================================================================================

import os
import sys
from pathlib import Path
from typing import Dict, List, Literal, NamedTuple, Optional, Sequence, Tuple, TypedDict, Union

from beta9.clients.gateway import GatewayServiceStub
from beta9.sync import FileSyncer

from .. import env, terminal
from ..abstractions.base import BaseAbstraction
from ..clients.image import (
    BuildImageRequest,
    BuildImageResponse,
    BuildStep,
    ImageServiceStub,
    VerifyImageBuildRequest,
    VerifyImageBuildResponse,
)
from ..env import is_notebook_env
from ..type import GpuType, GpuTypeAlias, PythonVersion, PythonVersionAlias

LOCAL_PYTHON_VERSION = f"python{sys.version_info.major}.{sys.version_info.minor}"


class ImageBuildResult(NamedTuple):
    success: bool = False
    image_id: str = ""
    python_version: str = ""


class ImageCredentialValueNotFound(Exception):
    def __init__(self, key_name: str, *args: object) -> None:
        super().__init__(*args)
        self.key_name = key_name

    def __str__(self) -> str:
        return f"Did not find the environment variable {self.key_name}. Did you forget to set it?"


class AWSCredentials(TypedDict, total=False):
    """Amazon Web Services credentials"""

    AWS_ACCESS_KEY_ID: str
    AWS_SECRET_ACCESS_KEY: str
    AWS_SESSION_TOKEN: str
    AWS_REGION: str


class GCPCredentials(TypedDict, total=False):
    """Google Cloud Platform credentials"""

    GCP_ACCESS_TOKEN: str


class DockerHubCredentials(TypedDict, total=False):
    """Docker Hub credentials"""

    DOCKERHUB_USERNAME: str
    DOCKERHUB_PASSWORD: str


class NGCCredentials(TypedDict, total=False):
    """NVIDIA GPU Cloud credentials"""

    NGC_API_KEY: str


ImageCredentialKeys = Literal[
    "AWS_ACCESS_KEY_ID",
    "AWS_SECRET_ACCESS_KEY",
    "AWS_SESSION_TOKEN",
    "AWS_REGION",
    "DOCKERHUB_USERNAME",
    "DOCKERHUB_PASSWORD",
    "GCP_ACCESS_TOKEN",
    "NGC_API_KEY",
]

ImageCredentials = Union[
    AWSCredentials,
    DockerHubCredentials,
    GCPCredentials,
    NGCCredentials,
    List[ImageCredentialKeys],
]


def detected_python_version() -> PythonVersion:
    try:
        return PythonVersion(LOCAL_PYTHON_VERSION)
    except ValueError:
        return PythonVersion.Python3


class Image(BaseAbstraction):
    """
    Defines a custom container image that your code will run in.
    """

    def __init__(
        self,
        python_version: PythonVersionAlias = PythonVersion.Python3,
        python_packages: Union[List[str], str] = [],
        commands: List[str] = [],
        base_image: Optional[str] = None,
        base_image_creds: Optional[ImageCredentials] = None,
        env_vars: Optional[Union[str, List[str], Dict[str, str]]] = None,
    ):
        """
        Creates an Image instance.

        An Image object encapsulates the configuration of a custom container image
        that will be used as the runtime environment for executing tasks.

        If the `python_packages` variable is set, it will always run before `commands`.
        To control the order of execution, use the `add_commands` and `add_python_packages`
        methods. These will be executed in the order they are added.

        Parameters:
            python_version (Union[PythonVersion, str]):
                The Python version to be used in the image. Default is set to [PythonVersion.Python3](#pythonversion).
                When using [PythonVersion.Python3](#pythonversion), whatever version of Python 3 exists in the image will be used.
                If none exists, Python 3.10 will be installed. When running in a notebook environment without a custom base image,
                the Python version detected in the local environment will be used if compatible. Otherwise, Python 3.10 will be installed.
            python_packages (Union[List[str], str]):
                A list of Python packages to install in the container image. Alternatively, a string
                containing a path to a requirements.txt can be provided. Default is [].
            commands (List[str]):
                A list of shell commands to run when building your container image. These commands
                can be used for setting up the environment, installing dependencies, etc.
                Default is [].
            base_image (Optional[str]):
                A custom base image to replace the default ubuntu20.04 image used in your container.
                This can be a public or private image from Docker Hub, Amazon ECR, Google Cloud Artifact Registry, or
                NVIDIA GPU Cloud Registry. The formats for these registries are respectively `docker.io/my-org/my-image:0.1.0`,
                `111111111111.dkr.ecr.us-east-1.amazonaws.com/my-image:latest`,
                `us-east4-docker.pkg.dev/my-project/my-repo/my-image:0.1.0`, and `nvcr.io/my-org/my-repo:0.1.0`.
                Default is None.
            base_image_creds (Optional[ImageCredentials]):
                A key/value pair or key list of environment variables that contain credentials to
                a private registry. When provided as a dict, you must supply the correct keys and values.
                When provided as a list, the keys are used to lookup the environment variable value
                for you. Default is None.
            env_vars (Optional[Union[str, List[str], Dict[str, str]]):
                Adds environment variables to an image. These will be available when building the image
                and when the container is running. This can be a string, a list of strings, or a
                dictionary of strings. The string must be in the format of "KEY=VALUE". If a list of
                strings is provided, each element should be in the same format. Deafult is None.

        Example:

            Docker Hub

            To use a private image from Docker Hub, export your Docker Hub credentials.

            ```sh
            export DOCKERHUB_USERNAME=user123
            export DOCKERHUB_PASSWORD=pass123
            ```

            Then configure the Image object with those environment variables.

            ```python
            image = Image(
                python_version="python3.12",
                base_image="docker.io/my-org/my-image:0.1.0",
                base_image_creds=["DOCKERHUB_USERNAME", "DOCKERHUB_PASSWORD"],
            )

            @endpoint(image=image)
            def handler():
                pass
            ```

            Amazon Elastic Container Registry (ECR)

            To use a private image from Amazon ECR, export your AWS environment variables.
            Then configure the Image object with those environment variables.

            ```python
            image = Image(
                python_version="python3.12",
                base_image="111111111111.dkr.ecr.us-east-1.amazonaws.com/my-image:latest,
                base_image_creds=["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY", "AWS_REGION"],
            )

            @endpoint(image=image)
            def handler():
                pass
            ```

            Google Artifact Registry (GAR)

            To use a private image from Google Artifact Registry, export your access token.

            ```sh
            export GCP_ACCESS_TOKEN=$(gcloud auth print-access-token --project=my-project)
            ```

            Then configure the Image object to use the environment variable.

            ```python
            image = Image(
                python_version="python3.12",
                base_image="us-east4-docker.pkg.dev/my-project/my-repo/my-image:0.1.0",
                base_image_creds=["GCP_ACCESS_TOKEN"],
            )

            @endpoint(image=image)
            def handler():
                pass

            NVIDIA GPU Cloud (NGC)

            To use a private image from NVIDIA GPU Cloud, export your API key.

            ```sh
            export NGC_API_KEY=abc123
            ```

            Then configure the Image object to use the environment variable.

            ```python
            image = Image(
                python_version="python3.12",
                base_image="nvcr.io/nvidia/tensorrt:24.10-py3",
                base_image_creds=["NGC_API_KEY"],
            )

            @endpoint(image=image)
            def handler():
                pass
            ```

            Custom Dockerfile

            To use a custom Dockerfile, you can use the `from_dockerfile` class method.
            This will build the image using your Dockerfile. You can set the docker build's
            context directory using the `context_dir` parameter.

            The context directory should contain all files referenced in your Dockerfile
            (like files being COPYed). If no context_dir is specified, the directory
            containing the Dockerfile will be used as the context.

            ```python
            # Basic usage - uses Dockerfile's directory as context
            image = Image.from_dockerfile("path/to/Dockerfile")

            # Specify a different context directory
            image = Image.from_dockerfile(
                "path/to/Dockerfile",
                context_dir="path/to/context"
            )

            # You can still chain additional commands and python packages
            image.add_commands(["echo 'Hello, World!'"]).add_python_packages(["numpy"])

            @endpoint(image=image)
            def handler():
                pass
            ```

            Building in a GPU environment

            By default, the image will be built on a CPU node. If you need to build on a GPU node,
            you can set the `gpu` parameter to the GPU type you need. This might be necessary if you
            are using a library or framework that will install differently depending on the availability
            of a GPU.

            ```python
            image = Image(
                python_version="python3.12",
            ).build_with_gpu("A10G")
            ```
        """
        super().__init__()
        self._gateway_stub: Optional[GatewayServiceStub] = None

        if isinstance(python_packages, str):
            python_packages = self._load_requirements_file(python_packages)

        # Only attempt to detect an appropriate default python version if we are in a notebook environment
        # and there is no base image (it might provide a required python version that we will have to detect)
        if is_notebook_env() and base_image is None:
            python_version = detected_python_version()

        self.python_version = python_version
        self.python_packages = self._sanitize_python_packages(python_packages)
        self.commands = commands
        self.build_steps = []
        self.base_image = base_image or ""
        self.base_image_creds = base_image_creds or {}
        self.env_vars = []
        self.secrets = []
        self._stub: Optional[ImageServiceStub] = None
        self.dockerfile = ""
        self.build_ctx_object = ""
        self.gpu = GpuType.NoGPU
        self.ignore_python = False

        self.with_envs(env_vars or [])

    @property
    def gateway_stub(self) -> GatewayServiceStub:
        if not self._gateway_stub:
            self._gateway_stub = GatewayServiceStub(self.channel)
        return self._gateway_stub

    @gateway_stub.setter
    def gateway_stub(self, value) -> None:
        self._gateway_stub = value

    @property
    def stub(self) -> ImageServiceStub:
        if not self._stub:
            self._stub = ImageServiceStub(self.channel)
        return self._stub

    def __eq__(self: "Image", other: "Image"):
        return (
            self.python_version == other.python_version
            and self.python_packages == other.python_packages
            and self.base_image == other.base_image
            and self.base_image_creds == other.base_image_creds
        )

    def __str__(self) -> str:
        return f"Python Version: {self.python_version}, Python Packages: {self.python_packages}, Base Image: {self.base_image}, Base Image Credentials: {self.base_image_creds}"

    def _sanitize_python_packages(self, packages: List[str]) -> List[str]:
        # https://pip.pypa.io/en/stable/reference/requirements-file-format/
        prefix_exceptions = ["--", "-"]
        sanitized = []
        for p in packages:
            if any(p.startswith(prefix) for prefix in prefix_exceptions):
                sanitized.append(p)
            elif p.startswith("#"):
                continue
            else:
                sanitized.append(p.replace(" ", ""))
        return sanitized

    def _load_requirements_file(self, path: str) -> List[str]:
        requirements_file = Path(path)

        if requirements_file.is_file():
            with open(requirements_file, "r") as f:
                contents = f.read()
                lines = contents.split("\n")
                lines = list(filter(lambda r: r != "", lines))
                return lines
        else:
            raise FileNotFoundError

    @classmethod
    def from_dockerfile(cls, path: str, context_dir: Optional[str] = None) -> "Image":
        """
        Build the base image based on a Dockerfile.

        This method will sync the context directory and use the Dockerfile at the provided path to
        build the base image.

        Parameters:
            path: The path to the Dockerfile.
            context_dir: The directory to sync. If not provided, the directory of the Dockerfile will be used.

        Returns:
            Image: The Image object.
        """
        image = cls()
        if env.is_remote():
            return image

        if not context_dir:
            context_dir = os.path.dirname(path)

        syncer = FileSyncer(gateway_stub=image.gateway_stub, root_dir=context_dir)
        result = syncer.sync()
        if not result.success:
            raise ValueError("Failed to sync context directory.")

        image.build_ctx_object = result.object_id

        with open(path, "r") as f:
            dockerfile = f.read()
        image.dockerfile = dockerfile
        return image

    def exists(self) -> Tuple[bool, ImageBuildResult]:
        r: VerifyImageBuildResponse = self.stub.verify_image_build(
            VerifyImageBuildRequest(
                python_packages=self.python_packages,
                python_version=self.python_version,
                commands=self.commands,
                build_steps=self.build_steps,
                force_rebuild=False,
                existing_image_uri=self.base_image,
                env_vars=self.env_vars,
                dockerfile=self.dockerfile,
                build_ctx_object=self.build_ctx_object,
                secrets=self.secrets,
                gpu=self.gpu,
                ignore_python=self.ignore_python,
            )
        )

        return (
            r.exists,
            ImageBuildResult(
                success=r.exists, image_id=r.image_id, python_version=self.python_version
            ),
        )

    def build(self) -> ImageBuildResult:
        terminal.header("Building image")

        if is_notebook_env():
            if LOCAL_PYTHON_VERSION != self.python_version:
                terminal.warn(
                    f"Local version {LOCAL_PYTHON_VERSION.value} differs from image version {self.python_version}. This may cause issues in your remote environment."
                )

        if self.base_image != "" and self.dockerfile != "":
            raise ValueError("Cannot use from_dockerfile and provide a custom base image.")

        exists, exists_response = self.exists()
        if exists:
            terminal.header("Using cached image")
            return ImageBuildResult(
                success=True,
                image_id=exists_response.image_id,
                python_version=exists_response.python_version,
            )

        with terminal.progress("Working..."):
            last_response = BuildImageResponse(success=False)
            for r in self.stub.build_image(
                BuildImageRequest(
                    python_packages=self.python_packages,
                    python_version=self.python_version,
                    commands=self.commands,
                    build_steps=self.build_steps,
                    existing_image_uri=self.base_image,
                    existing_image_creds=self.get_credentials_from_env(),
                    env_vars=self.env_vars,
                    dockerfile=self.dockerfile,
                    build_ctx_object=self.build_ctx_object,
                    secrets=self.secrets,
                    gpu=self.gpu,
                    ignore_python=self.ignore_python,
                )
            ):
                if r.warning:
                    terminal.warn("WARNING: " + r.msg)
                elif r.msg != "" and not r.done:
                    terminal.detail(r.msg, end="")

                if r.done:
                    last_response = r
                    break

        if not last_response.success:
            terminal.error(str(last_response.msg).rstrip(), exit=False)
            return ImageBuildResult(success=False)

        terminal.header("Build complete 🎉")
        return ImageBuildResult(
            success=True,
            image_id=last_response.image_id,
            python_version=last_response.python_version,
        )

    def get_credentials_from_env(self) -> Dict[str, str]:
        if env.is_remote():
            return {}

        keys = (
            self.base_image_creds.keys()
            if isinstance(self.base_image_creds, dict)
            else self.base_image_creds
        )

        creds = {}
        for key in keys:
            if v := os.getenv(key):
                creds[key] = v
            else:
                raise ImageCredentialValueNotFound(key)
        return creds

    def micromamba(self) -> "Image":
        """
        Use micromamba to manage python packages.
        """
        if self.python_version == PythonVersion.Python3:
            self.python_version = PythonVersion.Python311

        self.python_version = self.python_version.replace("python", "micromamba")
        return self

    def add_micromamba_packages(
        self, packages: Union[Sequence[str], str], channels: Sequence[str] = []
    ) -> "Image":
        """
        Add micromamba packages that will be installed when building the image.

        These will be executed at the end of the image build and in the
        order they are added. If a single string is provided, it will be
        interpreted as a path to a requirements.txt file.

        Parameters:
            packages: The micromamba packages to add or the path to a requirements.txt file.
            channels: The micromamba channels to use.
        """
        # Error if micromamba is not enabled
        if not self.python_version.startswith("micromamba"):
            raise ValueError("Micromamba must be enabled to use this method.")

        # Check if we were given a .txt requirement file
        if isinstance(packages, str):
            packages = self._sanitize_python_packages(self._load_requirements_file(packages))

        for package in packages:
            self.build_steps.append(BuildStep(command=package, type="micromamba"))

        for channel in channels:
            self.build_steps.append(BuildStep(command=f"-c {channel}", type="micromamba"))

        return self

    def add_commands(self, commands: Sequence[str]) -> "Image":
        """
        Add shell commands that will be executed when building the image.

        These will be executed at the end of the image build and in the
        order they are added.

        Parameters:
            commands: The shell commands to execute.

        Returns:
            Image: The Image object.
        """
        for command in commands:
            self.build_steps.append(BuildStep(command=command, type="shell"))
        return self

    def add_python_packages(self, packages: Union[Sequence[str], str]) -> "Image":
        """
        Add python packages that will be installed when building the image.

        These will be executed at the end of the image build and in the
        order they are added.

        Parameters:
            packages: The Python packages to add or the path to a requirements.txt file. Valid package names are: numpy, pandas==2.2.2, etc.

        Returns:
            Image: The Image object.
        """

        if isinstance(packages, str):
            try:
                packages = self._sanitize_python_packages(self._load_requirements_file(packages))
            except FileNotFoundError:
                raise ValueError(
                    f"Could not find valid requirements.txt file at {packages}. Libraries must be specified as a list of valid package names or a path to a requirements.txt file."
                )

        for package in packages:
            self.build_steps.append(BuildStep(command=package, type="pip"))
        return self

    def with_envs(
        self, env_vars: Union[str, List[str], Dict[str, str]], clear: bool = False
    ) -> "Image":
        """
        Add environment variables to the image.

        These will be available when building the image and when the container is running.

        Parameters:
            env_vars: Environment variables. This can be a string, a list of strings, or a
                dictionary of strings. The string must be in the format of "KEY=VALUE". If a list of
                strings is provided, each element should be in the same format. Deafult is None.
            clear: Clear existing environment variables before adding the new ones.

        Returns:
            Image: The Image object.
        """
        if isinstance(env_vars, dict):
            env_vars = [f"{key}={value}" for key, value in env_vars.items()]
        elif isinstance(env_vars, str):
            env_vars = [env_vars]

        self.validate_env_vars(env_vars)

        if clear:
            self.env_vars.clear()

        self.env_vars.extend(env_vars)

        return self

    def validate_env_vars(self, env_vars: List[str]) -> None:
        for env_var in env_vars:
            key, sep, value = env_var.partition("=")
            if not sep:
                raise ValueError(f"Environment variable must contain '=': {env_var}")
            if not key:
                raise ValueError(f"Environment variable key cannot be empty: {env_var}")
            if not value:
                raise ValueError(f"Environment variable value cannot be empty: {env_var}")
            if "=" in value:
                raise ValueError(
                    f"Environment variable cannot contain multiple '=' characters: {env_var}"
                )

    def with_secrets(self, secrets: List[str]) -> "Image":
        """
        Adds secrets stored in the platform to the build environment.

        Parameters:
            secrets: The secrets to add.

        Returns:
            Image: The Image object.
        """
        self.secrets.extend(secrets)
        return self

    def build_with_gpu(self, gpu: GpuTypeAlias) -> "Image":
        """
        Build the image on a GPU node.

        Parameters:
            gpu: The GPU type to use.

        Returns:
            Image: The Image object.
        """
        self.gpu = gpu
        return self

================================================================================
# File: abstractions/integrations/__init__.py
# Path: src/beta9/abstractions/integrations/__init__.py
================================================================================

from .fastmcp import MCPServer, MCPServerArgs
from .vllm import VLLM, VLLMArgs

__all__ = ["MCPServer", "MCPServerArgs", "VLLM", "VLLMArgs"]

================================================================================
# File: abstractions/integrations/fastmcp.py
# Path: src/beta9/abstractions/integrations/fastmcp.py
================================================================================

import os
from dataclasses import dataclass
from typing import Any, Callable, List, Optional, Union

from ... import terminal
from ...abstractions.base.container import Container
from ...abstractions.base.runner import ASGI_DEPLOYMENT_STUB_TYPE, ASGI_SERVE_STUB_TYPE
from ...abstractions.endpoint import ASGI
from ...abstractions.image import Image
from ...abstractions.volume import CloudBucket, Volume
from ...channel import with_grpc_error_handling
from ...clients.endpoint import StartEndpointServeRequest, StartEndpointServeResponse
from ...clients.gateway import DeployStubRequest, DeployStubResponse
from ...config import ConfigContext
from ...type import Autoscaler, GpuType, GpuTypeAlias, PricingPolicy, QueueDepthAutoscaler


@dataclass
class MCPServerArgs:
    """
    Configuration for the underlying FastMCP server.
    """

    fastmcp_version: str = "2.7.1"


class MCPServer(ASGI):
    """
    FastMCP is a wrapper around the FastMCP library that allows you to deploy it as an ASGI app.

    Parameters:
        server (FastMCP):
            The FastMCP class instance to serve/deploy.
        args (Optional[MCPServerArgs]):
            The arguments for the underlying FastMCP server. If not specified, the default arguments will be used.
        cpu (Union[int, float, str]):
            The number of CPU cores allocated to the container. Default is 1.0.
        memory (Union[int, str]):
            The amount of memory allocated to the container. It should be specified in
            MiB, or as a string with units (e.g. "1Gi"). Default is 128 MiB.
        gpu (Union[GpuType, str]):
            The type or name of the GPU device to be used for GPU-accelerated tasks. If not
            applicable or no GPU required, leave it empty. Default is [GpuType.NoGPU](#gputype).
        image (Union[Image, dict]):
            The container image used for the task execution. Whatever you pass here will have an additional `add_python_packages` call
            with `["fastapi", "vllm", "huggingface_hub"]` added to it to ensure that we can run vLLM in the container.
        workers (int):
            The number of workers to run in the container. Default is 1.
        concurrent_requests (int):
            The maximum number of concurrent requests to handle. Default is 10.
        keep_warm_seconds (int):
            The number of seconds to keep the container warm after the last request. Default is 60.
        max_pending_tasks (int):
            The maximum number of pending tasks to allow in the container. Default is 100.
        timeout (int):
            The maximum number of seconds to wait for the container to start. Default is 3600.
        authorized (bool):
            Whether the endpoints require authorization. Default is False.
        name (str):
            The name of the MCP server app. Default is none, which means you must provide it during deployment.
        volumes (List[Union[Volume, CloudBucket]]):
            The volumes and/or cloud buckets to mount into the MCP server container. Default is an empty list.
        secrets (List[str]):
            The secrets to pass to the MCP server container.
        autoscaler (Autoscaler):
            The autoscaler to use. Default is a queue depth autoscaler.
        on_start (Callable[..., None]):
            A function to call when the MCP server app is started. Default is None.

    Example:
        ```python
        from beta9 import integrations, MCPServerArgs, MCPServer

        args = MCPServerArgs(fastmcp_version="2.7.1")
        fastmcp_app = MCPServer(name="fastmcp-abstraction-1", fastmcp_args=args)
        ```
    """

    def __init__(
        self,
        server: "FastMCP",  # type: ignore  # noqa: F821
        args: MCPServerArgs = MCPServerArgs(),
        cpu: Union[int, float, str] = 1.0,
        memory: Union[int, str] = 128,
        gpu: Union[GpuTypeAlias, List[GpuTypeAlias]] = GpuType.NoGPU,
        gpu_count: int = 0,
        image: Image = Image(python_version="python3.11"),
        workers: int = 1,
        concurrent_requests: int = 10,
        keep_warm_seconds: int = 60,
        max_pending_tasks: int = 100,
        timeout: int = 3600,
        authorized: bool = False,
        name: Optional[str] = None,
        volumes: Optional[List[Union[Volume, CloudBucket]]] = [],
        secrets: Optional[List[str]] = None,
        autoscaler: Autoscaler = QueueDepthAutoscaler(),
        on_start: Optional[Callable[..., None]] = None,
        pricing: Optional[PricingPolicy] = None,
    ):
        image = image.add_python_packages(
            [
                f"fastmcp=={args.fastmcp_version}",
            ]
        )

        super().__init__(
            cpu=cpu,
            memory=memory,
            gpu=gpu,
            gpu_count=gpu_count,
            image=image,
            workers=workers,
            concurrent_requests=concurrent_requests,
            keep_warm_seconds=keep_warm_seconds,
            max_pending_tasks=max_pending_tasks,
            timeout=timeout,
            authorized=authorized,
            name=name,
            volumes=volumes,
            secrets=secrets,
            autoscaler=autoscaler,
            on_start=on_start,
            pricing=pricing,
        )

        self.fastmcp_server = server
        self.fastmcp_args = args

    def __name__(self) -> str:
        return self.name or f"fastmcp=={self.fastmcp_args.fastmcp_version}"

    def set_handler(self, handler: str):
        self.handler = handler

    def func(self, *args: Any, **kwargs: Any):
        pass

    def __call__(self, *args: Any, **kwargs: Any):
        return self.fastmcp_server.http_app(transport="sse", path="/sse")

    def deploy(
        self,
        name: Optional[str] = None,
        context: Optional[ConfigContext] = None,
        invocation_details_func: Optional[Callable[..., None]] = None,
        **invocation_details_options: Any,
    ) -> bool:
        self.name = name or self.name
        if not self.name:
            terminal.error(
                "You must specify an app name (either in the decorator or via the --name argument)."
            )

        if context is not None:
            self.config_context = context

        if not self.prepare_runtime(
            stub_type=ASGI_DEPLOYMENT_STUB_TYPE,
            force_create_stub=True,
        ):
            return False

        terminal.header("Deploying")
        deploy_response: DeployStubResponse = self.gateway_stub.deploy_stub(
            DeployStubRequest(stub_id=self.stub_id, name=self.name)
        )

        self.deployment_id = deploy_response.deployment_id
        if deploy_response.ok:
            terminal.header("Deployed 🎉")
            if invocation_details_func:
                invocation_details_func(**invocation_details_options)
            else:
                self.print_invocation_snippet(**invocation_details_options)

        return deploy_response.ok

    @with_grpc_error_handling
    def serve(self, timeout: int = 0, url_type: str = ""):
        stub_type = ASGI_SERVE_STUB_TYPE

        if not self.prepare_runtime(func=self.func, stub_type=stub_type, force_create_stub=True):
            return False

        try:
            with terminal.progress("Serving endpoint..."):
                self.print_invocation_snippet(url_type=url_type)
                return self._serve(dir=os.getcwd(), timeout=timeout)
        except KeyboardInterrupt:
            terminal.header("Stopping serve container")
            terminal.print("Goodbye 👋")
            os._exit(0)  # kills all threads immediately

    def _serve(self, *, dir: str, timeout: int = 0):
        r: StartEndpointServeResponse = self.endpoint_stub.start_endpoint_serve(
            StartEndpointServeRequest(
                stub_id=self.stub_id,
                timeout=timeout,
            )
        )
        if not r.ok:
            return terminal.error(r.error_msg)

        container = Container(container_id=r.container_id)
        container.attach(container_id=r.container_id, sync_dir=dir)

================================================================================
# File: abstractions/integrations/vllm.py
# Path: src/beta9/abstractions/integrations/vllm.py
================================================================================

import os
from dataclasses import dataclass
from types import SimpleNamespace
from typing import Any, Callable, Dict, List, Literal, Mapping, Optional, Tuple, Type, Union

from ... import terminal
from ...abstractions.base.container import Container
from ...abstractions.base.runner import ASGI_DEPLOYMENT_STUB_TYPE, ASGI_SERVE_STUB_TYPE
from ...abstractions.endpoint import ASGI
from ...abstractions.image import Image
from ...abstractions.volume import CloudBucket, Volume
from ...channel import with_grpc_error_handling
from ...clients.endpoint import StartEndpointServeRequest, StartEndpointServeResponse
from ...clients.gateway import DeployStubRequest, DeployStubResponse
from ...config import ConfigContext
from ...type import Autoscaler, GpuType, GpuTypeAlias, QueueDepthAutoscaler

DEFAULT_VLLM_CACHE_DIR = "./vllm_cache"
DEFAULT_VLLM_CACHE_ROOT = "./vllm_cache_root"


# vllm/engine/arg_utils.py:EngineArgs
@dataclass
class VLLMArgs:
    """
    The configuration for the vLLM engine. For more information, see the vllm documentation:
    https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#command-line-arguments-for-the-server
    Each of these arguments corresponds to a command line argument for the vllm server.
    """

    # Args for init_app_state
    model: str = "facebook/opt-125m"
    max_log_len: Optional[int] = None
    chat_template: Optional[str] = None
    lora_modules: Optional[List[str]] = None
    prompt_adapters: Optional[List[str]] = None
    response_role: Optional[str] = "assistant"
    chat_template_content_format: str = "auto"
    return_tokens_as_token_ids: bool = False
    enable_auto_tool_choice: bool = False
    tool_call_parser: Optional[str] = None
    reasoning_parser: Optional[str] = None
    enable_prompt_tokens_details: bool = False
    enable_server_load_tracking: bool = False
    chat_template_url: Optional[str] = None
    tool_parser_plugin: Optional[str] = None

    # Args for AsyncEngineArgs
    skip_tokenizer_init: bool = False
    tokenizer_mode: str = "auto"
    task: str = "auto"
    trust_remote_code: bool = False
    download_dir: Optional[str] = DEFAULT_VLLM_CACHE_DIR
    load_format: str = "auto"
    config_format: str = "auto"
    dtype: str = "auto"
    kv_cache_dtype: str = "auto"
    seed: int = 0
    max_model_len: Optional[int] = None
    distributed_executor_backend: Optional[Union[str, Any]] = None
    pipeline_parallel_size: int = 1
    tensor_parallel_size: int = 1
    max_parallel_loading_workers: Optional[int] = None
    block_size: int = 16
    enable_prefix_caching: bool = False
    disable_sliding_window: bool = False
    use_v2_block_manager: bool = True
    swap_space: float = 4  # GiB
    cpu_offload_gb: float = 0  # GiB
    gpu_memory_utilization: float = 0.90
    max_num_batched_tokens: Optional[int] = None
    max_num_seqs: int = 256
    max_logprobs: int = 20
    revision: Optional[str] = None
    code_revision: Optional[str] = None
    rope_scaling: Optional[dict] = None
    rope_theta: Optional[float] = None
    tokenizer_revision: Optional[str] = None
    quantization: Optional[str] = None
    enforce_eager: Optional[bool] = None
    max_seq_len_to_capture: int = 8192
    disable_custom_all_reduce: bool = False
    tokenizer_pool_size: int = 0
    tokenizer_pool_type: Union[str, Any] = "ray"
    tokenizer_pool_extra_config: Optional[dict] = None
    limit_mm_per_prompt: Optional[Mapping[str, int]] = None
    enable_lora: bool = False
    max_loras: int = 1
    max_lora_rank: int = 16
    enable_prompt_adapter: bool = False
    max_prompt_adapters: int = 1
    max_prompt_adapter_token: int = 0
    fully_sharded_loras: bool = False
    lora_extra_vocab_size: int = 256
    long_lora_scaling_factors: Optional[Tuple[float]] = None
    lora_dtype: Optional[Union[str, Any]] = "auto"
    max_cpu_loras: Optional[int] = None
    device: str = "auto"
    num_scheduler_steps: int = 1
    multi_step_stream_outputs: bool = True
    ray_workers_use_nsight: bool = False
    num_gpu_blocks_override: Optional[int] = None
    num_lookahead_slots: int = 0
    model_loader_extra_config: Optional[dict] = None
    ignore_patterns: Optional[Union[str, List[str]]] = None
    preemption_mode: Optional[str] = None
    scheduler_delay_factor: float = 0.0
    enable_chunked_prefill: Optional[bool] = None
    guided_decoding_backend: str = "outlines"
    qlora_adapter_name_or_path: Optional[str] = None
    otlp_traces_endpoint: Optional[str] = None
    collect_detailed_traces: Optional[str] = None
    disable_async_output_proc: bool = False
    override_neuron_config: Optional[Dict[str, Any]] = None
    override_pooler_config: Optional[Any] = None
    mm_processor_kwargs: Optional[Dict[str, Any]] = None
    scheduling_policy: Literal["fcfs", "priority"] = "fcfs"
    disable_log_requests: bool = False
    kv_transfer_config: Optional[Any] = None
    logits_processor_pattern: Optional[str] = None
    max_long_partial_prefills: Optional[int] = 1
    disable_mm_preprocessor_cache: bool = False
    allowed_local_media_path: str = ""
    disable_log_stats: bool = False
    enable_expert_parallel: bool = False
    prefix_caching_hash_algo: str = "builtin"
    enable_lora_bias: bool = False
    use_tqdm_on_load: bool = False
    model_impl: str = "auto"
    worker_cls: str = "auto"
    additional_config: Optional[Dict[str, Any]] = None
    hf_token: Optional[Union[bool, str]] = None
    worker_extension_cls: str = ""
    served_model_name: Optional[Union[str, List[str]]] = None
    override_generation_config: Optional[Dict[str, Any]] = None
    show_hidden_metrics_for_version: Optional[str] = None
    enable_reasoning: Optional[bool] = None
    scheduler_cls: Union[str, Type[object]] = "vllm.core.scheduler.Scheduler"
    tokenizer: Optional[str] = None
    generation_config: Optional[str] = "auto"
    hf_overrides: Optional[Any] = None
    long_prefill_token_threshold: Optional[int] = 0
    max_num_partial_prefills: Optional[int] = 1
    calculate_kv_scales: Optional[bool] = None
    reasoning_parser: Optional[str] = None
    speculative_config: Optional[Dict[str, Any]] = None
    disable_cascade_attn: bool = False
    enable_sleep_mode: bool = False
    hf_config_path: Optional[str] = None
    disable_chunked_mm_input: bool = False
    data_parallel_size: int = 1
    compilation_config: Optional[Any] = None
    vllm_cache_root: Optional[str] = DEFAULT_VLLM_CACHE_ROOT


class VLLM(ASGI):
    """
    vllm is a wrapper around the vLLM library that allows you to deploy it as an ASGI app.

    Parameters:
        cpu (Union[int, float, str]):
            The number of CPU cores allocated to the container. Default is 1.0.
        memory (Union[int, str]):
            The amount of memory allocated to the container. It should be specified in
            MiB, or as a string with units (e.g. "1Gi"). Default is 128 MiB.
        gpu (Union[GpuType, str]):
            The type or name of the GPU device to be used for GPU-accelerated tasks. If not
            applicable or no GPU required, leave it empty. Default is [GpuType.NoGPU](#gputype).
        image (Union[Image, dict]):
            The container image used for the task execution. Whatever you pass here will have an additional `add_python_packages` call
            with `["fastapi", "vllm", "huggingface_hub"]` added to it to ensure that we can run vLLM in the container.
        vllm_version (str):
            The version of vLLM that will be installed from PyPI. As the configuration of the vLLM engine depends on the version of vLLM, using a non-default vllm_version might require subclassing VLLMArgs in order to add the missing configuration options. Default is version 0.8.4.
        huggingface_hub_version (str):
            The version of huggingface_hub that will be installed from PyPI. Different versions of vLLM require different versions of huggingface_hub, thus using a non-default vLLM version might require using a non-default version of huggingface_hub.  Default is version 0.30.2.
        workers (int):
            The number of workers to run in the container. Default is 1.
        concurrent_requests (int):
            The maximum number of concurrent requests to handle. Default is 1.
        keep_warm_seconds (int):
            The number of seconds to keep the container warm after the last request. Default is 60.
        max_pending_tasks (int):
            The maximum number of pending tasks to allow in the container. Default is 100.
        timeout (int):
            The maximum number of seconds to wait for the container to start. Default is 3600.
        authorized (bool):
            Whether the endpoints require authorization. Default is True.
        name (str):
            The name of the container. Default is none, which means you must provide it during deployment.
        volumes (List[Union[Volume, CloudBucket]]):
            The volumes and/or cloud buckets to mount into the container. Default is a single volume named "vllm_cache" mounted to "./vllm_cache".
            It is used as the download directory for vLLM models.
        secrets (List[str]):
            The secrets to pass to the container. If you need huggingface authentication to download models, you should set HF_TOKEN in the secrets.
        autoscaler (Autoscaler):
            The autoscaler to use. Default is a queue depth autoscaler.
        vllm_args (VLLMArgs):
            The arguments for the vLLM model.

    Example:
        ```python
        from beta9 import integrations

        e = integrations.VLLMArgs()
        e.device = "cpu"
        e.chat_template = "./chatml.jinja"

        vllm_app = integrations.VLLM(name="vllm-abstraction-1", vllm_args=e)
        ```
    """

    def __init__(
        self,
        cpu: Union[int, float, str] = 1.0,
        memory: Union[int, str] = 128,
        gpu: Union[GpuTypeAlias, List[GpuTypeAlias]] = GpuType.NoGPU,
        gpu_count: int = 0,
        image: Image = Image(python_version="python3.11"),
        vllm_version: str = "0.8.4",
        huggingface_hub_version: str = "0.30.2",
        workers: int = 1,
        concurrent_requests: int = 1,
        keep_warm_seconds: int = 60,
        max_pending_tasks: int = 100,
        timeout: int = 3600,
        authorized: bool = True,
        name: Optional[str] = None,
        volumes: Optional[List[Union[Volume, CloudBucket]]] = [],
        secrets: Optional[List[str]] = None,
        autoscaler: Autoscaler = QueueDepthAutoscaler(),
        vllm_args: VLLMArgs = VLLMArgs(),
    ):
        if vllm_args.download_dir == DEFAULT_VLLM_CACHE_DIR:
            # Add default vllm cache volume to preserve it if custom volumes are specified for chat templates
            volumes.append(Volume(name="vllm_cache", mount_path=DEFAULT_VLLM_CACHE_DIR))

        volumes.append(Volume(name="vllm_cache_root", mount_path=vllm_args.vllm_cache_root))

        image = image.add_python_packages(
            [
                "fastapi",
                "numpy",
                f"vllm=={vllm_version}",
                f"huggingface_hub=={huggingface_hub_version}",
            ]
        )

        super().__init__(
            cpu=cpu,
            memory=memory,
            gpu=gpu,
            gpu_count=gpu_count,
            image=image,
            workers=workers,
            concurrent_requests=concurrent_requests,
            keep_warm_seconds=keep_warm_seconds,
            max_pending_tasks=max_pending_tasks,
            timeout=timeout,
            authorized=authorized,
            name=name,
            volumes=volumes,
            secrets=secrets,
            autoscaler=autoscaler,
        )

        self.chat_template_url = vllm_args.chat_template_url
        self.engine_args = vllm_args
        self.app_args = SimpleNamespace(
            model=vllm_args.model,
            served_model_name=vllm_args.served_model_name,
            disable_log_requests=vllm_args.disable_log_requests,
            max_log_len=vllm_args.max_log_len,
            disable_log_stats=vllm_args.disable_log_stats,
            chat_template=vllm_args.chat_template,
            lora_modules=vllm_args.lora_modules,
            prompt_adapters=vllm_args.prompt_adapters,
            response_role=vllm_args.response_role,
            chat_template_content_format=vllm_args.chat_template_content_format,
            return_tokens_as_token_ids=vllm_args.return_tokens_as_token_ids,
            enable_auto_tool_choice=vllm_args.enable_auto_tool_choice,
            tool_call_parser=vllm_args.tool_call_parser,
            enable_reasoning=vllm_args.enable_reasoning,
            reasoning_parser=vllm_args.reasoning_parser,
            enable_prompt_tokens_details=vllm_args.enable_prompt_tokens_details,
            enable_server_load_tracking=vllm_args.enable_server_load_tracking,
        )

    def __name__(self) -> str:
        return self.name or "vllm"

    def set_handler(self, handler: str):
        self.handler = handler

    def func(self, *args: Any, **kwargs: Any):
        pass

    def __call__(self, *args: Any, **kwargs: Any):
        import asyncio

        import vllm.entrypoints.openai.api_server as api_server
        from fastapi import FastAPI
        from vllm.engine.arg_utils import AsyncEngineArgs
        from vllm.engine.async_llm_engine import AsyncLLMEngine
        from vllm.entrypoints.openai.tool_parsers import ToolParserManager
        from vllm.usage.usage_lib import UsageContext

        if self.engine_args.vllm_cache_root:
            os.environ["VLLM_CACHE_ROOT"] = self.engine_args.vllm_cache_root

        if self.chat_template_url:
            import requests

            chat_template_filename = self.chat_template_url.split("/")[-1]

            if not os.path.exists(f"{self.engine_args.download_dir}/{chat_template_filename}"):
                response = requests.get(self.chat_template_url)
                with open(
                    f"{self.engine_args.download_dir}/{chat_template_filename}", "wb"
                ) as file:
                    file.write(response.content)

            self.app_args.chat_template = (
                f"{self.engine_args.download_dir}/{chat_template_filename}"
            )

        if "HF_TOKEN" in os.environ:
            hf_token = os.environ["HF_TOKEN"]
            import huggingface_hub

            huggingface_hub.login(hf_token)

        if self.engine_args.tool_parser_plugin and len(self.engine_args.tool_parser_plugin) > 3:
            ToolParserManager.import_tool_parser(self.engine_args.tool_parser_plugin)

        valide_tool_parses = ToolParserManager.tool_parsers.keys()
        if (
            self.engine_args.enable_auto_tool_choice
            and self.engine_args.tool_call_parser not in valide_tool_parses
        ):
            raise KeyError(
                f"invalid tool call parser: {self.engine_args.tool_call_parser} "
                f"(chose from {{ {','.join(valide_tool_parses)} }})"
            )

        app = FastAPI()

        @app.get("/health")
        async def health_check():
            return {"status": "healthy"}

        app.include_router(api_server.router)

        engine_args = AsyncEngineArgs.from_cli_args(self.engine_args)

        engine_client = AsyncLLMEngine.from_engine_args(
            engine_args, usage_context=UsageContext.OPENAI_API_SERVER
        )

        model_config = asyncio.run(engine_client.get_model_config())
        asyncio.run(
            api_server.init_app_state(
                engine_client,
                model_config,
                app.state,
                self.app_args,
            )
        )

        return app

    def deploy(
        self,
        name: Optional[str] = None,
        context: Optional[ConfigContext] = None,
        invocation_details_func: Optional[Callable[..., None]] = None,
        **invocation_details_options: Any,
    ) -> bool:
        self.name = name or self.name
        if not self.name:
            terminal.error(
                "You must specify an app name (either in the decorator or via the --name argument)."
            )

        if (
            self.engine_args.download_dir != DEFAULT_VLLM_CACHE_DIR
            and self.engine_args.download_dir not in [v.mount_path for v in self.volumes]
        ):
            terminal.error(
                "The engine's download directory must match a mount path in the volumes list."
            )

        if context is not None:
            self.config_context = context

        if not self.prepare_runtime(
            stub_type=ASGI_DEPLOYMENT_STUB_TYPE,
            force_create_stub=True,
        ):
            return False

        terminal.header("Deploying")
        deploy_response: DeployStubResponse = self.gateway_stub.deploy_stub(
            DeployStubRequest(stub_id=self.stub_id, name=self.name)
        )

        self.deployment_id = deploy_response.deployment_id
        if deploy_response.ok:
            terminal.header("Deployed 🎉")
            if invocation_details_func:
                invocation_details_func(**invocation_details_options)
            else:
                self.print_invocation_snippet(**invocation_details_options)

        return deploy_response.ok

    @with_grpc_error_handling
    def serve(self, timeout: int = 0, url_type: str = ""):
        stub_type = ASGI_SERVE_STUB_TYPE

        if not self.prepare_runtime(func=self.func, stub_type=stub_type, force_create_stub=True):
            return False

        try:
            with terminal.progress("Serving endpoint..."):
                self.parent.print_invocation_snippet(url_type=url_type)

                return self._serve(dir=os.getcwd(), timeout=timeout)
        except KeyboardInterrupt:
            terminal.header("Stopping serve container")
            terminal.print("Goodbye 👋")
            os._exit(0)  # kills all threads immediately

    def _serve(self, *, dir: str, timeout: int = 0):
        r: StartEndpointServeResponse = self.endpoint_stub.start_endpoint_serve(
            StartEndpointServeRequest(
                stub_id=self.stub_id,
                timeout=timeout,
            )
        )
        if not r.ok:
            return terminal.error(r.error_msg)

        container = Container(container_id=r.container_id)
        container.attach(container_id=r.container_id, sync_dir=dir)

================================================================================
# File: abstractions/map.py
# Path: src/beta9/abstractions/map.py
================================================================================

from typing import Any, Optional

import cloudpickle

from ..abstractions.base import BaseAbstraction
from ..clients.map import (
    MapCountRequest,
    MapCountResponse,
    MapDeleteRequest,
    MapDeleteResponse,
    MapGetRequest,
    MapGetResponse,
    MapKeysRequest,
    MapKeysResponse,
    MapServiceStub,
    MapSetRequest,
    MapSetResponse,
)


class Map(BaseAbstraction):
    """A distributed python dictionary."""

    def __init__(self, *, name: str) -> None:
        """
        Creates a Map Instance.

        Use this a concurrency safe key/value store, accessible both locally and within
        remote containers. Serialization is done using cloudpickle, so any object that supported
        by that should work here. The interface is that of a standard python dictionary.

        Because this is backed by a distributed dictionary, it will persist between runs.

        Parameters:
            name (str):
                The name of the map (any arbitrary string).

        Example:
        ```python
        from beta9 import Map

        # Name the map
        m = Map(name="test")

        # Set a key
        m["some_key"] = True

        # Delete a key
        del m["some_key"]

        # Iterate through the map
        for k, v in m.items():
            print("key: ", k)
            print("value: ", v)
        ```
        """
        super().__init__()

        self.name: str = name
        self._stub: Optional[MapServiceStub] = None

    @property
    def stub(self) -> MapServiceStub:
        if not self._stub:
            self._stub = MapServiceStub(self.channel)
        return self._stub

    @stub.setter
    def stub(self, value: MapServiceStub):
        self._stub = value

    def set(self, key: str, value: Any, ttl: int = 604_800) -> bool:
        """
        Set a key in the map.

        Args:
            key: The key to set.
            value: The value to set. The max value size is 1 MiB.
            ttl: The time to live for the key in seconds. Cannot be more than 7 days. Defaults to 7 days.

        Raises:
            ValueError: If the key could not be set.

        Returns:
            bool: True if the key was set, False otherwise.
        """
        r: MapSetResponse = self.stub.map_set(
            MapSetRequest(name=self.name, key=key, value=cloudpickle.dumps(value), ttl=ttl)
        )

        if not r.ok:
            raise ValueError(r.err_msg)

        return r.ok

    def get(self, key: str) -> Any:
        r: MapGetResponse = self.stub.map_get(MapGetRequest(name=self.name, key=key))
        return cloudpickle.loads(r.value) if r.ok else None

    def __setitem__(self, key, value):
        self.set(key, value)

    def __getitem__(self, key):
        return self.get(key)

    def __delitem__(self, key):
        r: MapDeleteResponse = self.stub.map_delete(MapDeleteRequest(name=self.name, key=key))

        if not r.ok:
            raise KeyError(key)

    def __len__(self):
        r: MapCountResponse = self.stub.map_count(MapCountRequest(name=self.name))
        return r.count if r.ok else 0

    def __iter__(self):
        r: MapKeysResponse = self.stub.map_keys(MapKeysRequest(name=self.name))
        return iter(r.keys) if r.ok else iter([])

    def items(self):
        keys_response: MapKeysResponse = self.stub.map_keys(MapKeysRequest(name=self.name))
        if not keys_response.ok:
            return iter([])

        def _generate_items():
            for key in keys_response.keys:
                value_response: MapGetResponse = self.stub.map_get(
                    MapGetRequest(name=self.name, key=key)
                )

                if value_response.ok:
                    value = cloudpickle.loads(value_response.value)
                    yield (key, value)

        return _generate_items()

================================================================================
# File: abstractions/mixins.py
# Path: src/beta9/abstractions/mixins.py
================================================================================

import inspect
import threading
import urllib.parse
from typing import Any, Callable, ClassVar, Optional

from .. import terminal
from ..abstractions.base.container import Container
from ..abstractions.base.runner import SHELL_STUB_TYPE
from ..channel import with_grpc_error_handling
from ..clients.gateway import DeployStubRequest, DeployStubResponse, GetUrlRequest, GetUrlResponse
from ..clients.shell import CreateShellInExistingContainerRequest, CreateStandaloneShellRequest
from ..config import ConfigContext
from .base.runner import RunnerAbstraction
from .shell import SSHShell


class DeployableMixin:
    func: Callable
    parent: RunnerAbstraction
    deployment_id: Optional[str] = None
    deployment_stub_type: ClassVar[str]

    def _validate(self):
        if not hasattr(self, "func") or not isinstance(self.func, Callable):
            raise AttributeError("func variable not set or is incorrect type")

        if not hasattr(self, "parent") or not isinstance(self.parent, RunnerAbstraction):
            raise AttributeError("parent variable not set or is incorrect type")

        if not hasattr(self, "deployment_stub_type") or not self.deployment_stub_type:
            raise AttributeError("deployment_stub_type variable not set")

    def _is_abstraction_callable_wrapper(self, func: Callable, ab_name: str) -> bool:
        return (
            hasattr(func, "parent")
            and inspect.isclass(type(func.parent))
            and func.parent.__class__.__name__ == ab_name
        )

    def deploy(
        self,
        name: Optional[str] = None,
        context: Optional[ConfigContext] = None,
        invocation_details_func: Optional[Callable[..., None]] = None,
        **invocation_details_options: Any,
    ) -> bool:
        self._validate()

        self.parent.name = name or self.parent.name
        if not self.parent.name:
            terminal.error(
                "You must specify an app name (either in the decorator or via the --name argument)."
            )

        if context is not None:
            self.parent.config_context = context

        if self.parent.on_deploy and self._is_abstraction_callable_wrapper(
            self.parent.on_deploy, "Function"
        ):
            terminal.header("Running on_deploy hook")
            self.parent.on_deploy()

        if not self.parent.prepare_runtime(
            func=self.func, stub_type=self.deployment_stub_type, force_create_stub=True
        ):
            return False

        terminal.header("Deploying")
        deploy_response: DeployStubResponse = self.parent.gateway_stub.deploy_stub(
            DeployStubRequest(
                stub_id=self.parent.stub_id,
                name=self.parent.name,
            )
        )

        self.parent.deployment_id = deploy_response.deployment_id
        if deploy_response.ok:
            terminal.header("Deployed 🎉")
            if invocation_details_func:
                invocation_details_func(**invocation_details_options)
            else:
                self.parent.print_invocation_snippet(**invocation_details_options)

        return deploy_response.ok

    def _attach_and_sync(self, container_id: str, sync_dir: str):
        try:
            container = Container(
                container_id=container_id,
            )
            container.attach(container_id=container_id, sync_dir=sync_dir, hide_logs=True)
        except BaseException:
            terminal.header(f"Stopped syncing directory '{sync_dir}'")

    @with_grpc_error_handling
    def shell(
        self, url_type: str = "", sync_dir: Optional[str] = None, container_id: Optional[str] = None
    ):
        # First, spin up the shell container
        username = "root"
        password = ""

        if container_id:
            with terminal.progress("Creating shell..."):
                create_shell_response = self.parent.shell_stub.create_shell_in_existing_container(
                    CreateShellInExistingContainerRequest(
                        container_id=container_id,
                    )
                )

                if not create_shell_response.ok:
                    return terminal.error(
                        f"Failed to create shell: {create_shell_response.err_msg} ❌"
                    )

                username = create_shell_response.username
                password = create_shell_response.password
                self.parent.stub_id = create_shell_response.stub_id
        else:
            stub_type = SHELL_STUB_TYPE

            if not self.parent.prepare_runtime(
                func=self.func, stub_type=stub_type, force_create_stub=True
            ):
                return False

            create_shell_response = self.parent.shell_stub.create_standalone_shell(
                CreateStandaloneShellRequest(
                    stub_id=self.parent.stub_id,
                )
            )
            if not create_shell_response.ok:
                return terminal.error(f"Failed to create shell: {create_shell_response.err_msg} ❌")

            container_id = create_shell_response.container_id
            username = create_shell_response.username
            password = create_shell_response.password

        # Then, we can retrieve the URL and establish a tunnel
        res: GetUrlResponse = self.parent.gateway_stub.get_url(
            GetUrlRequest(
                stub_id=self.parent.stub_id,
                deployment_id=getattr(self, "deployment_id", ""),
                url_type="/path",
                is_shell=True,
            )
        )
        if not res.ok:
            return terminal.error(f"Failed to get shell connection URL: {res.err_msg} ❌")

        # Parse the URL to extract the container_id
        parsed_url = urllib.parse.urlparse(res.url)
        proxy_host, proxy_port = parsed_url.hostname, parsed_url.port

        if not proxy_port:
            proxy_port = 443 if parsed_url.scheme == "https" else 80

        if sync_dir:
            threading.Thread(
                target=self._attach_and_sync,
                args=(container_id, sync_dir),
                daemon=True,
            ).start()

        with SSHShell(
            host=proxy_host,
            port=proxy_port,
            path=parsed_url.path,
            container_id=container_id,
            stub_id=self.parent.stub_id,
            auth_token=self.parent.config_context.token,
            username=username,
            password=password,
        ) as shell:
            shell.start()

================================================================================
# File: abstractions/output.py
# Path: src/beta9/abstractions/output.py
================================================================================

import os
import shutil
import uuid
import zipfile
from datetime import datetime
from pathlib import Path
from typing import Any, BinaryIO, Dict, Generator, NamedTuple, Optional, Protocol, Union

from betterproto import Casing

from ..abstractions.base import BaseAbstraction
from ..clients.output import (
    OutputPublicUrlRequest,
    OutputPublicUrlResponse,
    OutputSaveRequest,
    OutputSaveResponse,
    OutputServiceStub,
    OutputStatRequest,
    OutputStatResponse,
)
from ..env import is_local
from ..runner.common import USER_OUTPUTS_DIR
from ..sync import CHUNK_SIZE


class Output(BaseAbstraction):
    """
    A file or directory to persist after a task has ended.
    """

    _tmp_dir = Path("/tmp/outputs")

    def __init__(self, *, path: Union[Path, str]) -> None:
        """
        Creates an Output instance.

        Use this to store the output of an invocation of a task (function, taskqueue, etc.).
        This can be any file or directory.

        When the output is a directory, it is zipped up before being saved. When it
        is a file, it's saved as-as (no zipping).

        Args:
            path: The path to a file or directory.

        Raises:
            OutputCannotRunLocallyError:
                When trying to create an Output outside of a container environment.
            OutputTaskIdError:
                When the container environment doesn't set the task ID. This is needed
                to tell the server where to save the Output.
            FileNotFoundError:
                When the user-provided path doesn't exist.

        Example:

            Saving a directory.

            ```python
            mydir = Path(path="/volumes/myvol/mydir")
            output = Output(path=mydir)
            output.save()
            ```

            Saving a file and generating a public URL.

            ```python
            myfile = "path/to/my.txt"
            output = Output(path=myfile)
            output.save()
            output_url = output.public_url()
            ```

            Saving a PIL.Image object.

            ```python
            image = pipe( ... )
            output = Output.from_pil_image(image)
            output.save()
            ```
        """
        super().__init__()
        self.prepare_tmp_dir()

        if is_local():
            raise OutputCannotRunLocallyError

        self.task_id = os.getenv("TASK_ID", "")
        if not self.task_id:
            raise OutputTaskIdError

        self.path = Path(path)
        if not self.path.exists():
            raise FileNotFoundError

        self.id: Optional[str] = None
        self._stub: Optional[OutputServiceStub] = None

    def __del__(self) -> None:
        """
        Cleans up temp data stored on disk.
        """
        try:
            shutil.rmtree(self._tmp_dir / str(id(self)))
        except FileNotFoundError:
            pass

    @property
    def stub(self) -> OutputServiceStub:
        if not self._stub:
            self._stub = OutputServiceStub(self.channel)
        return self._stub

    @stub.setter
    def stub(self, value: OutputServiceStub) -> None:
        self._stub = value

    @property
    def zipped_path(self) -> Path:
        """
        Gets a zipped path for a directory.

        The path is unique to the instance of this class.

        Raises:
            ValueError: An error indicating the output is not a directory.
        """
        if not self.path.is_dir():
            raise ValueError("Output must be a directory to get the zipped path.")

        return self._tmp_dir / str(id(self)) / f"{self.path.name}.zip"

    @classmethod
    def from_pil_image(cls, image: "PILImage", format: Optional[str] = "png") -> "Output":
        """
        Creates an instance of Output with a PIL.Image object.
        """
        cls.prepare_tmp_dir()

        path = cls._tmp_dir / str(uuid.uuid4())
        if format:
            suffix = format.lower() if format.startswith(".") else f".{format.lower()}"
            path = path.with_suffix(suffix)

        image.save(path, format=format.lstrip(".") if format else format)

        self = cls(path=path)
        new_dir = self.path.parent / str(id(self))
        new_dir.mkdir(mode=755, exist_ok=True, parents=True)
        self.path = self.path.rename(new_dir / self.path.name)

        return self

    @classmethod
    def from_file(cls, file_handle: BinaryIO) -> "Output":
        """
        Creates an instance of Output from a file-like object.

        For example:

            ```python
            with open("myfile.txt", "rb") as f:
                output = Output.from_file(f)
                output.save()
            ```
        """
        cls.prepare_tmp_dir()
        path = cls._tmp_dir / str(uuid.uuid4())

        with open(path, "wb") as tmp_file:
            shutil.copyfileobj(file_handle, tmp_file)

        return cls(path=path)

    @classmethod
    def prepare_tmp_dir(cls) -> None:
        cls._tmp_dir.mkdir(mode=755, parents=True, exist_ok=True)

    def save(self) -> "Output":
        """
        Saves the Output.

        If the Output is a directory, it will be zipped before being uploaded
        as a single file. If the Output is a file, it will be uploaded as-is
        (not zipped).

        Raises:
            OutputSaveError: An error indicating the output failed to save.
        """
        path = Path(self.path)
        if self.path.is_dir():
            path = self.zip_dir(self.path)

        storage_available = os.getenv("STORAGE_AVAILABLE", "false").lower() == "true"
        if storage_available:
            output_id = str(uuid.uuid4())
            output_path = Path(USER_OUTPUTS_DIR) / self.task_id / output_id / path.name
            output_path.parent.mkdir(mode=755, parents=True, exist_ok=True)
            shutil.copy(path, output_path)

            # Ensure the file is written to disk
            with open(output_path, "rb+") as f:
                os.fsync(f.fileno())

            self.id = output_id
            return self

        def stream_request(p: Path) -> Generator[OutputSaveRequest, None, None]:
            if p.stat().st_size == 0:
                yield OutputSaveRequest(self.task_id, p.name, b"")
                return

            with open(p, mode="rb") as file:
                while chunk := file.read(CHUNK_SIZE):
                    yield OutputSaveRequest(self.task_id, p.name, chunk)

        res: OutputSaveResponse = self.stub.output_save_stream(stream_request(path))
        if not res.ok:
            raise OutputSaveError(res.err_msg)

        self.id = res.id
        return self

    def stat(self) -> "Stat":
        """
        Gets file metadata of the saved Output.

        Raises:
            OutputNotSavedError: An error indicating the output must be saved first.
            OutputNotFoundError: An error indicating the output does not exist on the server.
        """
        if not self.id:
            raise OutputNotSavedError

        path_name = self.zipped_path.name if self.path.is_dir() else self.path.name

        res: OutputStatResponse = self.stub.output_stat(
            OutputStatRequest(self.id, self.task_id, path_name)
        )

        if not res.ok:
            raise OutputNotFoundError(res.err_msg)

        stat = res.stat.to_pydict(casing=Casing.SNAKE)  # type:ignore

        size = stat.get("size")
        if size is None:
            stat["size"] = 0

        return Stat(**stat)

    def exists(self) -> bool:
        """
        Checks if the output exists on the server.
        """
        try:
            return True if self.stat() else False
        except (OutputNotSavedError, OutputNotFoundError):
            return False

    def public_url(self, expires: int = 3600) -> str:
        """
        Generates a unique publicly accessible URL.

        Args:
            expires: When the URL expires in seconds. Defaults to 3600.

        Raises:
            OutputNotSavedError: An error indicating the output must be saved first.
            OutputPublicURLError: An error indicating an issue with the server generating the URL.
        """
        if not self.id:
            raise OutputNotSavedError

        res: OutputPublicUrlResponse
        res = self.stub.output_public_url(
            OutputPublicUrlRequest(
                self.id,
                self.task_id,
                self.zipped_path.name if self.path.is_dir() else self.path.name,
                expires,
            )
        )

        if not res.ok:
            raise OutputPublicURLError(res.err_msg)

        return res.public_url

    def zip_dir(self, dir_path: Union[Path, str], compress_level: int = 9) -> Path:
        """
        Zips the provided directory.
        """
        dir_path = Path(dir_path)
        self.zipped_path.unlink(missing_ok=True)
        self.zipped_path.parent.mkdir(mode=755, parents=True, exist_ok=True)

        with zipfile.ZipFile(
            file=self.zipped_path,
            mode="w",
            compression=zipfile.ZIP_DEFLATED,
            compresslevel=compress_level,
        ) as zipf:
            for file_path in dir_path.rglob("*"):
                if file_path.is_file():
                    zipf.write(file_path, file_path.relative_to(dir_path))

        return self.zipped_path


class Stat(NamedTuple):
    mode: str  # protection bits
    size: int  # size in bytes
    atime: datetime  # accessed time
    mtime: datetime  # modified time

    def to_dict(self) -> Dict[str, Any]:
        return self._asdict()


class PILImage(Protocol):
    """
    Describes the attributes and methods needed on a PIL.Image object.
    """

    def save(self, fp, format=None, **params) -> None: ...


class OutputCannotRunLocallyError(Exception):
    pass


class OutputNotFoundError(Exception):
    pass


class OutputSaveError(Exception):
    pass


class OutputNotSavedError(Exception):
    pass


class OutputPublicURLError(Exception):
    pass


class OutputTaskIdError(Exception):
    pass

================================================================================
# File: abstractions/pod.py
# Path: src/beta9/abstractions/pod.py
================================================================================

import os
import uuid
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union

from .. import terminal
from ..abstractions.base.runner import (
    POD_DEPLOYMENT_STUB_TYPE,
    POD_RUN_STUB_TYPE,
    RunnerAbstraction,
)
from ..abstractions.image import Image
from ..abstractions.mixins import DeployableMixin
from ..abstractions.volume import CloudBucket, Volume
from ..channel import with_grpc_error_handling
from ..clients.gateway import (
    DeployStubRequest,
    DeployStubResponse,
    GatewayServiceStub,
    StopContainerRequest,
    StopContainerResponse,
)
from ..clients.pod import (
    CreatePodRequest,
    CreatePodResponse,
    PodServiceStub,
)
from ..config import ConfigContext, get_settings
from ..sync import FileSyncer
from ..type import GpuType, GpuTypeAlias
from ..utils import get_init_args_kwargs
from .base import BaseAbstraction


@dataclass
class PodInstance(BaseAbstraction):
    """
    Stores the result of creating a Pod.

    Attributes:
        container_id: The unique ID of the created container.
        url: The URL for accessing the container over HTTP (if ports were exposed).
    """

    container_id: str
    url: str
    ok: bool = field(default=False)
    error_msg: str = field(default="")
    gateway_stub: "GatewayServiceStub" = field(init=False)

    def __post_init__(self):
        super().__init__()
        self.gateway_stub = GatewayServiceStub(self.channel)

    def terminate(self) -> bool:
        """
        Terminate the container associated with this pod instance. Returns True if the container was terminated, False otherwise.
        """
        res: "StopContainerResponse" = self.gateway_stub.stop_container(
            StopContainerRequest(container_id=self.container_id)
        )
        return res.ok


class Pod(RunnerAbstraction, DeployableMixin):
    """
    Pod allows you to run arbitrary services in fast, scalable, and secure remote containers.

    Parameters:
        app (str):
            Assign the pod to an app. If the app does not exist, it will be created with the given name.
            An app is a group of resources (endpoints, task queues, functions, etc).
        entrypoint (Optional[List[str]]):
            The command to run in the container. Default is [].
        ports (Optional[List[int]]):
            The ports to expose the container to. Default is [].
        name (Optional[str]):
            An optional app name for this pod. If not specified, it will be the name of the
            working directory containing the python file with the pod class.
        cpu (Union[int, float, str]):
            The number of CPU cores allocated to the pod. Default is 1.0.
        memory (Union[int, str]):
            The amount of memory allocated to the pod. It should be specified in
            MiB, or as a string with units (e.g. "1Gi"). Default is 128 MiB.
        gpu (Union[GpuTypeAlias, List[GpuTypeAlias]]):
            The type or name of the GPU device to be used for GPU-accelerated tasks. If not
            applicable or no GPU required, leave it empty.
            You can specify multiple GPUs by providing a list of GpuTypeAlias. If you specify several GPUs,
            the scheduler prioritizes their selection based on their order in the list.
        gpu_count (int):
            The number of GPUs allocated to the pod. Default is 0. If a GPU is
            specified but this value is set to 0, it will be automatically updated to 1.
        image (Union[Image, dict]):
            The container image used for the task execution. Default is [Image](#image).
        volumes (Optional[List[Union[Volume, CloudBucket]]]):
            A list of volumes and/or cloud buckets to be mounted to the pod. Default is None.
        secrets (Optional[List[str]):
            A list of secrets that are injected into the pod as environment variables. Default is [].
        env (Optional[Dict[str, str]]):
            A dictionary of environment variables to be injected into the container. Default is {}.
        keep_warm_seconds (int):
            The number of seconds to keep the container up the last request. -1 means never scale down to zero.
            Default is 600 seconds (10 minutes).
        authorized (bool):
            If false, allows the pod to be accessed without an auth token.
            Default is False.

    Example usage:
        ```
        from beta9 import Image, Pod

        image = Image()
        pod = Pod(cpu=2, memory=512, image=image, ports=[8080])
        result = pod.create(entrypoint=["python", "-c", "\"print('Hello, World!')\""])
        print(result.container_id)
        print(result.url)

        ```
    """

    def __init__(
        self,
        app: str = "",
        entrypoint: List[str] = [],
        ports: Optional[List[int]] = [],
        name: Optional[str] = None,
        cpu: Union[int, float, str] = 1.0,
        memory: Union[int, str] = 128,
        gpu: Union[GpuTypeAlias, List[GpuTypeAlias]] = GpuType.NoGPU,
        gpu_count: int = 0,
        image: Image = Image(),
        volumes: Optional[List[Union[Volume, CloudBucket]]] = None,
        secrets: Optional[List[str]] = None,
        env: Optional[Dict[str, str]] = {},
        keep_warm_seconds: int = 600,
        authorized: bool = False,
    ) -> None:
        super().__init__(
            cpu=cpu,
            memory=memory,
            gpu=gpu,
            gpu_count=gpu_count,
            image=image,
            volumes=volumes,
            secrets=secrets,
            env=env,
            entrypoint=entrypoint,
            ports=ports,
            name=name,
            authorized=authorized,
            keep_warm_seconds=keep_warm_seconds,
            app=app,
        )
        self.parent = self
        self.func = None
        self.task_id = ""
        self._pod_stub: Optional[PodServiceStub] = None
        self.syncer: FileSyncer = FileSyncer(self.gateway_stub)
        self.image.ignore_python = True

        # This a temporary id generated by each class during each runtime
        self._id = str(uuid.uuid4())[:8]

    @property
    def stub(self) -> PodServiceStub:
        if not self._pod_stub:
            self._pod_stub = PodServiceStub(self.channel)
        return self._pod_stub

    @stub.setter
    def stub(self, value: PodServiceStub) -> None:
        self._pod_stub = value

    def parse_image(self, image: Image) -> Image:
        image.ignore_python = True
        return image

    def create(self, entrypoint: List[str] = []) -> PodInstance:
        """
        Create a new container that will run until either it completes normally, or times out.

        Args:
            entrypoint (List[str]): The command to run in the pod container (overrides the entrypoint specified in the Pod constructor).
        """
        if entrypoint:
            self.entrypoint = entrypoint

        if not self.entrypoint:
            terminal.error("You must specify an entrypoint.")

        if not self.prepare_runtime(stub_type=POD_RUN_STUB_TYPE, force_create_stub=True):
            return PodInstance(
                container_id="",
                url="",
                ok=False,
                error_msg="Failed to prepare runtime",
            )

        terminal.header("Creating container")
        create_response: CreatePodResponse = self.stub.create_pod(
            CreatePodRequest(
                stub_id=self.stub_id,
            )
        )

        url = ""
        if create_response.ok:
            terminal.header(f"Container created successfully ===> {create_response.container_id}")

            if self.keep_warm_seconds < 0:
                terminal.header("This container has no timeout, it will run until it completes.")
            else:
                terminal.header(
                    f"This container will timeout after {self.keep_warm_seconds} seconds."
                )

            url_res = self.print_invocation_snippet()
            url = url_res.url

        return PodInstance(
            container_id=create_response.container_id,
            url=url,
            ok=create_response.ok,
            error_msg=create_response.error_msg,
        )

    def deploy(
        self,
        name: Optional[str] = None,
        context: Optional[ConfigContext] = None,
        **invocation_details_options: Any,
    ):
        self.name = name or self.name
        if not self.name:
            terminal.error(
                "You must specify an app name (either in the decorator or via the --name argument)."
            )

        if not self.entrypoint:
            terminal.error("You must specify an entrypoint.")
            return False

        if context is not None:
            self.config_context = context

        if not self.prepare_runtime(stub_type=POD_DEPLOYMENT_STUB_TYPE, force_create_stub=True):
            return False

        terminal.header("Deploying")
        deploy_response: DeployStubResponse = self.gateway_stub.deploy_stub(
            DeployStubRequest(stub_id=self.stub_id, name=self.name)
        )

        self.deployment_id = deploy_response.deployment_id
        if deploy_response.ok:
            terminal.header("Deployed 🎉")

            if len(self.ports) > 0:
                self.print_invocation_snippet()

        return deploy_response.ok

    def generate_deployment_artifacts(self, **kwargs) -> str:
        imports = ["Pod"]

        pod_py = """
from {module} import {import_string}

app = Pod(
{arguments}
)
"""
        arguments = []
        argkwargs = get_init_args_kwargs(self.__class__)
        for key, value in kwargs.items():
            if key not in argkwargs or value is None:
                continue

            if isinstance(value, Image):
                imports.append("Image")
                value = f'Image(base_image="{value.base_image}")'
            elif isinstance(value, tuple):
                value = list(value)
            elif isinstance(value, str):
                value = f'"{value}"'

            arguments.append(f"    {key}={value}")

        content = pod_py.format(
            module=get_settings().name.lower(),
            import_string=", ".join(imports),
            arguments=",\n".join(arguments),
        )

        with open(f"pod-{self._id}.py", "w") as f:
            f.write(content)

    def cleanup_deployment_artifacts(self):
        if os.path.exists(f"pod-{self._id}.py"):
            os.remove(f"pod-{self._id}.py")

    @with_grpc_error_handling
    def shell(
        self, url_type: str = "", sync_dir: Optional[str] = None, container_id: Optional[str] = None
    ):
        self.authorized = True
        super().shell(url_type=url_type, sync_dir=sync_dir, container_id=container_id)

    def serve(self, **kwargs):
        terminal.error("Serve has not yet been implemented for Pods.")

================================================================================
# File: abstractions/queue.py
# Path: src/beta9/abstractions/queue.py
================================================================================

from typing import Any, Optional

import cloudpickle

from ..abstractions.base import BaseAbstraction
from ..clients.simplequeue import (
    SimpleQueueEmptyResponse,
    SimpleQueuePeekResponse,
    SimpleQueuePopRequest,
    SimpleQueuePopResponse,
    SimpleQueuePutRequest,
    SimpleQueuePutResponse,
    SimpleQueueRequest,
    SimpleQueueServiceStub,
)


class SimpleQueueInternalServerError(Exception):
    pass


class SimpleQueue(BaseAbstraction):
    """A distributed python queue."""

    def __init__(self, *, name: str, max_size=100) -> None:
        """
        Creates a Queue instance.

        Use this a concurrency safe distributed queue, accessible both locally and within
        remote containers. Serialization is done using cloudpickle, so any object that supported
        by that should work here. The interface is that of a standard python queue.

        Because this is backed by a distributed queue, it will persist between runs.

        Parameters:
            name (str):
                The name of the queue (any arbitrary string).

        Example:
        ```python
        from beta9 import Queue

        val = [1, 2, 3]

        # Initialize the Queue
        q = Queue(name="myqueue")

        for i in range(100):
            # Insert something to the queue
            q.put(val)
        while not q.empty():
            # Remove something from the queue
            val = q.pop()
            print(val)
        ```
        """
        super().__init__()

        self.name: str = name
        self._stub: Optional[SimpleQueueServiceStub] = None
        self.max_size: int = max_size

    @property
    def stub(self) -> SimpleQueueServiceStub:
        if not self._stub:
            self._stub = SimpleQueueServiceStub(self.channel)
        return self._stub

    def __len__(self):
        r = self.stub.simple_queue_size(SimpleQueueRequest(name=self.name))
        return r.size if r.ok else 0

    def put(self, value: Any) -> bool:
        r: SimpleQueuePutResponse = self.stub.simple_queue_put(
            SimpleQueuePutRequest(name=self.name, value=cloudpickle.dumps(value))
        )

        if not r.ok:
            raise SimpleQueueInternalServerError

        return True

    def pop(self) -> Any:
        r: SimpleQueuePopResponse = self.stub.simple_queue_pop(
            SimpleQueuePopRequest(name=self.name)
        )

        if not r.ok:
            raise SimpleQueueInternalServerError

        if len(r.value) > 0:
            return cloudpickle.loads(r.value)

        return None

    def empty(self) -> bool:
        r: SimpleQueueEmptyResponse = self.stub.simple_queue_empty(
            SimpleQueueRequest(name=self.name)
        )

        if not r.ok:
            raise SimpleQueueInternalServerError

        return r.empty if r.ok else True

    def peek(self) -> Any:
        r: SimpleQueuePeekResponse = self.stub.simple_queue_peek(SimpleQueueRequest(name=self.name))

        if not r.ok:
            raise SimpleQueueInternalServerError

        if len(r.value) > 0:
            return cloudpickle.loads(r.value)

        return None

    def remote(self):
        raise NotImplementedError

================================================================================
# File: abstractions/sandbox.py
# Path: src/beta9/abstractions/sandbox.py
================================================================================

import io
import time
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple, Union

from .. import terminal
from ..abstractions.base.runner import (
    SANDBOX_STUB_TYPE,
    BaseAbstraction,
)
from ..abstractions.image import Image
from ..abstractions.pod import Pod
from ..abstractions.volume import CloudBucket, Volume
from ..clients.gateway import GatewayServiceStub, StopContainerRequest, StopContainerResponse
from ..clients.pod import (
    CreatePodRequest,
    CreatePodResponse,
    PodSandboxDeleteFileRequest,
    PodSandboxDownloadFileRequest,
    PodSandboxExecRequest,
    PodSandboxExposePortRequest,
    PodSandboxExposePortResponse,
    PodSandboxFindFilesRequest,
    PodSandboxKillRequest,
    PodSandboxListFilesRequest,
    PodSandboxReplaceInFilesRequest,
    PodSandboxStatFileRequest,
    PodSandboxStatusRequest,
    PodSandboxStderrRequest,
    PodSandboxStdoutRequest,
    PodSandboxUploadFileRequest,
    PodServiceStub,
)
from ..exceptions import SandboxFileSystemError, SandboxProcessError
from ..type import GpuType, GpuTypeAlias


class Sandbox(Pod):
    """

    Parameters:
        cpu (Union[int, float, str]):
            The number of CPU cores allocated to the container. Default is 1.0.
        memory (Union[int, str]):
            The amount of memory allocated to the container. It should be specified in
            MiB, or as a string with units (e.g. "1Gi"). Default is 128 MiB.
        gpu (Union[GpuType, str]):
            The type or name of the GPU device to be used for GPU-accelerated tasks. If not
            applicable or no GPU required, leave it empty. Default is [GpuType.NoGPU](#gputype).
        image (Union[Image, dict]):
            The container image used for the task execution. Whatever you pass here will have an additional `add_python_packages` call
            with `["fastapi", "vllm", "huggingface_hub"]` added to it to ensure that we can run vLLM in the container.
        keep_warm_seconds (int):
            The number of seconds to keep the sandbox around. Default is -1 (requires manual termination).
        name (str):
            The name of the Sandbox app. Default is none, which means you must provide it during deployment.
        volumes (List[Union[Volume, CloudBucket]]):
            The volumes and/or cloud buckets to mount into the Sandbox container. Default is an empty list.
        secrets (List[str]):
            The secrets to pass to the Sandbox container.
    """

    def __init__(
        self,
        cpu: Union[int, float, str] = 1.0,
        memory: Union[int, str] = 128,
        gpu: Union[GpuTypeAlias, List[GpuTypeAlias]] = GpuType.NoGPU,
        gpu_count: int = 0,
        image: Image = Image(python_version="python3.11"),
        keep_warm_seconds: int = -1,
        authorized: bool = False,
        name: Optional[str] = None,
        volumes: Optional[List[Union[Volume, CloudBucket]]] = [],
        secrets: Optional[List[str]] = None,
    ):
        self.debug_buffer = io.StringIO()

        super().__init__(
            cpu=cpu,
            memory=memory,
            gpu=gpu,
            gpu_count=gpu_count,
            image=image,
            keep_warm_seconds=keep_warm_seconds,
            authorized=authorized,
            name=name,
            volumes=volumes,
            secrets=secrets,
        )

    def debug(self):
        print(self.debug_buffer.getvalue())

    def create(self) -> "SandboxInstance":
        """
        Create a new sandbox instance.

        """

        self.entrypoint = ["tail", "-f", "/dev/null"]

        if not self.prepare_runtime(
            stub_type=SANDBOX_STUB_TYPE,
            force_create_stub=True,
        ):
            return SandboxInstance(
                container_id="",
                url="",
                ok=False,
                error_msg="Failed to prepare runtime",
            )

        terminal.header("Creating sandbox")

        create_response: CreatePodResponse = self.stub.create_pod(
            CreatePodRequest(
                stub_id=self.stub_id,
            )
        )

        if create_response.ok:
            terminal.header(f"Sandbox created successfully ===> {create_response.container_id}")

            if self.keep_warm_seconds < 0:
                terminal.header(
                    "This sandbox has no timeout, it will run until it is shut down manually."
                )
            else:
                terminal.header(
                    f"This sandbox will timeout after {self.keep_warm_seconds} seconds."
                )

            return SandboxInstance(
                stub_id=self.stub_id,
                container_id=create_response.container_id,
                ok=create_response.ok,
                error_msg=create_response.error_msg,
            )


@dataclass
class SandboxInstance(BaseAbstraction):
    """
    Stores the result of creating a Sandbox.

    Attributes:
        container_id: The unique ID of the created sandbox container.
        url: The URL for accessing the container over HTTP (if ports were exposed).
    """

    container_id: str
    stub_id: str
    ok: bool = field(default=False)
    error_msg: str = field(default="")
    gateway_stub: "GatewayServiceStub" = field(init=False)
    stub: "PodServiceStub" = field(init=False)

    def __post_init__(self):
        super().__init__()
        self.gateway_stub = GatewayServiceStub(self.channel)
        self.stub = PodServiceStub(self.channel)
        self.fs = SandboxFileSystem(self)
        self.process = SandboxProcessManager(self)

    def terminate(self) -> bool:
        """
        Terminate the container associated with this sandbox instance. Returns True if the container was terminated, False otherwise.
        """
        res: "StopContainerResponse" = self.gateway_stub.stop_container(
            StopContainerRequest(container_id=self.container_id)
        )
        return res.ok

    def expose_port(self, port: int) -> str:
        """
        Dynamically expose a port to the internet. Returns an SSL terminated endpoint to access the sandbox.
        """
        res: "PodSandboxExposePortResponse" = self.stub.sandbox_expose_port(
            PodSandboxExposePortRequest(
                container_id=self.container_id, stub_id=self.stub_id, port=port
            )
        )

        if res.ok:
            return res.url

        raise SandboxProcessError("Failed to expose port")


class SandboxProcessResponse:
    def __init__(
        self,
        pid: int,
        exit_code: int,
        stdout: "SandboxProcessStream",
        stderr: "SandboxProcessStream",
    ):
        self.pid = pid
        self.exit_code = exit_code
        self.result: str = stdout.read() + stderr.read()


class SandboxProcessManager:
    def __init__(self, sandbox_instance: SandboxInstance) -> "SandboxProcess":
        self.sandbox_instance: SandboxInstance = sandbox_instance
        self.processes: Dict[int, SandboxProcess] = {}

    def run_code(
        self,
        code: str,
        blocking: bool = True,
        cwd: Optional[str] = None,
        env: Optional[Dict[str, str]] = None,
    ) -> Union["SandboxProcessResponse", "SandboxProcess"]:
        process = self._exec("python3", "-c", f"'{code}'", cwd=cwd, env=env)

        if blocking:
            process.wait()
            return SandboxProcessResponse(
                pid=process.pid,
                exit_code=process.exit_code,
                stdout=process.stdout,
                stderr=process.stderr,
            )

        return process

    def exec(
        self, *args, cwd: Optional[str] = None, env: Optional[Dict[str, str]] = None
    ) -> "SandboxProcess":
        args = list(args)
        args = ["bash", "-c", "'" + " ".join(args) + "'"]
        return self._exec(args, cwd=cwd, env=env)

    def _exec(
        self, *args, cwd: Optional[str] = None, env: Optional[Dict[str, str]] = None
    ) -> "SandboxProcess":
        command = list(args) if not isinstance(args[0], list) else args[0]
        shell_command = " ".join(command)

        response = self.sandbox_instance.stub.sandbox_exec(
            PodSandboxExecRequest(
                container_id=self.sandbox_instance.container_id,
                command=shell_command,
                cwd=cwd,
                env=env,
            )
        )
        if not response.ok or response.pid <= 0:
            raise SandboxProcessError(response.error_msg)

        if response.pid > 0:
            process = SandboxProcess(self.sandbox_instance, response.pid)
            self.processes[response.pid] = process
            return process

    def list_processes(self) -> List["SandboxProcess"]:
        return list(self.processes.values())

    def get_process(self, pid: int) -> "SandboxProcess":
        if pid not in self.processes:
            raise SandboxProcessError(f"Process with pid {pid} not found")

        return self.processes[pid]


class SandboxProcessStream:
    def __init__(self, process: "SandboxProcess", fetch_fn):
        self.process = process
        self.fetch_fn = fetch_fn
        self._buffer = ""
        self._closed = False
        self._last_output = ""

    def __iter__(self):
        return self

    def __next__(self):
        while True:
            if "\n" in self._buffer:
                line, self._buffer = self._buffer.split("\n", 1)
                return line + "\n"

            if self._closed:
                if self._buffer:
                    line, self._buffer = self._buffer, ""
                    return line
                raise StopIteration

            chunk = self._fetch_next_chunk()
            if chunk:
                self._buffer += chunk
            else:
                exit_code, _ = self.process.status()
                if exit_code >= 0:
                    last_chunk = self._fetch_next_chunk()
                    if last_chunk:
                        self._buffer += last_chunk
                        continue

                    self._closed = True
                else:
                    time.sleep(0.1)

    def _fetch_next_chunk(self):
        output = self.fetch_fn()

        if output == self._last_output:
            return ""

        new_output = output[len(self._last_output) :]
        self._last_output = output
        return new_output

    def read(self):
        output = []
        for line in self:
            output.append(line)

        return "".join(output)


class SandboxProcess:
    def __init__(self, sandbox_instance: SandboxInstance, pid: int):
        self.sandbox_instance = sandbox_instance
        self.pid = pid
        self.exit_code = -1
        self._status = ""

    def wait(self) -> int:
        self.exit_code, self._status = self.status()

        while self.exit_code < 0:
            self.exit_code, self._status = self.status()
            time.sleep(0.1)

        return self.exit_code

    def kill(self):
        response = self.sandbox_instance.stub.sandbox_kill(
            PodSandboxKillRequest(container_id=self.sandbox_instance.container_id, pid=self.pid)
        )
        if not response.ok:
            raise SandboxProcessError(response.error_msg)

    def status(self) -> Tuple[int, str]:
        response = self.sandbox_instance.stub.sandbox_status(
            PodSandboxStatusRequest(container_id=self.sandbox_instance.container_id, pid=self.pid)
        )

        if not response.ok:
            raise SandboxProcessError(response.error_msg)

        return response.exit_code, response.status

    @property
    def stdout(self):
        return SandboxProcessStream(
            self,
            lambda: self.sandbox_instance.stub.sandbox_stdout(
                PodSandboxStdoutRequest(
                    container_id=self.sandbox_instance.container_id, pid=self.pid
                )
            ).stdout,
        )

    @property
    def stderr(self):
        return SandboxProcessStream(
            self,
            lambda: self.sandbox_instance.stub.sandbox_stderr(
                PodSandboxStderrRequest(
                    container_id=self.sandbox_instance.container_id, pid=self.pid
                )
            ).stderr,
        )

    @property
    def logs(self):
        """
        Returns a combined stream of both stdout and stderr.
        This is a convenience property that combines both output streams.
        The streams are read concurrently, so if one stream is empty, it won't block
        the other stream from being read.
        """

        class CombinedStream:
            def __init__(self, process):
                self.process = process
                self._stdout = process.stdout
                self._stderr = process.stderr
                self._queue = []
                self._streams = {
                    "stdout": {"stream": self._stdout, "buffer": "", "exhausted": False},
                    "stderr": {"stream": self._stderr, "buffer": "", "exhausted": False},
                }

            def _process_stream(self, stream_name):
                """Process a single stream, adding any complete lines to the queue."""
                stream_info = self._streams[stream_name]
                if stream_info["exhausted"]:
                    return

                chunk = stream_info["stream"]._fetch_next_chunk()
                if chunk:
                    stream_info["buffer"] += chunk

                    while "\n" in stream_info["buffer"]:  # Process any complete lines
                        line, stream_info["buffer"] = stream_info["buffer"].split("\n", 1)
                        self._queue.append(line + "\n")

                elif self.process.exit_code >= 0:  # Process has exited
                    if stream_info["buffer"]:
                        self._queue.append(stream_info["buffer"])
                        stream_info["buffer"] = ""

                    stream_info["exhausted"] = True

            def _fill_queue(self):
                self._process_stream("stdout")
                self._process_stream("stderr")

            def __iter__(self):
                return self

            def __next__(self):
                while True:
                    # If queue is empty, try to fill it
                    if not self._queue:
                        self._fill_queue()
                        # If still empty after trying to fill, we're done
                        if not self._queue and all(s["exhausted"] for s in self._streams.values()):
                            raise StopIteration

                        # If queue is still empty but streams aren't exhausted, wait and try again
                        if not self._queue:
                            try:
                                time.sleep(0.1)
                                continue
                            except KeyboardInterrupt:
                                raise

                    # Return the next line from the queue
                    return self._queue.pop(0)

            def read(self):
                stdout_data = self._stdout.read()
                stderr_data = self._stderr.read()
                return stdout_data + stderr_data

        return CombinedStream(self)


@dataclass
class SandboxFileInfo:
    name: str
    is_dir: bool
    size: int
    mode: int
    mod_time: int
    permissions: int
    owner: str
    group: str

    def __str__(self):
        octal_perms = oct(self.permissions & 0o7777)
        return f"SandboxFileInfo(name='{self.name}', is_dir={self.is_dir}, size={self.size}, mode={self.mode}, mod_time={self.mod_time}, permissions={octal_perms}, owner='{self.owner}', group='{self.group}')"


class SandboxFileSystem:
    def __init__(self, sandbox_instance: SandboxInstance):
        self.sandbox_instance = sandbox_instance

    def upload_file(self, local_path: str, sandbox_path: str):
        with open(local_path, "rb") as f:
            content = f.read()

            response = self.sandbox_instance.stub.sandbox_upload_file(
                PodSandboxUploadFileRequest(
                    container_id=self.sandbox_instance.container_id,
                    container_path=sandbox_path,
                    data=content,
                    mode=644,
                )
            )

            if not response.ok:
                raise SandboxFileSystemError(response.error_msg)

    def download_file(self, sandbox_path: str, local_path: str):
        response = self.sandbox_instance.stub.sandbox_download_file(
            PodSandboxDownloadFileRequest(
                container_id=self.sandbox_instance.container_id,
                container_path=sandbox_path,
            )
        )

        if not response.ok:
            raise SandboxFileSystemError(response.error_msg)

        with open(local_path, "wb") as f:
            f.write(response.data)

    def stat_file(self, sandbox_path: str) -> "SandboxFileInfo":
        response = self.sandbox_instance.stub.sandbox_stat_file(
            PodSandboxStatFileRequest(
                container_id=self.sandbox_instance.container_id,
                container_path=sandbox_path,
            )
        )
        if not response.ok:
            raise SandboxFileSystemError(response.error_msg)

        return SandboxFileInfo(
            **{
                "name": response.file_info.name,
                "is_dir": response.file_info.is_dir,
                "size": response.file_info.size,
                "mode": response.file_info.mode,
                "mod_time": response.file_info.mod_time,
                "owner": response.file_info.owner,
                "group": response.file_info.group,
                "permissions": response.file_info.permissions,
            }
        )

    def list_files(self, sandbox_path: str) -> List["SandboxFileInfo"]:
        response = self.sandbox_instance.stub.sandbox_list_files(
            PodSandboxListFilesRequest(
                container_id=self.sandbox_instance.container_id,
                container_path=sandbox_path,
            )
        )
        if not response.ok:
            raise SandboxFileSystemError(response.error_msg)

        file_infos = []
        for file in response.files:
            f = {
                "name": file.name,
                "is_dir": file.is_dir,
                "size": file.size,
                "mode": file.mode,
                "mod_time": file.mod_time,
                "owner": file.owner,
                "group": file.group,
                "permissions": file.permissions,
            }
            file_infos.append(SandboxFileInfo(**f))

        return file_infos

    def create_directory(self, sandbox_path: str):
        raise NotImplementedError("Create directory not implemented")

    def delete_directory(self, sandbox_path: str):
        raise NotImplementedError("Delete directory not implemented")

    def delete_file(self, sandbox_path: str):
        response = self.sandbox_instance.stub.sandbox_delete_file(
            PodSandboxDeleteFileRequest(
                container_id=self.sandbox_instance.container_id,
                container_path=sandbox_path,
            )
        )

        if not response.ok:
            raise SandboxFileSystemError(response.error_msg)

    def replace_in_files(self, sandbox_path: str, old_string: str, new_string: str):
        response = self.sandbox_instance.stub.sandbox_replace_in_files(
            PodSandboxReplaceInFilesRequest(
                container_id=self.sandbox_instance.container_id,
                container_path=sandbox_path,
                pattern=old_string,
                new_string=new_string,
            )
        )

        if not response.ok:
            raise SandboxFileSystemError(response.error_msg)

    def find_files(self, sandbox_path: str, pattern: str) -> List["SandboxFileInfo"]:
        response = self.sandbox_instance.stub.sandbox_find_files(
            PodSandboxFindFilesRequest(
                container_id=self.sandbox_instance.container_id,
                container_path=sandbox_path,
                pattern=pattern,
            )
        )

        if not response.ok:
            raise SandboxFileSystemError(response.error_msg)

        return response.results

================================================================================
# File: abstractions/shell.py
# Path: src/beta9/abstractions/shell.py
================================================================================

import os
import socket
import ssl
import struct
import sys
import time
from dataclasses import dataclass
from typing import Optional

from .. import terminal
from ..env import is_local

if is_local():
    import paramiko


def create_socket(
    proxy_host: str, proxy_port: int, path: str, container_id: str, auth_token: str
) -> socket.socket:
    """
    Create a socket connection to the server and authenticate with the given token.
    """
    sock = socket.create_connection((proxy_host, proxy_port), timeout=60)
    if proxy_port == 443:
        sock = ssl.create_default_context().wrap_socket(sock=sock, server_hostname=proxy_host)

    request = (
        f"GET {path}/{container_id} HTTP/1.1\r\n"
        f"Host: {proxy_host}\r\n"
        f"Authorization: Bearer {auth_token}\r\n"
        "\r\n"
    )

    sock.sendall(request.encode())

    wait_for_ok(sock)

    return sock


def wait_for_ok(sock: socket.socket, max_retries: int = 5, delay: float = 0.25):
    """
    Wait until 'OK' is received from a socket.
    """
    for _ in range(max_retries):
        if data := sock.recv(4096).decode():
            if "OK" in data:
                return
            elif "ERROR" in data:
                raise ConnectionError(f"Error received from server: {data.lstrip('ERROR: ')}")
        delay *= 2
        time.sleep(delay)

    raise ConnectionError(f"Failed to setup socket after {max_retries} retries")


EXIT_STATUS_CTRL_C = 130
EXIT_STATUS_NOT_FOUND = 127


@dataclass
class SSHShell:
    """Interactive ssh shell that can be used as a context manager - for use with 'shell' command"""

    host: str
    port: int
    path: str
    container_id: str
    stub_id: str
    auth_token: str
    username: str
    password: str
    transport: Optional["paramiko.Transport"] = None

    def _open(self):
        self.socket: Optional[socket.socket] = None
        self.channel: Optional["paramiko.Channel"] = None

        try:
            self.socket = create_socket(
                self.host,
                self.port,
                self.path,
                self.container_id,
                self.auth_token,
            )
        except BaseException:
            return terminal.error("Failed to establish ssh tunnel.")

        self.transport = paramiko.Transport(self.socket)
        self.transport.connect(username=self.username, password=self.password)
        self.channel = self.transport.open_session()

        # Get terminal size - https://stackoverflow.com/a/943921
        rows, columns = os.popen("stty size", "r").read().split()

        self.channel.get_pty(
            term=os.getenv("TERM", "xterm-256color"), width=int(columns), height=int(rows)
        )
        self.channel.invoke_shell()

    def _close(self):
        if self.channel:
            self.channel.close()

        if self.transport:
            self.transport.close()

        if self.socket:
            try:
                # Set SO_LINGER to zero to forcefully close the socket
                self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_LINGER, struct.pack("ii", 1, 0))
                self.socket.shutdown(socket.SHUT_RDWR)
            except OSError:
                pass  # Ignore any errors that occur after the socket is already closed
            finally:
                self.socket.close()

    def __enter__(self):
        try:
            with terminal.progress("Connecting..."):
                self._open()
        except paramiko.SSHException:
            self._close()
            terminal.error("SSH error occurred.")
        except BaseException:
            self._close()
            terminal.error("Unexpected error occurred in shell.")

        return self

    def __exit__(self, exception_type, exception_value, traceback):
        self._close()

    def start(self):
        """Start the interactive shell session."""
        try:
            interactive_shell(self.channel)

            # Check the exit status after the shell session ends
            exit_status = self.channel.recv_exit_status()
            if (
                exit_status != 0
                and exit_status != EXIT_STATUS_CTRL_C
                and exit_status != EXIT_STATUS_NOT_FOUND
            ):
                terminal.warn("Lost connection to shell, attempting to reconnect in 5 seconds...")
                time.sleep(5)

                with terminal.progress("Connecting..."):
                    self._open()

                self.start()

        except paramiko.SSHException:
            self._close()
            terminal.error("SSH error occurred in shell.")
        except BaseException:
            self._close()
            terminal.error("Unexpected error occurred in shell.")


"""
   NOTE: much of the interactive shell code below is pulled from paramiko's examples, with a few slight modifications for use here.
   Original license / source information is as follows:
"""
# Source: https://github.com/paramiko/paramiko/blob/main/demos/interactive.py

# Copyright (C) 2003-2007  Robey Pointer <robeypointer@gmail.com>
#
# This file is part of paramiko.
#
# Paramiko is free software; you can redistribute it and/or modify it under the
# terms of the GNU Lesser General Public License as published by the Free
# Software Foundation; either version 2.1 of the License, or (at your option)
# any later version.
#
# Paramiko is distributed in the hope that it will be useful, but WITHOUT ANY
# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
# A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more
# details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Paramiko; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301 USA.


# https://invisible-island.net/xterm/ctlseqs/ctlseqs.html#h2-Bracketed-Paste-Mode
START_PASTE = "\x1b\x5b\x32\x30\x30\x7e"  # ESC[200~
END_PASTE = "\x1b\x5b\x32\x30\x31\x7e"  # ESC[201~

# windows does not have termios...
try:
    import termios
    import tty

    has_termios = True
except ImportError:
    has_termios = False


def is_int(val: str) -> bool:
    try:
        int(val)
        return True
    except Exception:
        return False


def interactive_shell(chan: "paramiko.Channel"):
    if has_termios:
        posix_shell(chan)
    else:
        windows_shell(chan)


def posix_readkey() -> str:
    """Get a keypress. If an escaped key is pressed, the full sequence is
    read and returned.

        Copied from readchar:
        https://github.com/magmax/python-readchar/blob/master/readchar/_posix_read.py#L30
    """

    c1 = sys.stdin.read(1)

    if c1 != "\x1b":  # ESC
        return c1

    c2 = sys.stdin.read(1)
    if c2 not in "\x4f\x5b":  # O[
        return c1 + c2

    c3 = sys.stdin.read(1)
    if c3 not in "\x31\x32\x33\x35\x36":  # 12356
        return c1 + c2 + c3

    c4 = sys.stdin.read(1)
    if c4 not in "\x30\x31\x33\x34\x35\x37\x38\x39":  # 01345789
        return c1 + c2 + c3 + c4

    c5 = sys.stdin.read(1)
    key = c1 + c2 + c3 + c4 + c5

    # Bracketed Paste Mode: # https://invisible-island.net/xterm/ctlseqs/ctlseqs.html#h2-Bracketed-Paste-Mode
    if key == START_PASTE[:-1] or key == END_PASTE[:-1]:
        c6 = sys.stdin.read(1)
        return key + c6

    return key


def windows_readkey() -> str:
    """Reads the next keypress. If an escaped key is pressed, the full
    sequence is read and returned.

        Copied from readchar:
        https://github.com/magmax/python-readchar/blob/master/readchar/_win_read.py#LL14C1-L30C24
    """

    ch = sys.stdin.read(1)

    # if it is a normal character:
    if ch not in "\x00\xe0":
        return ch

    # if it is a scpeal key, read second half:
    ch2 = sys.stdin.read(1)

    return "\x00" + ch2


def posix_shell(chan: "paramiko.Channel"):  # noqa: C901
    import select

    oldtty = termios.tcgetattr(sys.stdin)

    try:
        tty.setraw(sys.stdin.fileno())
        tty.setcbreak(sys.stdin.fileno())
        chan.settimeout(0.0)
        while True:
            r, w, e = select.select([chan, sys.stdin], [], [])
            if chan in r:
                try:
                    x = chan.recv(1024).decode("utf-8", errors="replace")
                    if len(x) == 0:
                        sys.stdout.write("\r\n")
                        break
                    sys.stdout.write(x)
                    sys.stdout.flush()
                except socket.timeout:
                    pass
            if sys.stdin in r:
                key = posix_readkey()
                # When pasting something, we need to read the entire pasted blob at once
                # Otherwise it'll hang until the next key press.
                # This has to do with how 'select.select' detects changes.
                # A paste is a single event of many characters, so we must handle them all as one event
                if key == START_PASTE:
                    # Start reading the pasted text
                    key = posix_readkey()
                    # Until we reach the end of the pasted text
                    while key != END_PASTE:
                        chan.send(key)
                        key = posix_readkey()
                    # We've exhausted the paste event, wait for next event
                    continue

                if len(key) == 0:
                    break
                chan.send(key)

    finally:
        termios.tcsetattr(sys.stdin, termios.TCSADRAIN, oldtty)


# thanks to Mike Looijmans for this code
def windows_shell(chan: "paramiko.Channel"):
    import threading

    sys.stdout.write("Line-buffered terminal emulation. Press F6 or ^Z to send EOF.\r\n\r\n")

    def writeall(sock):
        while True:
            data = sock.recv(256)
            if not data:
                sys.stdout.write("\r\n*** EOF ***\r\n\r\n")
                sys.stdout.flush()
                break
            sys.stdout.write(data)
            sys.stdout.flush()

    writer = threading.Thread(target=writeall, args=(chan,))
    writer.start()

    try:
        while True:
            d = windows_readkey()
            if not d:
                break
            chan.send(d)
    except EOFError:
        # user hit ^Z or F6
        pass

================================================================================
# File: abstractions/taskqueue.py
# Path: src/beta9/abstractions/taskqueue.py
================================================================================

import json
import os
from typing import Any, Callable, Dict, List, Optional, Type, Union

from .. import terminal
from ..abstractions.base.container import Container
from ..abstractions.base.runner import (
    TASKQUEUE_DEPLOYMENT_STUB_TYPE,
    TASKQUEUE_SERVE_STUB_TYPE,
    TASKQUEUE_STUB_TYPE,
    AbstractCallableWrapper,
    RunnerAbstraction,
)
from ..abstractions.image import Image
from ..abstractions.volume import CloudBucket, Volume
from ..channel import with_grpc_error_handling
from ..clients.taskqueue import (
    StartTaskQueueServeRequest,
    StartTaskQueueServeResponse,
    TaskQueuePutRequest,
    TaskQueuePutResponse,
    TaskQueueServiceStub,
)
from ..env import is_local
from ..schema import Schema
from ..type import (
    Autoscaler,
    GpuType,
    GpuTypeAlias,
    PricingPolicy,
    QueueDepthAutoscaler,
    TaskPolicy,
)
from .mixins import DeployableMixin


class TaskQueue(RunnerAbstraction):
    """
    Decorator which allows you to create a task queue out of the decorated function. The tasks are executed
    asynchronously, in remote containers. You can interact with the task queue either through an API (when deployed), or directly
    in python through the .put() method.

    Parameters:
        app (str):
            Assign the task queue to an app. If the app does not exist, it will be created with the given name.
            An app is a group of resources (endpoints, task queues, functions, etc).
        cpu (Union[int, float, str]):
            The number of CPU cores allocated to the container. Default is 1.0.
        memory (Union[int, str]):
            The amount of memory allocated to the container. It should be specified in
            MiB, or as a string with units (e.g. "1Gi"). Default is 128 MiB.
        gpu (Union[GpuTypeAlias, List[GpuTypeAlias]]):
            The type or name of the GPU device to be used for GPU-accelerated tasks. If not
            applicable or no GPU required, leave it empty.
            You can specify multiple GPUs by providing a list of GpuTypeAlias. If you specify several GPUs,
            the scheduler prioritizes their selection based on their order in the list.
        gpu_count (int):
            The number of GPUs allocated to the container. Default is 0. If a GPU is
            specified but this value is set to 0, it will be automatically updated to 1.
        image (Union[Image, dict]):
            The container image used for the task execution. Default is [Image](#image).
        timeout (Optional[int]):
            The maximum number of seconds a task can run before it times out.
            Default is 3600. Set it to -1 to disable the timeout.
        retries (Optional[int]):
            The maximum number of times a task will be retried if the container crashes. Default is 3.
        workers (Optional[int]):
            The number of processes handling tasks per container.
            Modifying this parameter can improve throughput for certain workloads.
            Workers will share the CPU, Memory, and GPU defined.
            You may need to increase these values to increase concurrency.
            Default is 1.
        keep_warm_seconds (int):
            The duration in seconds to keep the task queue warm even if there are no pending
            tasks. Keeping the queue warm helps to reduce the latency when new tasks arrive.
            Default is 10s.
        max_pending_tasks (int):
            The maximum number of tasks that can be pending in the queue. If the number of
            pending tasks exceeds this value, the task queue will stop accepting new tasks.
            Default is 100.
        on_start (Optional[Callable]):
            An optional function to run once (per process) when the container starts. Can be used for downloading data,
            loading models, or anything else computationally expensive.
        callback_url (Optional[str]):
            An optional URL to send a callback to when a task is completed, timed out, or cancelled.
        volumes (Optional[List[Union[Volume, CloudBucket]]]):
            A list of storage volumes and/or cloud buckets to be associated with the taskqueue. Default is [].
        secrets (Optional[List[str]):
            A list of secrets that are injected into the container as environment variables. Default is [].
        env (Optional[Dict[str, str]]):
            A dictionary of environment variables to be injected into the container. Default is {}.
        name (Optional[str]):
            An optional app name for this task queue. If not specified, it will be the name of the
            working directory containing the python file with the decorated function.
        authorized (bool):
            If false, allows the endpoint to be invoked without an auth token.
            Default is True.
        autoscaler (Autoscaler):
            Configure a deployment autoscaler - if specified, you can use scale your function horizontally using
            various autoscaling strategies. Default is QueueDepthAutoscaler().
        task_policy (TaskPolicy):
            The task policy for the function. This helps manage the lifecycle of an individual task.
            Setting values here will override timeout and retries.
        retry_for (Optional[List[BaseException]]):
            A list of exceptions that will trigger a retry if raised by your handler.
        checkpoint_enabled (bool):
            (experimental) Whether to enable checkpointing for the task queue. Default is False.
            If enabled, the app will be checkpointed after the on_start function has completed.
            On next invocation, each container will restore from a checkpoint and resume execution instead of
            booting up from cold.
        inputs (Optional[Schema]):
            The input schema for the task queue. Default is None.
        outputs (Optional[Schema]):
            The output schema for the task queue. Default is None.
    Example:
        ```python
        from beta9 import task_queue, Image

        @task_queue(cpu=1.0, memory=128, gpu="T4", image=Image(python_packages=["torch"]), keep_warm_seconds=1000)
        def transcribe(filename: str):
            print(filename)
            return

        transcribe.put("some_file.mp4")

        ```
    """

    def __init__(
        self,
        app: str = "",
        cpu: Union[int, float, str] = 1.0,
        memory: Union[int, str] = 128,
        gpu: Union[GpuTypeAlias, List[GpuTypeAlias]] = GpuType.NoGPU,
        gpu_count: int = 0,
        image: Image = Image(),
        timeout: int = 3600,
        retries: int = 3,
        workers: int = 1,
        keep_warm_seconds: int = 10,
        max_pending_tasks: int = 100,
        on_start: Optional[Callable] = None,
        on_deploy: Optional[AbstractCallableWrapper] = None,
        callback_url: Optional[str] = None,
        volumes: Optional[List[Union[Volume, CloudBucket]]] = None,
        secrets: Optional[List[str]] = None,
        env: Optional[Dict[str, str]] = {},
        name: Optional[str] = None,
        authorized: bool = True,
        autoscaler: Autoscaler = QueueDepthAutoscaler(),
        task_policy: TaskPolicy = TaskPolicy(),
        checkpoint_enabled: bool = False,
        retry_for: Optional[List[Type[Exception]]] = None,
        pricing: Optional[PricingPolicy] = None,
        inputs: Optional[Schema] = None,
        outputs: Optional[Schema] = None,
    ) -> None:
        super().__init__(
            cpu=cpu,
            memory=memory,
            gpu=gpu,
            gpu_count=gpu_count,
            image=image,
            workers=workers,
            timeout=timeout,
            retries=retries,
            keep_warm_seconds=keep_warm_seconds,
            max_pending_tasks=max_pending_tasks,
            on_start=on_start,
            on_deploy=on_deploy,
            callback_url=callback_url,
            volumes=volumes,
            secrets=secrets,
            env=env,
            name=name,
            authorized=authorized,
            autoscaler=autoscaler,
            task_policy=task_policy,
            checkpoint_enabled=checkpoint_enabled,
            app=app,
            pricing=pricing,
            inputs=inputs,
            outputs=outputs,
        )
        self._taskqueue_stub: Optional[TaskQueueServiceStub] = None
        self.retry_for = retry_for or []

    @property
    def taskqueue_stub(self) -> TaskQueueServiceStub:
        if not self._taskqueue_stub:
            self._taskqueue_stub = TaskQueueServiceStub(self.channel)
        return self._taskqueue_stub

    @taskqueue_stub.setter
    def taskqueue_stub(self, value: TaskQueueServiceStub) -> None:
        self._taskqueue_stub = value

    def __call__(self, func):
        return _CallableWrapper(func, self)


class _CallableWrapper(DeployableMixin):
    deployment_stub_type = TASKQUEUE_DEPLOYMENT_STUB_TYPE

    def __init__(self, func: Callable, parent: TaskQueue):
        self.func: Callable = func
        self.parent: TaskQueue = parent

    def __call__(self, *args, **kwargs) -> Any:
        if not is_local():
            return self.local(*args, **kwargs)

        raise NotImplementedError(
            "Direct calls to TaskQueues are not yet supported."
            + " To enqueue items use .put(*args, **kwargs)"
        )

    def local(self, *args, **kwargs) -> Any:
        return self.func(*args, **kwargs)

    @with_grpc_error_handling
    def serve(self, timeout: int = 0, url_type: str = ""):
        if not self.parent.prepare_runtime(
            func=self.func, stub_type=TASKQUEUE_SERVE_STUB_TYPE, force_create_stub=True
        ):
            return False

        try:
            with terminal.progress("Serving taskqueue..."):
                self.parent.print_invocation_snippet(url_type=url_type)

                return self._serve(dir=os.getcwd(), timeout=timeout)
        except KeyboardInterrupt:
            terminal.header("Stopping serve container")
            terminal.print("Goodbye 👋")
            os._exit(0)  # kills all threads immediately

    def _serve(self, *, dir: str, timeout: int = 0):
        stream = self.parent.taskqueue_stub.start_task_queue_serve(
            StartTaskQueueServeRequest(
                stub_id=self.parent.stub_id,
                timeout=timeout,
            )
        )

        r: StartTaskQueueServeResponse = stream
        if not r.ok:
            return terminal.error(r.error_msg)

        container = Container(container_id=r.container_id)
        container.attach(container_id=r.container_id, sync_dir=dir)

    def put(self, *args, **kwargs) -> bool:
        if not self.parent.prepare_runtime(
            func=self.func,
            stub_type=TASKQUEUE_STUB_TYPE,
        ):
            return False

        payload = {"args": args, "kwargs": kwargs}
        json_payload = json.dumps(payload)

        r: TaskQueuePutResponse = self.parent.taskqueue_stub.task_queue_put(
            TaskQueuePutRequest(stub_id=self.parent.stub_id, payload=json_payload.encode("utf-8"))
        )

        if not r.ok:
            terminal.error("Failed to enqueue task")
            return False

        terminal.detail(f"Enqueued task: {r.task_id}")
        return True

================================================================================
# File: abstractions/volume.py
# Path: src/beta9/abstractions/volume.py
================================================================================

from dataclasses import dataclass
from typing import Optional

from ..abstractions.base import BaseAbstraction
from ..clients.gateway import Volume as VolumeGateway
from ..clients.types import MountPointConfig as VolumeConfigGateway
from ..clients.volume import GetOrCreateVolumeRequest, GetOrCreateVolumeResponse, VolumeServiceStub


class Volume(BaseAbstraction):
    def __init__(
        self,
        name: str,
        mount_path: str,
    ) -> None:
        """
        Creates a Volume instance.

        When your container runs, your volume will be available at `./{name}` and `/volumes/{name}`.

        Parameters:
            name (str):
                The name of the volume, a descriptive identifier for the data volume. Note that when using an external provider, the name must be the same as the bucket name.
            mount_path (str):
                The path where the volume is mounted within the container environment.
            config (Optional[VolumeConfig]):
                Optional configuration for the volumes from external providers (AWS, Cloudflare, Tigris).

        Example:
            ```python
            from beta9 import Volume, VolumeConfig

            # Shared Volume
            shared_volume = Volume(name="model_weights", mount_path="./my-weights")

            @function(volumes=[shared_volume])
            def my_function():
                pass
            ```
        """
        super().__init__()

        self.name = name
        self.ready = False
        self.volume_id = None
        self.mount_path = mount_path
        self._stub: Optional[VolumeServiceStub] = None

    @property
    def stub(self) -> VolumeServiceStub:
        if not self._stub:
            self._stub = VolumeServiceStub(self.channel)
        return self._stub

    @stub.setter
    def stub(self, value: VolumeServiceStub):
        self._stub = value

    def get_or_create(self) -> bool:
        resp: GetOrCreateVolumeResponse
        resp = self.stub.get_or_create_volume(GetOrCreateVolumeRequest(name=self.name))

        if resp.ok:
            self.ready = True
            self.volume_id = resp.volume.id
            return True

        return False

    def export(self):
        vol = VolumeGateway(
            id=self.volume_id,
            mount_path=self.mount_path,
        )

        return vol


@dataclass
class CloudBucketConfig:
    """
    Configuration for a cloud bucket.

    Parameters:
        read_only (bool):
            Whether the volume is read-only.
        force_path_style (bool):
            Whether to use the force path style option while mounting the volume (some non-AWS S3 providers require this).
        access_key (str):
            The name of the secret containing the S3 access key for the external provider.
        secret_key (str):
            The name of the secret containing the S3 secret key for the external provider.
        endpoint (Optional[str]):
            The S3 endpoint for the external provider.
        region (Optional[str]):
            The region for the external provider.
    """

    read_only: bool = False
    force_path_style: bool = False
    access_key: Optional[str] = None
    secret_key: Optional[str] = None
    endpoint: Optional[str] = None
    region: Optional[str] = None


class CloudBucket(Volume):
    def __init__(self, name: str, mount_path: str, config: CloudBucketConfig) -> None:
        """
        Creates a CloudBucket instance.

        When your container runs, your cloud bucket will be available at `./{name}` and `/volumes/{name}`.

        Parameters:
            name (str):
                The name of the cloud bucket, must be the same as the bucket name in the cloud provider.
            mount_path (str):
                The path where the cloud bucket is mounted within the container environment.
            config (CloudBucketConfig):
                Configuration for the cloud bucket.

        Example:
            ```python
            from beta9 import CloudBucket, CloudBucketConfig

            # Cloud Bucket
            cloud_bucket = CloudBucket(
                name="other_model_weights",
                mount_path="./other-weights",
                config=CloudBucketConfig(
                    access_key="MY_ACCESS_KEY_SECRET",
                    secret_key="MY_SECRET_KEY_SECRET",
                    endpoint="https://s3-endpoint.com",
                ),
            )

            @function(volumes=[cloud_bucket])
            def my_function():
                pass
            ```
        """
        super().__init__(name, mount_path)
        self.config = config

    def get_or_create(self) -> bool:
        return True

    def export(self):
        vol = super().export()
        vol.config = VolumeConfigGateway(
            bucket_name=self.name,
            access_key=self.config.access_key,
            secret_key=self.config.secret_key,
            endpoint_url=self.config.endpoint,
            region=self.config.region,
            read_only=self.config.read_only,
            force_path_style=self.config.force_path_style,
        )
        return vol

================================================================================
# File: aio.py
# Path: src/beta9/aio.py
================================================================================

import asyncio
from asyncio import AbstractEventLoop
from typing import Any, Coroutine, Union


def run_sync(coroutine: Coroutine, loop: Union[AbstractEventLoop, None] = None) -> Any:
    if loop is None:
        loop = asyncio.get_event_loop()

    return loop.run_until_complete(coroutine)

================================================================================
# File: channel.py
# Path: src/beta9/channel.py
================================================================================

import functools
import sys
import traceback
from abc import ABC, abstractmethod
from contextlib import contextmanager
from typing import Any, Callable, Generator, List, NewType, Optional, Sequence, Tuple, cast

import grpc
from grpc import ChannelCredentials, RpcError
from grpc._interceptor import _Channel as InterceptorChannel

from . import terminal
from .clients.gateway import AuthorizeRequest, AuthorizeResponse, GatewayServiceStub
from .clients.secret import SecretServiceStub
from .clients.volume import VolumeServiceStub
from .config import (
    DEFAULT_CONTEXT_NAME,
    ConfigContext,
    SDKSettings,
    get_config_context,
    load_config,
    prompt_for_config_context,
    save_config,
)
from .env import is_remote
from .exceptions import RunnerException

GRPC_MAX_MESSAGE_SIZE = 16 * 1024 * 1024


def channel_reconnect_event(connect_status: grpc.ChannelConnectivity) -> None:
    if connect_status not in (
        grpc.ChannelConnectivity.CONNECTING,
        grpc.ChannelConnectivity.IDLE,
        grpc.ChannelConnectivity.READY,
        grpc.ChannelConnectivity.SHUTDOWN,
    ):
        terminal.warn("Connection lost, reconnecting...")


class Channel(InterceptorChannel):
    def __init__(
        self,
        addr: str,
        token: Optional[str] = None,
        credentials: Optional[ChannelCredentials] = None,
        options: Optional[Sequence[Tuple[str, Any]]] = None,
        retry: Tuple[Callable[[grpc.ChannelConnectivity], None], bool] = (
            channel_reconnect_event,
            True,
        ),
    ):
        if options is None:
            options = [
                ("grpc.max_receive_message_length", GRPC_MAX_MESSAGE_SIZE),
                ("grpc.max_send_message_length", GRPC_MAX_MESSAGE_SIZE),
            ]

        if credentials is not None:
            channel = grpc.secure_channel(addr, credentials, options=options)
        elif addr.endswith("443"):
            channel = grpc.secure_channel(addr, grpc.ssl_channel_credentials(), options=options)
        else:
            channel = grpc.insecure_channel(addr, options=options)

        # NOTE: we observed that in a multiprocessing context, this
        # retry mechanism did not work as expected. We're not sure why,
        # but for now, just don't subscribe to these events in containers
        if not is_remote():
            channel.subscribe(*retry)

        interceptor = AuthTokenInterceptor(token)
        super().__init__(channel=channel, interceptor=interceptor)


MetadataType = NewType("MetadataType", List[Tuple[Any, Any]])


class ClientCallDetails(ABC):
    @property
    @abstractmethod
    def metadata(self) -> MetadataType:
        pass

    @abstractmethod
    def _replace(self, metadata: MetadataType) -> "ClientCallDetails":
        pass


class AuthTokenInterceptor(
    grpc.UnaryUnaryClientInterceptor,
    grpc.UnaryStreamClientInterceptor,
    grpc.StreamUnaryClientInterceptor,
    grpc.StreamStreamClientInterceptor,
):
    """A generic interceptor to add an authentication token to gRPC requests."""

    def __init__(self, token: Optional[str] = None):
        """Initialize the interceptor with an optional authentication token."""
        self._token = token

    def _add_auth_metadata(
        self,
        client_call_details: ClientCallDetails,
    ) -> ClientCallDetails:
        """Add authentication metadata to the client call."""
        if self._token:
            auth_headers = [("authorization", f"Bearer {self._token}")]
            if client_call_details.metadata is not None:
                new_metadata = client_call_details.metadata + auth_headers
            else:
                new_metadata = auth_headers
        else:
            new_metadata = client_call_details.metadata

        return client_call_details._replace(metadata=cast(MetadataType, new_metadata))

    def intercept_call(self, continuation, client_call_details, request):
        """Intercept all types of calls to add auth token."""
        new_details = self._add_auth_metadata(client_call_details)

        return continuation(new_details, request)

    def intercept_call_stream(self, continuation, client_call_details, request_iterator):
        return self.intercept_call(continuation, client_call_details, request=request_iterator)

    # Implement the four necessary interceptor methods using intercept_call
    intercept_unary_unary = intercept_call
    intercept_unary_stream = intercept_call
    intercept_stream_unary = intercept_call_stream
    intercept_stream_stream = intercept_call_stream


def handle_grpc_error(error: grpc.RpcError):
    code = error.code()
    details = error.details()

    if code == grpc.StatusCode.UNAUTHENTICATED:
        terminal.error("Unauthorized: Invalid auth token provided.")
    elif code == grpc.StatusCode.UNAVAILABLE:
        terminal.error("Unable to connect to gateway.")
    elif code == grpc.StatusCode.CANCELLED:
        return
    elif code == grpc.StatusCode.RESOURCE_EXHAUSTED:
        terminal.error("Please ensure your payload or function arguments are less than 4 MiB.")
    elif code == grpc.StatusCode.UNKNOWN:
        terminal.error(f"Error {details}")
    else:
        terminal.error(f"Unhandled GRPC error: {code}")


def with_grpc_error_handling(func: Callable) -> Callable:
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except grpc.RpcError as e:
            handle_grpc_error(e)

    return wrapper


def get_channel(context: Optional[ConfigContext] = None) -> Channel:
    if not context:
        _, context = prompt_for_config_context()

    return Channel(
        addr=f"{context.gateway_host}:{context.gateway_port}",
        token=context.token,
    )


def prompt_first_auth(settings: SDKSettings) -> None:
    terminal.header(f"Welcome to {settings.name.title()}! Let's get started 📡")
    terminal.print(settings.ascii_logo, highlight=True)

    name, context = prompt_for_config_context(
        name=DEFAULT_CONTEXT_NAME,
        gateway_host=settings.gateway_host,
        gateway_port=settings.gateway_port,
    )

    channel = Channel(
        addr=f"{context.gateway_host}:{context.gateway_port}",
        token=context.token,
    )

    terminal.header("Authorizing with gateway")
    with ServiceClient.with_channel(channel) as client:
        res: AuthorizeResponse
        res = client.gateway.authorize(AuthorizeRequest())
        if not res.ok:
            terminal.error(f"Unable to authorize with gateway: {res.error_msg}")

        terminal.header("Authorized 🎉")

    # Set new token, if one was returned
    context.token = res.new_token if res.new_token else context.token

    # Load config, add new context
    contexts = load_config(settings.config_path)
    contexts[name] = context
    contexts[DEFAULT_CONTEXT_NAME] = context

    # Write updated contexts to config
    save_config(contexts, settings.config_path)


def pass_channel(func: Callable) -> Callable:
    @functools.wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> Any:
        config = get_config_context()
        with get_channel(config) as channel:
            return func(*args, **kwargs, channel=channel)

    return wrapper


@contextmanager
def handle_error():
    exit_code = 0
    try:
        yield
    except RpcError as exc:
        handle_grpc_error(exc)
    except RunnerException as exc:
        exit_code = exc.code
    except SystemExit as exc:
        exit_code = exc.code
        raise
    except BaseException:
        exit_code = 1
    finally:
        if exit_code != 0:
            print(traceback.format_exc())
            sys.exit(exit_code)


@contextmanager
def runner_context() -> Generator[Channel, None, None]:
    with handle_error():
        config = get_config_context()
        channel = get_channel(config)
        yield channel
        channel.close()


def with_runner_context(func: Callable) -> Callable:
    @functools.wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> Any:
        with runner_context() as c:
            return func(*args, **kwargs, channel=c)

    return wrapper


class ServiceClient:
    def __init__(self, config: Optional[ConfigContext] = None) -> None:
        self._config: Optional[ConfigContext] = config
        self._channel: Optional[Channel] = None
        self._gateway: Optional[GatewayServiceStub] = None
        self._volume: Optional[VolumeServiceStub] = None
        self._secret: Optional[SecretServiceStub] = None

    def __enter__(self) -> "ServiceClient":
        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        if self._channel:
            self._channel.close()

    @classmethod
    def with_channel(cls, channel: Channel) -> "ServiceClient":
        self = cls()
        self.channel = channel
        return self

    @property
    def channel(self) -> Channel:
        if not self._channel:
            self._channel = get_channel(self._config)
        return self._channel

    @channel.setter
    def channel(self, value) -> None:
        if not value or not isinstance(value, Channel):
            raise ValueError("Invalid channel")
        self._channel = value

    @property
    def gateway(self) -> GatewayServiceStub:
        if not self._gateway:
            self._gateway = GatewayServiceStub(self.channel)
        return self._gateway

    @property
    def volume(self) -> VolumeServiceStub:
        if not self._volume:
            self._volume = VolumeServiceStub(self.channel)
        return self._volume

    @property
    def secret(self) -> SecretServiceStub:
        if not self._secret:
            self._secret = SecretServiceStub(self.channel)
        return self._secret

    def close(self) -> None:
        if self._channel:
            self._channel.close()

================================================================================
# File: cli/__init__.py
# Path: src/beta9/cli/__init__.py
================================================================================


================================================================================
# File: cli/config.py
# Path: src/beta9/cli/config.py
================================================================================

import json
from pathlib import Path
from typing import Any

import click
from rich.style import Style
from rich.table import Column, Table, box

from .. import terminal
from ..channel import ServiceClient
from ..cli import extraclick
from ..clients.gateway import ExportWorkspaceConfigRequest
from ..config import (
    DEFAULT_CONTEXT_NAME,
    ConfigContext,
    get_settings,
    load_config,
    prompt_for_config_context,
    save_config,
)
from .extraclick import ClickManagementGroup


def get_setting_callback(ctx: click.Context, param: click.Parameter, value: Any):
    return getattr(get_settings(), param.name) if not value else value


@click.group(
    name="config",
    help="Manage configuration contexts.",
    cls=ClickManagementGroup,
)
def management():
    pass


@management.command(
    name="list",
    help="Lists available contexts.",
)
@click.option(
    "--show-token",
    is_flag=True,
    required=False,
    default=False,
    help="Display token.",
)
@click.option(
    "--config-path",
    type=click.Path(),
    required=False,
    callback=get_setting_callback,
    help="Path to a config file.",
)
def list_contexts(show_token: bool, config_path: Path):
    contexts = load_config(config_path)

    table = Table(
        Column("Name"),
        Column("Host"),
        Column("Port"),
        Column("Token"),
        box=box.SIMPLE,
    )

    for name, context in contexts.items():
        # Style default context
        style = Style(bold=True) if name == DEFAULT_CONTEXT_NAME else Style()
        table.add_row(
            name,
            context.gateway_host,
            str(context.gateway_port),
            context.token
            if show_token
            else str(context.token)[0:6] + "..."
            if context.token and len(context.token) > 6
            else context.token,
            style=style,
        )

    terminal.print(table)


@management.command(
    name="delete",
    help="Delete a context.",
)
@click.argument(
    "name",
    type=click.STRING,
    required=True,
)
@click.option(
    "--config-path",
    type=click.Path(),
    required=False,
    callback=get_setting_callback,
    help="Path to a config file.",
)
@click.confirmation_option("--force")
def delete_context(name: str, config_path: Path):
    contexts = load_config(config_path)

    if name == DEFAULT_CONTEXT_NAME:
        contexts[name] = ConfigContext()

    elif name in contexts:
        del contexts[name]

    save_config(contexts=contexts, path=config_path)
    terminal.success(f"Deleted context {name}.")


@management.command(
    name="create",
    help="Create a new context.",
)
@click.argument("name", type=click.STRING, required=True)
@click.option("--token", type=click.STRING)
@click.option(
    "--gateway-host",
    type=click.STRING,
    callback=get_setting_callback,
)
@click.option(
    "--gateway-port",
    type=click.INT,
    callback=get_setting_callback,
)
@click.option(
    "--config-path",
    type=click.Path(),
    required=False,
    callback=get_setting_callback,
    help="Path to a config file.",
)
def create_context(config_path: Path, **kwargs):
    contexts = load_config(config_path)

    if name := kwargs.get("name"):
        if name in contexts and contexts[name].is_valid():
            text = f"Context '{name}' already exists. Overwrite?"
            if terminal.prompt(text=text, default="n").lower() in ["n", "no"]:
                return

    # Prompt user for context settings
    name, context = prompt_for_config_context(require_token=True, **kwargs)

    # Save context to config
    contexts[name] = context
    save_config(contexts=contexts, path=config_path)

    terminal.success(f"Added new context to {config_path}")


@management.command(
    name="select",
    help="Set a default context. This will overwrite the current default context.",
)
@click.argument(
    "name",
    type=click.STRING,
    required=True,
)
@click.option(
    "--config-path",
    type=click.Path(),
    required=False,
    callback=get_setting_callback,
    help="Path to a config file.",
)
def select_context(name: str, config_path: Path):
    contexts = load_config(config_path)

    if name not in contexts:
        terminal.error(f"Context '{name}' does not exist.")

    contexts[DEFAULT_CONTEXT_NAME] = contexts[name]
    save_config(contexts=contexts, path=config_path)

    terminal.success(f"Default context updated with '{name}'.")


@management.command(
    name="export",
    help="Export the current workspace configuration",
)
@click.argument(
    "name",
    type=click.STRING,
    default=DEFAULT_CONTEXT_NAME,
    required=False,
)
@extraclick.pass_service_client
def export_config(
    service: ServiceClient,
    name: str,
):
    contexts = load_config()
    context = contexts.get(name)

    if not context:
        terminal.error(f"Context '{name}' does not exist.")

    config = service.gateway.export_workspace_config(ExportWorkspaceConfigRequest()).to_dict()
    config["token"] = context.token
    terminal.print(json.dumps(config, indent=2))

================================================================================
# File: cli/container.py
# Path: src/beta9/cli/container.py
================================================================================

import datetime
from typing import List

import click
from betterproto import Casing
from rich.table import Column, Table, box

from .. import terminal
from ..abstractions.base.container import Container
from ..channel import ServiceClient
from ..cli import extraclick
from ..clients.gateway import (
    ListContainersRequest,
    StopContainerRequest,
    StopContainerResponse,
)
from .extraclick import ClickCommonGroup, ClickManagementGroup


@click.group(cls=ClickCommonGroup)
def common(**_):
    pass


@click.group(
    name="container",
    help="Manage containers.",
    cls=ClickManagementGroup,
)
def management():
    pass


AVAILABLE_LIST_COLUMNS = {
    "container_id": "ID",
    "status": "Status",
    "stub_id": "Stub ID",
    "deployment_id": "Deployment ID",
    "scheduled_at": "Scheduled At",
    "uptime": "Uptime",
    "worker_id": "Worker ID",
    "machine_id": "Machine ID",
}


@management.command(
    name="list",
    help="""
    List all current containers.
    """,
)
@click.option(
    "--format",
    type=click.Choice(("table", "json")),
    default="table",
    show_default=True,
    help="Change the format of the output.",
)
@click.option(
    "--columns",
    type=click.STRING,
    default="container_id,status,stub_id,scheduled_at,deployment_id,uptime",
    help="""
      Specify columns to display.
      Available columns: container_id, status, stub_id, scheduled_at, deployment_id, uptime
    """,
)
@extraclick.pass_service_client
@click.pass_context
def list_containers(ctx: click.Context, service: ServiceClient, format: str, columns: str):
    res = service.gateway.list_containers(ListContainersRequest())
    if not res.ok:
        terminal.error(res.error_msg)

    now = datetime.datetime.now(datetime.timezone.utc)
    if format == "json":
        containers = []
        for c in res.containers:
            container_dict = c.to_dict(casing=Casing.SNAKE)
            container_dict["uptime"] = (
                terminal.humanize_duration(now - c.started_at) if c.started_at else "N/A"
            )
            containers.append(container_dict)
        terminal.print_json(containers)
        return

    user_requested_columns = set(columns.split(","))

    # If admin columns are present on every container, include them.
    add_admin_columns = all(c.worker_id for c in res.containers)
    if add_admin_columns:
        user_requested_columns.update(["worker_id", "machine_id"])

    # Build the ordered list of columns based on the ordering of AVAILABLE_LIST_COLUMNS
    ordered_columns = [
        col for col in AVAILABLE_LIST_COLUMNS.keys() if col in user_requested_columns
    ]

    table_cols = [Column(AVAILABLE_LIST_COLUMNS[col]) for col in ordered_columns]

    if len(res.containers) == 0:
        terminal.print("No containers found.")
        return

    table = Table(*table_cols, box=box.SIMPLE)
    for container in res.containers:
        row = []
        for col in ordered_columns:
            if col == "uptime":
                value = (
                    terminal.humanize_duration(now - container.started_at)
                    if container.started_at
                    else "N/A"
                )
            else:
                value = getattr(container, col)
                if isinstance(value, datetime.datetime):
                    value = terminal.humanize_date(value)
            row.append(value)
        table.add_row(*row)

    table.add_section()
    table.add_row(f"[bold]{len(res.containers)} items")
    terminal.print(table)


@management.command(
    name="stop",
    help="Stop a container.",
)
@click.argument(
    "container_ids",
    nargs=-1,
    required=True,
)
@extraclick.pass_service_client
def stop_container(service: ServiceClient, container_ids: List[str]):
    for container_id in container_ids:
        res: StopContainerResponse
        res = service.gateway.stop_container(StopContainerRequest(container_id=container_id))

        if res.ok:
            terminal.success(f"Stopped container: {container_id}")
        else:
            terminal.error(f"{res.error_msg}", exit=False)


@management.command(
    name="attach",
    help="Attach to a running container.",
)
@click.argument(
    "container_id",
    required=True,
)
@extraclick.pass_service_client
def attach_to_container(_: ServiceClient, container_id: str):
    container = Container(container_id=container_id)
    container.attach(container_id=container_id)

================================================================================
# File: cli/deployment.py
# Path: src/beta9/cli/deployment.py
================================================================================

from typing import Dict, List

import click
from betterproto import Casing
from rich.table import Column, Table, box

from .. import terminal
from ..channel import ServiceClient
from ..cli import extraclick
from ..clients.gateway import (
    DeleteDeploymentRequest,
    DeleteDeploymentResponse,
    ListDeploymentsRequest,
    ListDeploymentsResponse,
    ScaleDeploymentRequest,
    ScaleDeploymentResponse,
    StartDeploymentRequest,
    StartDeploymentResponse,
    StopDeploymentRequest,
    StopDeploymentResponse,
    StringList,
)
from ..utils import load_module_spec
from .extraclick import (
    ClickCommonGroup,
    ClickManagementGroup,
    handle_config_override,
    override_config_options,
)


@click.group(cls=ClickCommonGroup)
def common(**_):
    pass


@common.command(
    name="deploy",
    help="""
    Deploy a new function.

    HANDLER is in the format of "file:function".
    """,
    epilog="""
      Examples:

        {cli_name} deploy --name my-app app.py:my_func

        {cli_name} deploy -n my-app-2 app.py:my_func
        \b
    """,
)
@click.option(
    "--name",
    "-n",
    type=click.STRING,
    help="The name the deployment.",
    required=False,
)
@click.argument(
    "handler",
    nargs=1,
    required=False,
)
@click.option(
    "--url-type",
    help="The type of URL to get back. [default is determined by the server] ",
    type=click.Choice(["host", "path"]),
)
@override_config_options
@click.pass_context
def deploy(
    ctx: click.Context,
    name: str,
    handler: str,
    url_type: str,
    **kwargs,
):
    ctx.invoke(
        create_deployment,
        name=name,
        handler=handler,
        url_type=url_type,
        **kwargs,
    )


@click.group(
    name="deployment",
    help="Manage deployments.",
    cls=ClickManagementGroup,
)
def management():
    pass


def _generate_pod_module(name: str, entrypoint: str):
    from beta9.abstractions.pod import Pod

    pod = Pod(
        name=name,
        entrypoint=entrypoint,
    )

    return pod


@management.command(
    name="create",
    help="Create a new deployment.",
    epilog="""
      Examples:

        {cli_name} deploy --name my-app --entrypoint app.py:handler
        \b
    """,
)
@click.option(
    "--name",
    "-n",
    help="The name of the deployment.",
    required=False,
)
@click.option(
    "--handler",
    help='The name the handler e.g. "file:function" or script to run.',
)
@click.option(
    "--url-type",
    help="The type of URL to get back. [default is determined by the server] ",
    type=click.Choice(["host", "path"]),
)
@override_config_options
@extraclick.pass_service_client
def create_deployment(
    service: ServiceClient,
    name: str,
    handler: str,
    url_type: str,
    **kwargs,
):
    module = None
    entrypoint = kwargs["entrypoint"]
    if handler:
        user_obj, module_name, obj_name = load_module_spec(handler, "deploy")

        if hasattr(user_obj, "set_handler"):
            user_obj.set_handler(f"{module_name}:{obj_name}")

    elif entrypoint:
        user_obj = _generate_pod_module(name, entrypoint)

    else:
        terminal.error("No handler or entrypoint specified")
        return

    if not handle_config_override(user_obj, kwargs):
        return

    if not module and hasattr(user_obj, "generate_deployment_artifacts"):
        user_obj.generate_deployment_artifacts(**kwargs)

    if not user_obj.deploy(name=name, context=service._config, url_type=url_type):  # type: ignore
        terminal.error("Deployment failed ☠️")

    if not module and hasattr(user_obj, "cleanup_deployment_artifacts"):
        user_obj.cleanup_deployment_artifacts()


@management.command(
    name="list",
    help="List all deployments.",
    epilog="""
    Examples:

      # List the first 10 deployments
      {cli_name} deployment list --limit 10

      # List deployments that are at version 9
      {cli_name} deployment list --filter version=9

      # List deployments that are not active
      {cli_name} deployment list --filter active=false
      {cli_name} deployment list --filter active=no

      # List deployments and output in JSON format
      {cli_name} deployment list --format json
      \b
    """,
)
@click.option(
    "--limit",
    type=click.IntRange(1, 100),
    default=20,
    help="The number of deployments to fetch.",
)
@click.option(
    "--format",
    type=click.Choice(("table", "json")),
    default="table",
    show_default=True,
    help="Change the format of the output.",
)
@click.option(
    "--filter",
    multiple=True,
    callback=extraclick.filter_values_callback,
    help="Filters deployments. Add this option for each field you want to filter on.",
)
@extraclick.pass_service_client
def list_deployments(
    service: ServiceClient,
    limit: int,
    format: str,
    filter: Dict[str, StringList],
):
    res: ListDeploymentsResponse
    res = service.gateway.list_deployments(ListDeploymentsRequest(filter, limit))

    if not res.ok:
        terminal.error(res.err_msg)

    if format == "json":
        deployments = [d.to_dict(casing=Casing.SNAKE) for d in res.deployments]  # type:ignore
        terminal.print_json(deployments)
        return

    table = Table(
        Column("ID"),
        Column("Name"),
        Column("Active"),
        Column("Version", justify="right"),
        Column("Created At"),
        Column("Updated At"),
        Column("Stub Name"),
        Column("Workspace Name"),
        box=box.SIMPLE,
    )

    for deployment in res.deployments:
        table.add_row(
            deployment.id,
            deployment.name,
            "Yes" if deployment.active else "No",
            str(deployment.version),
            terminal.humanize_date(deployment.created_at),
            terminal.humanize_date(deployment.updated_at),
            deployment.stub_name,
            deployment.workspace_name,
        )

    table.add_section()
    table.add_row(f"[bold]{len(res.deployments)} items")
    terminal.print(table)


@management.command(
    name="stop",
    help="Stop deployments.",
    epilog="""
    Examples:

      # Stop a deployment
      {cli_name} deployment stop 5bd2e248-6d7c-417b-ac7b-0b92aa0a5572

      # Stop multiple deployments
      {cli_name} deployment stop 5bd2e248-6d7c-417b-ac7b-0b92aa0a5572 7b968ad5-c001-4df3-ba05-e99895aa9596
      \b
    """,
)
@click.argument(
    "deployment_ids",
    nargs=-1,
    type=click.STRING,
    required=True,
)
@extraclick.pass_service_client
def stop_deployments(service: ServiceClient, deployment_ids: List[str]):
    for id in deployment_ids:
        res: StopDeploymentResponse
        res = service.gateway.stop_deployment(StopDeploymentRequest(id))

        if not res.ok:
            terminal.error(res.err_msg, exit=False)
            continue

        terminal.success(f"Stopped deployment: {id}")


@management.command(
    name="start",
    help="Start an inactive deployment.",
    epilog="""
    Examples:

        # Start a deployment
        {cli_name} deployment start 5bd2e248-6d7c-417b-ac7b-0b92aa0a5572
        """,
)
@click.argument(
    "deployment_id",
    type=click.STRING,
    required=True,
)
@extraclick.pass_service_client
def start_deployment(service: ServiceClient, deployment_id: str):
    res: StartDeploymentResponse
    res = service.gateway.start_deployment(StartDeploymentRequest(id=deployment_id))

    if not res.ok:
        terminal.error(res.err_msg)

    terminal.print(f"Starting deployment: {deployment_id}")


@management.command(
    name="delete",
    help="Delete a deployment.",
    epilog="""
    Examples:

        # Delete a deployment
        {cli_name} deployment delete 5bd2e248-6d7c-417b-ac7b-0b92aa0a5572
        \b
     """,
)
@click.argument(
    "deployment_id",
    nargs=1,
    type=click.STRING,
    required=True,
)
@extraclick.pass_service_client
def delete_deployment(service: ServiceClient, deployment_id: str):
    res: DeleteDeploymentResponse
    res = service.gateway.delete_deployment(DeleteDeploymentRequest(deployment_id))

    if not res.ok:
        terminal.error(res.err_msg)

    terminal.print(f"Deleted {deployment_id}")


@management.command(
    name="scale",
    help="Scale an active deployment.",
    epilog="""
    Examples:

        # Start a deployment
        {cli_name} deployment scale 5bd2e248-6d7c-417b-ac7b-0b92aa0a5572 --containers 2
        """,
)
@click.argument(
    "deployment_id",
    type=click.STRING,
    required=True,
)
@click.option(
    "--containers",
    type=click.INT,
    required=True,
    help="The number of containers to scale to.",
)
@extraclick.pass_service_client
def scale_deployment(service: ServiceClient, deployment_id: str, containers: int):
    res: ScaleDeploymentResponse
    res = service.gateway.scale_deployment(
        ScaleDeploymentRequest(id=deployment_id, containers=containers)
    )

    if not res.ok:
        terminal.error(res.err_msg)

    terminal.print(f"Scaled deployment {deployment_id} to {containers} containers")

================================================================================
# File: cli/dev.py
# Path: src/beta9/cli/dev.py
================================================================================

import click

from ..abstractions.pod import Pod
from ..channel import ServiceClient
from ..cli import extraclick
from ..utils import load_module_spec
from .extraclick import ClickCommonGroup, handle_config_override, override_config_options


@click.group(cls=ClickCommonGroup)
def common(**_):
    pass


@common.command(
    name="dev",
    help="""
    Spins up a remote environment to develop in. Will automatically sync the current directory to the container.
    You can optionally specify a handler to use as the base environment, or leave it blank to use a default environment.
    """,
    epilog="""
      Examples:

        {cli_name} dev app.py:handler

        {cli_name} dev app.py:my_func --gpu T4

        {cli_name} dev --sync ./newproj
        \b
    """,
)
@click.argument(
    "handler",
    nargs=1,
    required=False,
)
@click.option(
    "--sync",
    help="The directory to sync to the container",
    default="./",
)
@click.option(
    "--url-type",
    help="The type of URL to get back. [default is determined by the server] ",
    type=click.Choice(["host", "path"]),
)
@override_config_options
@extraclick.pass_service_client
@click.pass_context
def dev(
    ctx: click.Context,
    service: ServiceClient,
    handler: str,
    sync: str,
    url_type: str = "path",
    **kwargs,
):
    entrypoint = kwargs["entrypoint"]
    if handler:
        user_obj, module_name, obj_name = load_module_spec(handler, "dev")

        if hasattr(user_obj, "set_handler"):
            user_obj.set_handler(f"{module_name}:{obj_name}")
    elif entrypoint:
        user_obj = Pod(entrypoint=entrypoint)
    else:
        user_obj = Pod()

    if not handle_config_override(user_obj, kwargs):
        return

    user_obj.shell(url_type=url_type, sync_dir=sync)  # type:ignore

================================================================================
# File: cli/extraclick.py
# Path: src/beta9/cli/extraclick.py
================================================================================

import functools
import inspect
import shlex
import sys
import textwrap
from gettext import gettext
from typing import Any, Callable, Dict, List, Optional

import click

from .. import terminal
from ..abstractions import base as base_abstraction
from ..abstractions.image import Image
from ..channel import ServiceClient, with_grpc_error_handling
from ..clients.gateway import (
    StringList,
)
from ..config import DEFAULT_CONTEXT_NAME, get_config_context
from ..utils import get_init_args_kwargs

CLICK_CONTEXT_SETTINGS = dict(
    help_option_names=["-h", "--help"],
    show_default=True,
)

config_context_param = click.Option(
    param_decls=["-c", "--context"],
    default=DEFAULT_CONTEXT_NAME,
    required=False,
    help="The config context to use.",
    hidden=False,
)

config_context_option = click.option(
    "-c",
    "--context",
    required=config_context_param.required,
    help=config_context_param.help,
    hidden=config_context_param.hidden,
)


class Beta9Command(click.Command):
    def cli_name(self, ctx: click.Context) -> str:
        name, *_ = ctx.command_path.split()
        return name

    def format_epilog(self, ctx: click.Context, formatter: click.HelpFormatter):
        """
        Writes the epilog text to the formatter if it exists.
        """
        if not self.epilog:
            return

        name = self.cli_name(ctx)
        text = self.epilog.format(cli_name=name)
        text = textwrap.dedent(text)
        formatter.write(text)
        formatter.write("\n")

    def format_help_text(self, ctx: click.Context, formatter: click.HelpFormatter) -> None:
        """
        Writes the help text to the formatter if it exists.
        """
        if self.help is not None:
            # truncate the help text to the first form feed
            text = inspect.cleandoc(self.help).partition("\f")[0]
        else:
            text = ""

        if self.deprecated:
            text = gettext("(Deprecated) {text}").format(text=text)

        if text:
            name = self.cli_name(ctx)
            text = text.format(cli_name=name)

            formatter.write_paragraph()

            with formatter.indentation():
                text = textwrap.indent(text, " " * formatter.current_indent)
                formatter.write(text)
                formatter.write("\n")


class ClickCommonGroup(click.Group):
    command_class = Beta9Command

    def list_commands(self, ctx) -> List[str]:
        return list(self.commands)


class ClickManagementGroup(click.Group):
    command_class = Beta9Command

    def list_commands(self, ctx) -> List[str]:
        return list(self.commands)


class CommandGroupCollection(click.CommandCollection):
    def __init__(self, *args, **kwargs):
        params = kwargs.get("params", [])
        params.append(config_context_param)
        kwargs["params"] = params

        super().__init__(*args, **kwargs)

    def add_command(self, cmd: click.MultiCommand):
        """
        Alias method so it looks like a group.
        """
        return self.add_source(cmd)

    @property
    def sources_map(self) -> Dict[str, click.Group]:
        """
        A dictionary representation of {"command name": click_group}.
        """
        r = {}
        for source in self.sources:
            if not isinstance(source, click.Group):
                continue
            for command in source.commands:
                r[command] = source

        return r

    def invoke(self, ctx: click.Context) -> Any:
        if ctx.protected_args:
            if group := self.sources_map.get(ctx.protected_args[0]):
                group.invoke(ctx)
            else:
                print(self.get_help(ctx))
                sys.exit(1)
        else:
            super().invoke(ctx)

    def format_commands(self, ctx: click.Context, formatter: click.HelpFormatter) -> None:
        """
        Extra format methods for multi methods that adds all the commands after
        the options.
        """
        commands = {}
        commands["common"] = []
        commands["management"] = []

        for subcommand in self.list_commands(ctx):
            cmd = self.get_command(ctx, subcommand)
            if cmd is None:
                continue
            if cmd.hidden:
                continue

            if isinstance(cmd, ClickManagementGroup):
                commands["management"].append((subcommand, cmd))
            else:
                commands["common"].append((subcommand, cmd))

        # sort the management commands
        commands["management"].sort(key=lambda x: x[0])

        for cmdtype, cmds in commands.items():
            if not len(cmds):
                continue

            limit = formatter.width - 6 - max(len(cmd[0]) for cmd in cmds)

            rows = []
            for subcommand, cmd in cmds:
                help = cmd.get_short_help_str(limit)
                rows.append((subcommand, help))

            if rows:
                with formatter.section(gettext(cmdtype.title() + " Commands")):
                    formatter.write_dl(rows)

    def list_commands(self, ctx):
        sources = []
        for source in self.sources:
            sources.extend(source.list_commands(ctx))
        return sources


def pass_service_client(func: Callable):
    """
    Decorator that sets a ServiceClient as the first argument.

    We take the right most --context option from the command and work
    our way left of each subcommand. If no --context option is found, we use
    the default value.
    """

    @config_context_option
    @functools.wraps(func)
    @with_grpc_error_handling
    def decorator(context: Optional[str] = None, *args, **kwargs):
        ctx = click.get_current_context()

        context = context or ctx.params.get("context", None)

        if context is None and hasattr(ctx, "parent") and hasattr(ctx.parent, "params"):
            context = ctx.parent.params.get("context", None)

        if (
            context is None
            and hasattr(ctx, "parent")
            and hasattr(ctx.parent, "parent")
            and hasattr(ctx.parent.parent, "params")
        ):
            context = ctx.parent.parent.params.get("context", "")

        config = get_config_context(context or DEFAULT_CONTEXT_NAME)

        with ServiceClient(config) as client:
            base_abstraction.set_channel(client.channel)

            return func(client, *args, **kwargs)

    return decorator


def filter_values_callback(
    ctx: click.Context,
    param: click.Option,
    values: List[str],
) -> Dict[str, StringList]:
    filters: Dict[str, StringList] = {}

    for value in values:
        key, value = value.split("=")
        value_list = value.split(",") if "," in value else [value]

        if key == "status":
            value_list = [v.upper() for v in value_list]

        if not key or not value:
            raise click.BadParameter("Filter must be in the format key=value")

        filters[key] = StringList(values=value_list)

    return filters


class ImageParser(click.ParamType):
    name = "base_image"

    def convert(self, value, param, ctx):
        return Image(
            base_image=value,
        )


class ShlexParser(click.ParamType):
    name = "shlex"

    def convert(self, value, param, ctx):
        if not value:
            return []
        return shlex.split(value)


class CommaSeparatedList(click.ParamType):
    name = "comma_separated_list"

    def __init__(self, type: click.ParamType):
        self.type = type

    def convert(self, value, param, ctx):
        if not value:
            return []
        values = value.split(",")
        return [self.type.convert(v, param, ctx) for v in values]


def override_config_options(func: click.Command):
    f = click.option(
        "--cpu",
        type=click.FLOAT,
        help="The amount of CPU to allocate (in cores, e.g. --cpu 0.5).",
        required=False,
    )(func)
    f = click.option(
        "--memory",
        type=click.STRING,
        help="The amount of memory to allocate (in MB).",
        required=False,
    )(f)
    f = click.option(
        "--gpu", type=click.STRING, help="The type of GPU to allocate.", required=False
    )(f)
    f = click.option(
        "--gpu-count",
        type=click.INT,
        help="The number of GPUs to allocate to the container.",
        required=False,
    )(f)
    f = click.option(
        "--secrets",
        type=CommaSeparatedList(click.STRING),
        help="The secrets to inject into the container (e.g. --secrets SECRET1,SECRET2).",
    )(f)
    f = click.option(
        "--ports",
        type=CommaSeparatedList(click.INT),
        help="The ports to expose inside the container (e.g. --ports 8000,8001).",
    )(f)
    f = click.option(
        "--entrypoint",
        type=ShlexParser(),
        help="The entrypoint for the container - only used if a handler is not provided.",
    )(f)
    f = click.option(
        "--image",
        type=ImageParser(),
        help="The image to use for the container (e.g. --image python:3.10).",
        required=False,
    )(f)
    f = click.option(
        "--env",
        type=click.STRING,
        multiple=True,
        help="Environment variables to pass to the container (e.g. --env VAR1=value --env VAR2=value).",
    )(f)
    f = click.option(
        "--keep-warm-seconds",
        type=click.INT,
        help="The number of seconds to keep the container up after the last request (e.g. --keep-warm-seconds 600).",
        required=False,
    )(f)
    return f


PARSE_CONFIG_PREFIX = "parse_"


def handle_config_override(func, kwargs: Dict[str, str]) -> bool:
    current_key = None
    try:
        config_class_instance = None
        if hasattr(func, "parent"):
            config_class_instance = func.parent
        else:
            config_class_instance = func

        # We only want to override the config if the config class has an __init__ method
        # For example, ports is only available on a Pod
        init_kwargs = get_init_args_kwargs(config_class_instance)

        for key, value in kwargs.items():
            current_key = key
            if value is not None and key in init_kwargs:
                if isinstance(value, tuple):
                    value = list(value)

                    if len(value) == 0:
                        continue

                if hasattr(config_class_instance, f"{PARSE_CONFIG_PREFIX}{key}"):
                    value = config_class_instance.__getattribute__(f"{PARSE_CONFIG_PREFIX}{key}")(
                        value
                    )

                setattr(config_class_instance, key, value)

        return True
    except BaseException as e:
        terminal.error(f"Invalid CLI argument ==> {current_key}: {e}", exit=False)
        return False

================================================================================
# File: cli/machine.py
# Path: src/beta9/cli/machine.py
================================================================================

from datetime import datetime, timezone

import click
from betterproto import Casing
from rich.table import Column, Table, box

from .. import terminal
from ..channel import ServiceClient
from ..cli import extraclick
from ..clients.gateway import (
    CordonWorkerRequest,
    CreateMachineRequest,
    CreateMachineResponse,
    DeleteMachineRequest,
    DeleteMachineResponse,
    DrainWorkerRequest,
    ListMachinesRequest,
    ListMachinesResponse,
    ListWorkersRequest,
)
from .extraclick import ClickCommonGroup, ClickManagementGroup


@click.group(cls=ClickCommonGroup)
def common(**_):
    pass


@click.group(
    name="machine",
    help="Manage remote machines.",
    cls=ClickManagementGroup,
)
def management():
    pass


@management.command(
    name="list",
    help="List all external machines.",
    epilog="""
    Examples:

      # List the first 10 machines
      {cli_name} machine list --limit 10

      # List machines and output in JSON format
      {cli_name} machine list --format json
      \b
    """,
)
@click.option(
    "--limit",
    type=click.IntRange(1, 100),
    default=20,
    help="The number of machines to fetch.",
)
@click.option(
    "--format",
    type=click.Choice(("table", "json")),
    default="table",
    show_default=True,
    help="Change the format of the output.",
)
@click.option(
    "--pool",
    "-p",
    help="The pool to filter.",
    required=False,
)
@extraclick.pass_service_client
def list_machines(
    service: ServiceClient,
    limit: int,
    format: str,
    pool: str,
):
    res: ListMachinesResponse
    res = service.gateway.list_machines(ListMachinesRequest(pool_name=pool, limit=limit))

    if not res.ok:
        terminal.error(res.err_msg)

    res.gpus = {gpu: res.gpus[gpu] for gpu in sorted(res.gpus)}

    if format == "json":
        machines = [d.to_dict(casing=Casing.SNAKE) for d in res.machines]  # type:ignore
        terminal.print_json({"machines": machines, "gpus": res.gpus})
        return

    # Display GPU types available
    table = Table(
        Column("GPU Type"),
        Column("Available", justify="center"),
        box=box.SIMPLE,
    )
    for gpu_type, gpu_avail in res.gpus.items():
        table.add_row(gpu_type, "✅" if gpu_avail else "❌")
    if not res.gpus:
        table.add_row(*("-" * len(res.gpus)))
    table.add_section()
    table.add_row(f"[bold]{len(res.gpus)} items")
    terminal.print(table)

    # Display external provider machines connected to cluster
    if res.machines:
        table = Table(
            Column("ID"),
            Column("CPU"),
            Column("Memory"),
            Column("GPU"),
            Column("Status"),
            Column("Pool"),
            Column("Created"),
            Column("Last Keepalive"),
            Column("Agent Version"),
            Column("Free GPU Count"),
            box=box.SIMPLE,
        )

        for machine in res.machines:
            table.add_row(
                machine.id,
                f"{machine.cpu:,}m" if machine.cpu > 0 else "-",
                terminal.humanize_memory(machine.memory * 1024 * 1024)
                if machine.memory > 0
                else "-",
                machine.gpu,
                machine.status,
                machine.pool_name,
                terminal.humanize_date(
                    datetime.fromtimestamp(int(machine.created), tz=timezone.utc)
                ),
                terminal.humanize_date(
                    datetime.fromtimestamp(int(machine.last_keepalive), tz=timezone.utc)
                )
                if machine.last_keepalive != ""
                else "Never",
                f"v{machine.agent_version}" if machine.agent_version else "-",
                str(machine.machine_metrics.free_gpu_count),
            )

        table.add_section()
        table.add_row(f"[bold]{len(res.machines)} items")
        terminal.print(table)


@management.command(
    name="create",
    help="Create a new machine.",
    epilog="""
      Examples:

        {cli_name} machine create --pool ec2-t4
        \b
    """,
)
@click.option(
    "--pool",
    "-p",
    help="The pool to select for the machine.",
    required=True,
)
@extraclick.pass_service_client
def create_machine(service: ServiceClient, pool: str):
    res: CreateMachineResponse
    res = service.gateway.create_machine(CreateMachineRequest(pool_name=pool))
    if not res.ok:
        return terminal.error(f"Error: {res.err_msg}")

    terminal.header(
        f"Created machine with ID: '{res.machine.id}'. Use the following command to setup the node:"
    )

    cmd_args = [
        f'--token "{res.machine.registration_token}"',
        f'--machine-id "{res.machine.id}"',
        f'--tailscale-url "{res.machine.tailscale_url}"',
        f'--tailscale-auth "{res.machine.tailscale_auth}"',
        f'--pool-name "{res.machine.pool_name}"',
        f'--provider-name "{res.machine.provider_name}"',
    ]

    agent_url = "https://release.beam.cloud/agent/agent"
    if res.agent_upstream_url:
        cmd_args.append(f'--flux-upstream "{res.agent_upstream_url}"')

    if res.agent_upstream_branch:
        cmd_args.append(f'--flux-branch "{res.agent_upstream_branch}"')
        if res.agent_upstream_branch != "main":
            agent_url += f"-{res.agent_upstream_branch}"

    cmd_args_formatted = " \\\n\t  ".join(cmd_args)
    text = f"""# -- Agent setup
    sudo curl -L -o agent {agent_url} &&
    sudo chmod +x agent &&
    sudo ./agent {cmd_args_formatted}
    """

    if res.machine.user_data:
        text = f"""# -- User data\n{res.machine.user_data}\n{text}"""

    terminal.detail(text, crop=False, overflow="ignore")


@management.command(
    name="delete",
    help="Delete a machine.",
    epilog="""
      Examples:

        {cli_name} machine delete my-machine-id --pool ec2-t4
        \b
    """,
)
@click.argument(
    "machine_id",
    nargs=1,
    required=True,
)
@click.option(
    "--pool",
    "-p",
    help="The pool to select for the machine.",
    required=True,
)
@extraclick.pass_service_client
def delete_machine(service: ServiceClient, machine_id: str, pool: str):
    res: DeleteMachineResponse
    res = service.gateway.delete_machine(
        DeleteMachineRequest(machine_id=machine_id, pool_name=pool)
    )
    if res.ok:
        terminal.success(f"Deleted machine '{machine_id}' from pool '{pool}'")
    else:
        terminal.error(f"Error: {res.err_msg}")


@management.command(
    name="drain",
    help="""
    Drain all workers on a specific machine.

    This command will find all workers running on the specified machine ID and drain them.
    When a worker is drained, all running containers on it will be stopped. The worker will 
    continue until its idle timeout if was cordoned before being drained. 
    """,
    epilog="""
      Examples:

        # Drain all workers on a specific machine
        {cli_name} machine drain 0d123123
    """,
)
@click.argument(
    "machine_id",
    nargs=1,
    required=True,
)
@extraclick.pass_service_client
def drain_machine(
    service: ServiceClient,
    machine_id: str,
):
    res = service.gateway.list_workers(ListWorkersRequest())
    if not res.ok:
        return terminal.error(f"Failed to list workers: {res.err_msg}")

    matching_workers = [w.id for w in res.workers if w.machine_id == machine_id]

    if not matching_workers:
        return terminal.error(f"No workers found for machine ID: {machine_id}")

    terminal.detail(f"Found {len(matching_workers)} workers on machine {machine_id}")

    for worker_id in matching_workers:
        res = service.gateway.drain_worker(DrainWorkerRequest(worker_id=worker_id))
        if not res.ok:
            text = res.err_msg.capitalize()
            terminal.warn(text)
        else:
            terminal.success(f"Worker {worker_id} has been drained.")


@management.command(
    name="cordon",
    help="""
    Cordon all workers on a specific machine.

    This command will find all workers running on the specified machine ID and cordon them.
    When workers are cordoned, they will not accept new container requests but will
    continue running existing containers. This is useful when you want to gracefully
    remove workers from a machine.
    """,
    epilog="""
      Examples:

        # Cordon all workers on a specific machine
        {cli_name} machine cordon 0d123123
    """,
)
@click.argument(
    "machine_id",
    nargs=1,
    required=True,
)
@extraclick.pass_service_client
def cordon_machine(
    service: ServiceClient,
    machine_id: str,
):
    res = service.gateway.list_workers(ListWorkersRequest())
    if not res.ok:
        return terminal.error(f"Failed to list workers: {res.err_msg}")

    matching_workers = [w.id for w in res.workers if w.machine_id == machine_id]

    if not matching_workers:
        return terminal.error(f"No workers found for machine ID: {machine_id}")

    terminal.detail(f"Found {len(matching_workers)} workers on machine {machine_id}")

    for worker_id in matching_workers:
        res = service.gateway.cordon_worker(CordonWorkerRequest(worker_id=worker_id))
        if not res.ok:
            text = res.err_msg.capitalize()
            terminal.warn(text)
        else:
            terminal.success(f"Worker {worker_id} has been cordoned.")

================================================================================
# File: cli/main.py
# Path: src/beta9/cli/main.py
================================================================================

import os

os.environ["GRPC_VERBOSITY"] = os.getenv("GRPC_VERBOSITY") or "NONE"

import shutil
from types import ModuleType
from typing import Any, Optional

import click
import grpc

from ..channel import handle_grpc_error, prompt_first_auth
from ..config import SDKSettings, is_config_empty, set_settings
from . import (
    config,
    container,
    deployment,
    dev,
    machine,
    pool,
    run,
    secret,
    serve,
    shell,
    task,
    token,
    volume,
    worker,
)
from .extraclick import CLICK_CONTEXT_SETTINGS, ClickCommonGroup, CommandGroupCollection

click.formatting.FORCED_WIDTH = shutil.get_terminal_size().columns


class CLI:
    """
    The CLI application.

    This is used to dynamically register commands. Commands are of type
    click.Group and are named either "common" or "management".
    """

    def __init__(
        self,
        settings: Optional[SDKSettings] = None,
        context_settings: Optional[dict] = None,
    ) -> None:
        self.settings = SDKSettings() if settings is None else settings
        set_settings(self.settings)

        if context_settings is None:
            context_settings = CLICK_CONTEXT_SETTINGS

        self.management_group = ClickCommonGroup()
        self.common_group = CommandGroupCollection(
            sources=[self.management_group],
            context_settings=context_settings,
        )

    def __call__(self, **kwargs) -> None:
        self.common_group.main(prog_name=self.settings.name.lower(), **kwargs)

    def register(self, module: ModuleType) -> None:
        if hasattr(module, "common"):
            self.common_group.add_command(module.common)
        if hasattr(module, "management"):
            self.management_group.add_command(module.management)

    def check_config(self) -> None:
        if os.getenv("CI"):
            return
        if is_config_empty(self.settings.config_path):
            prompt_first_auth(self.settings)

    def load_version(self, package_name: Optional[str] = None):
        """
        Adds a version parameter to the top-level command.

        If an version parameter already exists, it'll be replaced
        with a new one. Setting package_name tells thie CLI
        to use your package's version instead of this one.

        Args:
            package_name: Name of Python package. Defaults to None.
        """
        option = click.version_option(package_name=package_name)

        for i, param in enumerate(self.common_group.params):
            if param.name == "version":
                self.common_group.params.pop(i)
                break

        self.common_group = option(self.common_group)


def load_cli(check_config=True, **kwargs: Any) -> CLI:
    cli = CLI(**kwargs)
    cli.register(task)
    cli.register(deployment)
    cli.register(serve)
    cli.register(volume)
    cli.register(config)
    cli.register(pool)
    cli.register(container)
    cli.register(machine)
    cli.register(secret)
    cli.register(token)
    cli.register(worker)
    cli.register(shell)
    cli.register(run)
    cli.register(dev)

    if check_config:
        cli.check_config()

    cli.load_version()

    return cli


def start():
    """Used as entrypoint in Poetry"""
    cli = load_cli()

    try:
        cli()
    except grpc.RpcError as error:
        handle_grpc_error(error=error)

================================================================================
# File: cli/pool.py
# Path: src/beta9/cli/pool.py
================================================================================

import json
import time
from typing import Any, Dict, List, Tuple

import click
from betterproto import Casing
from rich.columns import Columns
from rich.live import Live
from rich.panel import Panel
from rich.table import Table
from rich.text import Text

from .. import terminal
from ..channel import ServiceClient
from ..cli import extraclick
from ..clients.gateway import ListPoolsRequest, ListPoolsResponse, StringList
from .extraclick import ClickCommonGroup, ClickManagementGroup


@click.group(cls=ClickCommonGroup)
def common(**_):
    pass


@click.group(
    name="pool",
    help="Manage worker pools.",
    cls=ClickManagementGroup,
)
def management():
    pass


def _create_table(rows: List[Tuple[str, str]]) -> Table:
    table = Table(show_header=False, box=None, expand=True)
    for label, value in rows:
        table.add_row(label, value)
    return table


def _get_pool_renderable(
    service: ServiceClient, limit: int, output_format: str, filters: Dict[str, StringList]
) -> Any:
    """
    Returns a Rich renderable for the pool list
    """
    res: ListPoolsResponse = service.gateway.list_pools(ListPoolsRequest(filters, limit))
    if not res.ok:
        return Text(f"[red]{res.err_msg}")

    if output_format == "json":
        pools = [d.to_dict(casing=Casing.SNAKE) for d in res.pools]  # type: ignore
        return Text(json.dumps(pools, indent=2))
    else:
        name_filters = filters.get("name", StringList())
        pool_cards = []
        for pool in res.pools:
            if name_filters.values and pool.name not in name_filters.values:
                continue

            # Create pool config table
            config_rows = [
                ("GPU:", pool.gpu),
                ("Minimum Free GPU:", pool.min_free_gpu or "0"),
                ("Minimum Free CPU:", pool.min_free_cpu or "0"),
                ("Minimum Free Memory:", pool.min_free_memory or "0"),
                ("Default GPU Count (per worker):", pool.default_worker_gpu_count or "0"),
            ]
            config_table = _create_table(config_rows)

            # Create pool state table
            state_rows = [
                ("Status:", pool.state.status),
                ("Scheduling Latency (ms):", str(pool.state.scheduling_latency)),
                ("Free GPU:", str(pool.state.free_gpu)),
                ("Free CPU (millicores):", str(pool.state.free_cpu)),
                ("Free Memory (MB):", str(pool.state.free_memory)),
                ("Pending Workers:", str(pool.state.pending_workers)),
                ("Available Workers:", str(pool.state.available_workers)),
                ("Pending Containers:", str(pool.state.pending_containers)),
                ("Running Containers:", str(pool.state.running_containers)),
                ("Registered Machines:", str(pool.state.registered_machines)),
                ("Pending Machines:", str(pool.state.pending_machines)),
            ]
            state_table = _create_table(state_rows)

            content = Columns([config_table, state_table], equal=True, expand=True)
            card = Panel(content, title=pool.name, border_style="blue")
            pool_cards.append(card)

        return Columns(pool_cards, expand=True)


@management.command(
    name="list",
    help="List all worker pools.",
    epilog="""
    Examples:

      # List the first 10 pools
      {cli_name} pool list --limit 10

      # List pools and output in JSON format
      {cli_name} pool list --format json
      
      # Continuously refresh and show pool information every 1 second (default)
      {cli_name} pool list --watch

      # Continuously refresh every 2 seconds
      {cli_name} pool list --watch --period 2
      \b
    """,
)
@click.option(
    "--limit",
    type=click.IntRange(1, 100),
    default=20,
    help="The number of pools to fetch.",
)
@click.option(
    "--format",
    type=click.Choice(("table", "json")),
    default="table",
    show_default=True,
    help="Change the format of the output.",
)
@click.option(
    "--filter",
    multiple=True,
    callback=extraclick.filter_values_callback,
    help="Filters pools. Add this option for each field you want to filter on.",
)
@click.option(
    "-w",
    "--watch",
    is_flag=True,
    default=False,
    help="Periodically refreshes the output automatically and re-renders the data.",
)
@click.option(
    "--period",
    "-p",
    type=click.FloatRange(min=0.1),
    default=1.0,
    show_default=True,
    help="Refresh interval (in seconds) when in watch mode.",
)
@extraclick.pass_service_client
def list_pools(
    service: ServiceClient,
    limit: int,
    format: str,
    filter: Dict[str, StringList],
    watch: bool,
    period: float,
):
    """
    List all worker pools
    """
    if not watch:
        terminal.print(_get_pool_renderable(service, limit, format, filter))
        return

    with Live(
        _get_pool_renderable(service, limit, format, filter),
        console=terminal._console,
        screen=True,
        refresh_per_second=1 / period,
    ) as live:
        try:
            while True:
                live.update(_get_pool_renderable(service, limit, format, filter))
                time.sleep(period)
        except KeyboardInterrupt:
            pass

================================================================================
# File: cli/run.py
# Path: src/beta9/cli/run.py
================================================================================

import inspect

import click

from .. import terminal
from ..abstractions.base.container import Container
from ..abstractions.pod import Pod, PodInstance
from ..channel import ServiceClient
from ..utils import load_module_spec
from .extraclick import (
    ClickCommonGroup,
    handle_config_override,
    override_config_options,
    pass_service_client,
)


@click.group(cls=ClickCommonGroup)
def common(**_):
    pass


@common.command(
    name="run",
    help="""
    Run a container.

    """,
    epilog="""
      Examples:

        {cli_name} run app.py:handler

        {cli_name} run app.py:my_func

        {cli_name} run --image python:3.10 --gpu T4
        \b
    """,
)
@click.argument(
    "handler",
    nargs=1,
    required=False,
)
@click.option(
    "--sync",
    is_flag=True,
    default=False,
    help="Recursively sync the current directory to the container and watch for changes",
)
@override_config_options
@pass_service_client
def run(
    _: ServiceClient,
    handler: str,
    sync: bool,
    **kwargs,
):
    entrypoint = kwargs["entrypoint"]
    if handler:
        pod_spec, _, _ = load_module_spec(handler, "run")

        if not inspect.isclass(type(pod_spec)) or pod_spec.__class__.__name__ != "Pod":
            terminal.error("Invalid handler function specified. Expected a Pod abstraction.")

    elif entrypoint:
        pod_spec = Pod(entrypoint=entrypoint)

    else:
        terminal.error("No handler or entrypoint specified.")
        return

    if not handle_config_override(pod_spec, kwargs):
        return

    result: PodInstance = pod_spec.create()
    if not result.ok:
        terminal.error("Failed to create container.")
        return

    container = Container(container_id=result.container_id)

    sync_dir = None
    if sync:
        sync_dir = "./"
    else:
        sync_dir = None

    container.attach(container_id=result.container_id, sync_dir=sync_dir)

================================================================================
# File: cli/secret.py
# Path: src/beta9/cli/secret.py
================================================================================

import click
from betterproto import Casing
from rich.table import Column, Table, box

from .. import terminal
from ..channel import ServiceClient
from ..cli import extraclick
from ..clients.secret import (
    CreateSecretRequest,
    CreateSecretResponse,
    DeleteSecretRequest,
    DeleteSecretResponse,
    GetSecretRequest,
    GetSecretResponse,
    ListSecretsRequest,
    ListSecretsResponse,
    UpdateSecretRequest,
    UpdateSecretResponse,
)
from .extraclick import ClickCommonGroup, ClickManagementGroup


@click.group(cls=ClickCommonGroup)
def common(**_):
    pass


@click.group(
    name="secret",
    help="Manage secrets",
    cls=ClickManagementGroup,
)
def management():
    pass


@management.command(
    name="list",
    help="List all secrets.",
)
@click.option(
    "--format",
    type=click.Choice(("table", "json")),
    default="table",
    show_default=True,
    help="Change the format of the output.",
)
@extraclick.pass_service_client
def list_secrets(
    service: ServiceClient,
    format: str,
):
    res: ListSecretsResponse
    res = service.secret.list_secrets(ListSecretsRequest())

    if not res.ok:
        terminal.error(res.err_msg)

    if format == "json":
        secrets = [d.to_dict(casing=Casing.SNAKE) for d in res.secrets]  # type:ignore
        terminal.print_json(secrets)
        return

    table = Table(
        Column("Name"),
        Column("Last Updated"),
        Column("Created"),
        box=box.SIMPLE,
    )

    for secret in res.secrets:
        table.add_row(
            secret.name,
            terminal.humanize_date(secret.created_at),
            terminal.humanize_date(secret.updated_at),
        )

    table.add_section()
    table.add_row(f"[bold]{len(res.secrets)} items")
    terminal.print(table)


@management.command(
    name="create",
    help="Create new secret.",
)
@click.argument("name")
@click.argument("value")
@extraclick.pass_service_client
def create_secret(service: ServiceClient, name: str, value: str):
    res: CreateSecretResponse
    res = service.secret.create_secret(CreateSecretRequest(name=name, value=value))
    if res.ok:
        terminal.header(f"Created secret with name: '{res.name}'")
    else:
        terminal.error(f"Error: {res.err_msg}")


@management.command(
    name="modify",
    help="Modify existing secret.",
)
@click.argument("name")
@click.argument("value")
@extraclick.pass_service_client
def modify_secret(service: ServiceClient, name: str, value: str):
    res: UpdateSecretResponse
    res = service.secret.update_secret(UpdateSecretRequest(name=name, value=value))
    if res.ok:
        terminal.header(f"Modified secret '{name}'")
    else:
        terminal.error(f"Error: {res.err_msg}")


@management.command(
    name="delete",
    help="Delete secret.",
)
@click.argument("name")
@extraclick.pass_service_client
def delete_secret(service: ServiceClient, name: str):
    res: DeleteSecretResponse
    res = service.secret.delete_secret(DeleteSecretRequest(name=name))
    if res.ok:
        terminal.header(f"Deleted secret '{name}'")
    else:
        terminal.error(f"Error: {res.err_msg}")


@management.command(
    name="show",
    help="Show secret.",
)
@click.argument("name")
@extraclick.pass_service_client
def show_secret(service: ServiceClient, name: str):
    res: GetSecretResponse

    res = service.secret.get_secret(GetSecretRequest(name=name))

    if res.ok:
        terminal.header(f"Secret '{name}': {res.secret.value}")
    else:
        terminal.error(f"Error: {res.err_msg}")

================================================================================
# File: cli/serve.py
# Path: src/beta9/cli/serve.py
================================================================================

from typing import Optional

import click

from ..channel import ServiceClient
from ..cli import extraclick
from ..utils import load_module_spec
from .extraclick import ClickCommonGroup, handle_config_override, override_config_options


@click.group(cls=ClickCommonGroup)
def common(**_):
    pass


@common.command(
    name="serve",
    help="""
    Serve a function.

    HANDLER is in the format of "file:function".
    """,
    epilog="""
      Examples:

        {cli_name} serve app.py:my_func

        {cli_name} serve app.py:my_func
        \b
    """,
)
@click.argument(
    "handler",
    nargs=1,
    required=True,
)
@click.option(
    "--timeout",
    "-t",
    default=0,
    help="The inactivity timeout for the serve instance in seconds. Set to -1 for no timeout. Set to 0 to use default timeout (10 minutes)",
)
@click.option(
    "--url-type",
    help="The type of URL to get back. [default is determined by the server] ",
    type=click.Choice(["host", "path"]),
)
@override_config_options
@extraclick.pass_service_client
@click.pass_context
def serve(
    ctx: click.Context,
    service: ServiceClient,
    handler: str,
    timeout: Optional[int] = None,
    url_type: str = "path",
    **kwargs,
):
    user_obj, module_name, obj_name = load_module_spec(handler, "serve")

    if hasattr(user_obj, "set_handler"):
        user_obj.set_handler(f"{module_name}:{obj_name}")

    if not handle_config_override(user_obj, kwargs):
        return

    user_obj.serve(timeout=int(timeout), url_type=url_type)  # type:ignore

================================================================================
# File: cli/shell.py
# Path: src/beta9/cli/shell.py
================================================================================

import click

from .. import terminal
from ..abstractions.pod import Pod
from ..channel import ServiceClient
from ..cli import extraclick
from ..utils import load_module_spec
from .extraclick import ClickCommonGroup, handle_config_override, override_config_options


@click.group(cls=ClickCommonGroup)
def common(**_):
    pass


@common.command(
    name="shell",
    help="""
    Connect to a container with the same config as your handler.

    HANDLER is in the format of "file:function".
    """,
    epilog="""
      Examples:

        {cli_name} shell app.py:handler

        {cli_name} shell app.py:my_func
        \b
    """,
)
@click.argument(
    "handler",
    nargs=1,
    required=False,
)
@click.option(
    "--url-type",
    help="The type of URL to get back. [default is determined by the server] ",
    type=click.Choice(["host", "path"]),
)
@click.option(
    "--container-id",
    help="The ID of the container to connect to.",
    type=str,
)
@override_config_options
@extraclick.pass_service_client
@click.pass_context
def shell(
    ctx: click.Context,
    service: ServiceClient,
    handler: str,
    url_type: str = "path",
    container_id: str = None,
    **kwargs,
):
    entrypoint = kwargs["entrypoint"]
    if handler:
        user_obj, module_name, obj_name = load_module_spec(handler, "shell")

        if hasattr(user_obj, "set_handler"):
            user_obj.set_handler(f"{module_name}:{obj_name}")

    elif entrypoint or container_id:
        user_obj = Pod(entrypoint=entrypoint)

    else:
        terminal.error("No handler or entrypoint specified.")

    if not handle_config_override(user_obj, kwargs):
        return

    user_obj.shell(url_type=url_type, container_id=container_id)  # type:ignore

================================================================================
# File: cli/task.py
# Path: src/beta9/cli/task.py
================================================================================

from typing import Dict, List

import click
from betterproto import Casing
from rich.table import Column, Table, box

from .. import terminal
from ..channel import ServiceClient
from ..clients.gateway import (
    ListTasksRequest,
    ListTasksResponse,
    StopTasksRequest,
    StringList,
)
from . import extraclick
from .extraclick import ClickManagementGroup


@click.group(
    name="task",
    help="Manage tasks.",
    cls=ClickManagementGroup,
)
def management():
    pass


@management.command(
    name="list",
    help="List all tasks.",
    epilog="""
    Examples:

      # List the first 10 tasks
      {cli_name} task list --limit 10

      # List tasks with two different statuses and a specific stub name
      {cli_name} task list --filter status=running,pending --filter stub-name=function/test:handler

      # List tasks that belong to two different stubs
      {cli_name} task list --filter stub-id=function/test:handler1,endpoint/test:handler2

      # List tasks that are on a specific container
      {cli_name} task list --filter container-id=endpoint-05cc6c0d-fef2-491c-b9b7-313cc21c3496-3cef29e8

      # List tasks and output in JSON format
      {cli_name} task list --format json
      \b
    """,
)
@click.option(
    "--limit",
    type=click.IntRange(1, 1000),
    default=100,
    help="The number of tasks to fetch.",
)
@click.option(
    "--format",
    type=click.Choice(("table", "json")),
    default="table",
    show_default=True,
    help="Change the format of the output.",
)
@click.option(
    "--filter",
    multiple=True,
    callback=extraclick.filter_values_callback,
    help="Filters tasks. Add this option for each field you want to filter on.",
)
@extraclick.pass_service_client
def list_tasks(service: ServiceClient, limit: int, format: str, filter: Dict[str, StringList]):
    res: ListTasksResponse
    res = service.gateway.list_tasks(ListTasksRequest(filters=filter, limit=limit))

    if not res.ok:
        terminal.error(res.err_msg)

    if format == "json":
        tasks = [task.to_dict(casing=Casing.SNAKE) for task in res.tasks]  # type:ignore
        terminal.print_json(tasks)
        return

    table = Table(
        Column("Task ID"),
        Column("Status"),
        Column("Created At"),
        Column("Started At"),
        Column("Ended At"),
        Column("Container ID"),
        Column("Stub Name"),
        Column("Workspace Name"),
        box=box.SIMPLE,
    )

    for task in res.tasks:
        table.add_row(
            task.id,
            (
                f"[bold green]{task.status}"
                if task.status.lower() == "complete"
                else f"[bold yellow]{task.status}"
            ),
            terminal.humanize_date(task.created_at),
            terminal.humanize_date(task.started_at),
            terminal.humanize_date(task.ended_at),
            task.container_id,
            task.stub_name,
            task.workspace_name,
        )

    table.add_section()
    table.add_row(f"[bold]{res.total} items")
    terminal.print(table)


@management.command(
    name="stop",
    help="Stop a task.",
    epilog="""
    Examples:

      # Stop a task
      {cli_name} task stop 05cc6c0d-fef2-491c-b9b7-313cc21c3496

      # Stop multiple tasks
      {cli_name} task stop 05cc6c0d-fef2-491c-b9b7-313cc21c3496 22dee81b-eeab-4c0a-b28a-b81b159358f9

      # Stop multiple tasks from stdin
      {cli_name} task list --filter status=running --format=json | jq -r '.[] | .id' | {cli_name} task stop -
      \b
    """,
)
@click.argument(
    "task_ids",
    nargs=-1,
    required=True,
)
@extraclick.pass_service_client
def stop_task(service: ServiceClient, task_ids: List[str]):
    if task_ids and task_ids[0] == "-":
        task_ids = click.get_text_stream("stdin").read().strip().split()

    if not task_ids:
        return terminal.error("Must provide at least one task ID.")

    for task_id in task_ids:
        res = service.gateway.stop_tasks(StopTasksRequest(task_ids=[task_id]))
        if not res.ok:
            terminal.warn(str(res.err_msg).capitalize())
        else:
            terminal.success(f"Task {task_id} stopped.")

================================================================================
# File: cli/token.py
# Path: src/beta9/cli/token.py
================================================================================

import click
from betterproto import Casing
from rich.table import Column, Table, box

from .. import terminal
from ..channel import ServiceClient
from ..cli import extraclick
from ..clients.gateway import (
    CreateTokenRequest,
    CreateTokenResponse,
    DeleteTokenRequest,
    DeleteTokenResponse,
    ListTokensRequest,
    ListTokensResponse,
    ToggleTokenRequest,
    ToggleTokenResponse,
)
from .extraclick import ClickCommonGroup, ClickManagementGroup


@click.group(cls=ClickCommonGroup)
def common(**_):
    pass


@click.group(
    name="token",
    help="Manage tokens.",
    cls=ClickManagementGroup,
)
def management():
    pass


@management.command(
    name="list",
    help="List all tokens.",
    epilog="""
    Examples:

      # List tokens
      {cli_name} token list

      # List tokens and output in JSON format
      {cli_name} token list --format json
      \b
    """,
)
@click.option(
    "--format",
    type=click.Choice(("table", "json")),
    default="table",
    show_default=True,
    help="Change the format of the output.",
)
@extraclick.pass_service_client
def list_tokens(
    service: ServiceClient,
    format: str,
):
    res: ListTokensResponse
    res = service.gateway.list_tokens(ListTokensRequest())

    if not res.ok:
        terminal.error(res.err_msg)
        return

    if format == "json":
        tokens = [t.to_dict(casing=Casing.SNAKE) for t in res.tokens]  # type:ignore
        terminal.print_json(tokens)
        return

    table = Table(
        Column("ID"),
        Column("Active"),
        Column("Reusable"),
        Column("Token Type"),
        Column("Created At"),
        Column("Updated At"),
        Column("Workspace ID"),
        box=box.SIMPLE,
    )

    for token in res.tokens:
        table.add_row(
            token.token_id,
            "Yes" if token.active else "No",
            "Yes" if token.reusable else "No",
            token.token_type,
            terminal.humanize_date(token.created_at),
            terminal.humanize_date(token.updated_at),
            str(token.workspace_id) if token.workspace_id else "N/A",
        )

    table.add_section()
    table.add_row(f"[bold]{len(res.tokens)} items")
    terminal.print(table)


@management.command(
    name="create",
    help="Create a new token.",
    epilog="""
    Examples:

      # Create a new token. 
      {cli_name} token create

      \b
    """,
)
@extraclick.pass_service_client
def create_token(
    service: ServiceClient,
):
    res: CreateTokenResponse
    res = service.gateway.create_token(CreateTokenRequest())

    if not res.ok:
        terminal.error(res.err_msg)
    else:
        terminal.success("Token created successfully.")
        terminal.print(res.token.to_dict(casing=Casing.SNAKE))


@management.command(
    name="delete",
    help="Delete a token.",
    epilog="""
    Examples:

      # Delete a token. 
      {cli_name} token delete 49554ce9-5861-4834-899b-ec301e5d8534

      \b
    """,
)
@extraclick.pass_service_client
@click.argument("token_id", type=click.STRING)
def delete_token(
    service: ServiceClient,
    token_id: str,
):
    res: DeleteTokenResponse
    res = service.gateway.delete_token(DeleteTokenRequest(token_id=token_id))

    if not res.ok:
        terminal.error(res.err_msg)
    else:
        terminal.success("Deleted token.")


@management.command(
    name="toggle",
    help="Toggle a token's active status.",
    epilog="""
    Examples:

      # Toggle a token
      {cli_name} token toggle 49554ce9-5861-4834-899b-ec301e5d8534

      \b
    """,
)
@click.argument("token_id", type=click.STRING)
@extraclick.pass_service_client
def toggle_token(
    service: ServiceClient,
    token_id: str,
):
    res: ToggleTokenResponse
    res = service.gateway.toggle_token(ToggleTokenRequest(token_id=token_id))

    if not res.ok:
        terminal.error(res.err_msg)
    else:
        token = res.token.to_dict(casing=Casing.SNAKE)
        if "active" not in token:
            token["active"] = False
        terminal.success(
            f"Token {res.token.token_id} {'enabled' if  token['active'] else 'disabled'}."
        )
        terminal.print(token)

================================================================================
# File: cli/volume.py
# Path: src/beta9/cli/volume.py
================================================================================

import glob
from pathlib import Path
from typing import Iterable, List, Optional, Union

import click
from rich.table import Column, Table, box

from beta9 import multipart

from .. import terminal
from ..channel import ServiceClient
from ..clients.volume import (
    CopyPathRequest,
    CopyPathResponse,
    DeletePathRequest,
    DeleteVolumeRequest,
    DeleteVolumeResponse,
    GetFileServiceInfoRequest,
    GetOrCreateVolumeRequest,
    GetOrCreateVolumeResponse,
    ListPathRequest,
    ListPathResponse,
    ListVolumesRequest,
    ListVolumesResponse,
    MovePathRequest,
)
from ..multipart import PathTypeConverter, RemotePath
from ..terminal import StyledProgress, pluralize
from . import extraclick
from .extraclick import ClickCommonGroup, ClickManagementGroup


@click.group(cls=ClickCommonGroup)
def common(**_):
    pass


@common.command(
    help="List contents in a volume.",
    epilog="""
    Match Syntax:
      This command provides support for Unix shell-style wildcards, which are not the same as regular expressions.

      *       matches everything
      ?       matches any single character
      [seq]   matches any character in `seq`
      [!seq]  matches any character not in `seq`

      For a literal match, wrap the meta-characters in brackets. For example, '[?]' matches the character '?'.

    Examples:

      # List files in the volume named myvol, in the directory named subdir/
      {cli_name} ls myvol/subdir

      # List files ending in .txt in the directory named subdir
      {cli_name} ls myvol/subdir/\\*.txt

      # Same as above, but with single quotes to avoid escaping
      {cli_name} ls 'myvol/subdir/*.txt'
      \b
    """,
)
@click.argument(
    "remote_path",
    required=True,
)
@extraclick.pass_service_client
def ls(service: ServiceClient, remote_path: str):
    res: ListPathResponse
    res = service.volume.list_path(ListPathRequest(path=remote_path))

    if not res.ok:
        terminal.error(f"{remote_path} ({res.err_msg})")

    table = Table(
        Column("Name"),
        Column("Size", justify="right"),
        Column("Modified Time"),
        Column("IsDir"),
        box=box.SIMPLE,
    )

    total_size = 0
    for p in res.path_infos:
        total_size += p.size
        table.add_row(
            Path(p.path).name + ("/" if p.is_dir else ""),
            "" if p.is_dir else terminal.humanize_memory(p.size),
            terminal.humanize_date(p.mod_time),
            "Yes" if p.is_dir else "No",
        )

    table.add_section()
    table.add_row(
        f"[bold]{len(res.path_infos)} items | {terminal.humanize_memory(total_size)} used"
    )

    terminal.print(table)


def cp_v1(service: ServiceClient, local_path: str, remote_path: str):
    local_path = str(Path(local_path).resolve())
    files_to_upload: List[Path] = []

    for match in glob.glob(local_path, recursive=True):
        mpath = Path(match)
        if mpath.is_dir():
            files_to_upload.extend([p for p in mpath.rglob("*") if p.is_file()])
        else:
            files_to_upload.append(mpath)

    if not files_to_upload:
        terminal.error("Could not find files to upload.")

    num_upload, suffix = pluralize(files_to_upload)
    terminal.header(f"{remote_path} (copying {num_upload} object{suffix})")

    desc_width = max((len(f.name) for f in files_to_upload))
    for file in files_to_upload:
        dst = (remote_path / file.relative_to(Path.cwd())).as_posix()
        req = (
            CopyPathRequest(path=dst, content=chunk)
            for chunk in read_with_progress(file, max_desc_width=desc_width)
        )
        res: CopyPathResponse = service.volume.copy_path_stream(req)

        if not res.ok:
            terminal.error(f"{dst} ({res.err_msg})")


def read_with_progress(
    path: Union[Path, str],
    chunk_size: int = 1024 * 256,
    max_desc_width: int = 30,
) -> Iterable[bytes]:
    path = Path(path)
    name = "/".join(path.relative_to(Path.cwd()).parts[-(len(path.parts)) :])

    if len(name) > max_desc_width:
        desc = f"...{name[-(max_desc_width - 3):]}"
    else:
        desc = name.ljust(max_desc_width)

    with terminal.progress_open(path, "rb", description=desc) as file:
        while chunk := file.read(chunk_size):
            yield chunk
    yield b""


@common.command(
    help="Upload or download contents to or from a volume.",
    epilog="""
    Version 1 (Streaming uploads to the Gateway):

      Uploads files or directories to a volume. Downloads are not supported.

      SOURCE and DESTINATION syntax:

        SOURCE must always be a local path.
        DESTINATION must always be a remote path. The format is volume-name/path/to/file.txt.

      Match Syntax:
        This command provides support for Unix shell-style wildcards, which are
        not the same as regular expressions.

        *       matches everything
        ?       matches any single character
        [seq]   matches any character in `seq`
        [!seq]  matches any character not in `seq`

        For a literal match, wrap the meta-characters in brackets. For example,
        '[?]' matches the character '?'.

      Examples:

        # Copy contents to a remote volume
        {cli_name} cp mydir myvol/subdir
        {cli_name} cp myfile.txt myvol/subdir

        # Use a question mark to match a single character in a path
        {cli_name} cp 'mydir/?/data?.json' myvol/sub/path

        # Use an asterisk to match all characters in a path
        {cli_name} cp 'mydir/*/*.json' myvol/data

        # Use a sequence to match a specific set of characters in a path
        {cli_name} cp 'mydir/[a-c]/data[0-1].json' myvol/data


        # Escape special characters if you don't want to single quote your local path
        {cli_name} cp mydir/\\[a-c\\]/data[0-1].json' myvol/data
        {cli_name} cp mydir/\\?/data\\?.json myvol/sub/path

    Version 2 (Multipart uploads/downloads via an external file service):

      Upload and download files or directories between a volume and your system.

      This is available if the Gateway has a file service configured and enabled.
      When enabled, this command will use the external file service to upload and
      download files directly only using the Gateway for initiating, completing, or
      cancelling the transfer.

      SOURCE and DESTINATION syntax:

        SOURCE can be local or remote.
        DESTINATION can be local or remote.

        Local paths are files or directories on your system. For example, file.txt,
        ./mydir, or path/to/file.txt.

        Remote paths are files or directories in a volume. The format is
        $scheme://$volume_name/path/to/file.txt.

      Examples:

        # Upload a file
        {cli_name} cp file.txt {cli_name}://myvol/              # ./file.txt => {cli_name}://myvol/file.txt
        {cli_name} cp file.txt {cli_name}://myvol/file.txt      # ./file.txt => {cli_name}://myvol/file.txt
        {cli_name} cp file.txt {cli_name}://myvol/file.new      # ./file.txt => {cli_name}://myvol/file.new
        {cli_name} cp file.txt {cli_name}://myvol/hello         # ./file.txt => {cli_name}://myvol/hello.txt (keeps the extension)

        # Upload a directory
        {cli_name} cp mydir {cli_name}://myvol                  # ./mydir/file.txt => {cli_name}://myvol/file.txt
        {cli_name} cp mydir {cli_name}://myvol/mydir            # ./mydir/file.txt => {cli_name}://myvol/mydir/file.txt
        {cli_name} cp mydir {cli_name}://myvol/newdir           # ./mydir/file.txt => {cli_name}://myvol/newdir/file.txt

        # Upload a directory (with trailing slash)
        {cli_name} cp mydir {cli_name}://myvol/                 # ./mydir/file.txt => {cli_name}://myvol/mydir/file.txt
        {cli_name} cp mydir {cli_name}://myvol/newdir/          # ./mydir/file.txt => {cli_name}://myvol/newdir/mydir/file.txt
        {cli_name} cp . {cli_name}://myvol/                     # ./file.txt => {cli_name}://myvol/file.txt

        # Download a file
        {cli_name} cp {cli_name}://myvol/file.txt .             # {cli_name}://myvol/file.txt => ./file.txt
        {cli_name} cp {cli_name}://myvol/file.txt file.new      # {cli_name}://myvol/file.txt => ./file.new

        # Download a directory
        {cli_name} cp {cli_name}://myvol/mydir .                # {cli_name}://myvol/mydir/file.txt => ./file.txt

        # Download a directory (with trailing slash)
        {cli_name} cp {cli_name}://myvol/mydir/ .               # {cli_name}://myvol/mydir/file.txt => ./mydir/file.txt
        \b
    """,
)
@click.argument(
    "source",
    type=PathTypeConverter(),
    required=True,
)
@click.argument(
    "destination",
    type=PathTypeConverter(),
    required=True,
)
@click.option(
    "--multipart/--no-multipart",
    "use_multipart",
    help="Use multipart upload/download. [default: True if the server supports it, else False]",
    default=None,
    required=False,
)
@extraclick.pass_service_client
def cp(
    service: ServiceClient,
    source: Union[Path, RemotePath],
    destination: Union[Path, RemotePath],
    use_multipart: Optional[bool],
):
    res = service.volume.get_file_service_info(GetFileServiceInfoRequest())
    if res.enabled and (use_multipart is None or use_multipart):
        use_multipart = True
    elif not res.enabled and use_multipart:
        terminal.warn("Multipart upload/download is not currently supported by the server.")
        use_multipart = False

    if not use_multipart:
        if "://" in str(source) or "://" in str(destination):
            raise click.BadArgumentUsage(
                "Remote path syntax ($scheme://) is not supported without multipart transfers enabled."
            )
        return cp_v1(service, source, destination)  # type: ignore

    if isinstance(source, Path) and isinstance(destination, Path):
        return terminal.error("Source and destination cannot both be local paths.")
    if isinstance(source, RemotePath) and isinstance(destination, RemotePath):
        # TODO: Implement remote to remote copy
        return terminal.error("Source and destination cannot both be remote paths.")

    try:
        with StyledProgress() as p:
            multipart.copy(source, destination, service=service.volume, progress=p)
    except (KeyboardInterrupt, EOFError):
        terminal.warn("\rCancelled")
    except Exception as e:
        terminal.error(f"\rFailed: {e}")
    finally:
        terminal.reset_terminal()


@common.command(
    help="Remove content from a volume.",
    epilog="""
    Match Syntax:
      This command provides support for Unix shell-style wildcards, which are not the same as regular expressions.

      *       matches everything
      ?       matches any single character
      [seq]   matches any character in `seq`
      [!seq]  matches any character not in `seq`

      For a literal match, wrap the meta-characters in brackets. For example, '[?]' matches the character '?'.

    Examples:

      # Remove the directory
      {cli_name} rm myvol/subdir

      # Remove files ending in .json
      {cli_name} rm myvol/\\*.json

      # Remove files with letters a - c in their names
      {cli_name} rm myvol/\\[a-c\\].json

      # Remove files, use single quotes to avoid escaping
      {cli_name} rm 'myvol/?/[i-j][e-g]/*.txt'
      \b
    """,
)
@click.argument(
    "remote_path",
    type=click.STRING,
    required=True,
)
@extraclick.pass_service_client
def rm(service: ServiceClient, remote_path: str):
    req = DeletePathRequest(path=remote_path)
    res = service.volume.delete_path(req)

    if not res.ok:
        terminal.error(f"{remote_path} ({res.err_msg})")

    num_del, suffix = pluralize(res.deleted)
    terminal.header(f"{remote_path} ({num_del} object{suffix} deleted)")
    for deleted in res.deleted:
        terminal.print(deleted, highlight=False, markup=False)


@common.command(
    help="Move a file or directory to a new location within the same volume.",
    epilog="""
    Examples:

      # Move a directory within the same volume
      {cli_name} mv myvol/subdir1 myvol/subdir2

      # Move a file to another directory within the same volume
      {cli_name} mv myvol/subdir/file.txt myvol/anotherdir/file.txt
      \b
    """,
)
@click.argument(
    "original_path",
    type=click.STRING,
    required=True,
)
@click.argument(
    "new_path",
    type=click.STRING,
    required=True,
)
@extraclick.pass_service_client
def mv(service: ServiceClient, original_path: str, new_path: str):
    req = MovePathRequest(original_path=original_path, new_path=new_path)
    res = service.volume.move_path(req)

    if not res.ok:
        terminal.error(f"Failed to move {original_path} to {new_path} ({res.err_msg})")
    else:
        terminal.success(f"Moved {original_path} to {res.new_path}")


@click.group(
    name="volume",
    help="Manage volumes.",
    cls=ClickManagementGroup,
)
def management():
    pass


@management.command(
    name="list",
    help="List available volumes.",
)
@extraclick.pass_service_client
def list_volumes(service: ServiceClient):
    res: ListVolumesResponse
    res = service.volume.list_volumes(ListVolumesRequest())

    if not res.ok:
        terminal.error(res.err_msg)

    table = Table(
        Column("Name"),
        Column("Created At"),
        Column("Updated At"),
        Column("Workspace Name"),
        box=box.SIMPLE,
    )

    total_size = 0
    for volume in res.volumes:
        table.add_row(
            volume.name,
            terminal.humanize_date(volume.created_at),
            terminal.humanize_date(volume.updated_at),
            volume.workspace_name,
        )
        total_size += volume.size

    table.add_section()
    table.add_row(f"[bold]{len(res.volumes)} volumes")
    terminal.print(table)


@management.command(
    name="create",
    help="Create a new volume.",
)
@click.argument(
    "name",
    type=click.STRING,
    required=True,
)
@extraclick.pass_service_client
def create_volume(service: ServiceClient, name: str):
    res: GetOrCreateVolumeResponse
    res = service.volume.get_or_create_volume(GetOrCreateVolumeRequest(name=name))

    if not res.ok:
        terminal.print(res.volume)
        terminal.error(res.err_msg)

    table = Table(
        Column("Name"),
        Column("Created At"),
        Column("Updated At"),
        Column("Workspace Name"),
        box=box.SIMPLE,
    )

    table.add_row(
        res.volume.name,
        terminal.humanize_date(res.volume.created_at),
        terminal.humanize_date(res.volume.updated_at),
        res.volume.workspace_name,
    )
    terminal.print(table)


@management.command(
    name="delete",
    help="Delete a volume.",
)
@click.argument(
    "name",
    type=click.STRING,
    required=True,
)
@extraclick.pass_service_client
def delete_volume(service: ServiceClient, name: str):
    terminal.warn(
        "Any apps (functions, endpoints, taskqueue, etc) that\n"
        "refer to this volume should be updated before it is deleted."
    )

    if terminal.prompt(text="Are you sure? (y/n)", default="n") not in ["y", "yes"]:
        return

    res: DeleteVolumeResponse
    res = service.volume.delete_volume(DeleteVolumeRequest(name=name))

    if not res.ok:
        terminal.error(res.err_msg, exit=True)
    terminal.success(f"Deleted volume {name}")

================================================================================
# File: cli/worker.py
# Path: src/beta9/cli/worker.py
================================================================================

from typing import List

import click
from betterproto import Casing
from rich.table import Column, Table, box

from .. import terminal
from ..channel import ServiceClient
from ..cli import extraclick
from ..clients.gateway import (
    CordonWorkerRequest,
    DrainWorkerRequest,
    ListWorkersRequest,
    ListWorkersResponse,
    UncordonWorkerRequest,
)
from .extraclick import ClickCommonGroup, ClickManagementGroup


@click.group(cls=ClickCommonGroup)
def common(**_):
    pass


@click.group(
    name="worker",
    help="Manage workers.",
    cls=ClickManagementGroup,
)
def management():
    pass


@management.command(
    name="list",
    help="List all workers.",
    epilog="""
    Examples:

      # List workers and output in JSON format
      {cli_name} worker list --format json
      \b
    """,
)
@click.option(
    "--format",
    type=click.Choice(("table", "json")),
    default="table",
    show_default=True,
    help="Change the format of the output.",
)
@extraclick.pass_service_client
def list_workers(
    service: ServiceClient,
    format: str,
):
    res: ListWorkersResponse
    res = service.gateway.list_workers(ListWorkersRequest())

    if not res.ok:
        terminal.error(res.err_msg)

    if format == "json":
        workers = [d.to_dict(casing=Casing.SNAKE) for d in res.workers]  # type:ignore
        terminal.print_json(workers)
        return

    table = Table(
        Column("ID"),
        Column("Pool"),
        Column("Status"),
        Column("Machine ID"),
        Column("Priority"),
        Column("CPU"),
        Column("Memory"),
        Column("GPUs"),
        Column("CPU Available"),
        Column("Memory Available"),
        Column("GPUs Available"),
        Column("Containers"),
        Column("Version"),
        box=box.SIMPLE,
    )

    for worker in res.workers:
        table.add_row(
            worker.id,
            worker.pool_name,
            {
                "available": f"[green]{worker.status}[/green]",
                "pending": f"[yellow]{worker.status}[/yellow]",
                "disabled": f"[red]{worker.status}[/red]",
            }.get(worker.status, worker.status),
            worker.machine_id if worker.machine_id else "-",
            str(worker.priority),
            f"{worker.total_cpu:,}m" if worker.total_cpu > 0 else "-",
            terminal.humanize_memory(worker.total_memory * 1024 * 1024),
            str(worker.total_gpu_count),
            f"{worker.free_cpu:,}m",
            terminal.humanize_memory(worker.free_memory * 1024 * 1024),
            str(worker.free_gpu_count),
            str(len(worker.active_containers)),
            worker.build_version,
        )

    table.add_section()
    table.add_row(f"[bold]{len(res.workers)} items")

    terminal.print(table)


@management.command(
    name="cordon",
    help="""
    Cordon a worker.

    When a worker is cordoned, it will not accept new container requests. However,
    it will continue to run existing containers until they are finished. This is
    useful when you want to gracefully remove a worker from the pool.
    """,
    epilog="""
    Examples:

      # Cordon a worker
      {cli_name} worker cordon 675a65c3

      # Cordon multiple workers from stdin (useful for piping)
      {cli_name} worker list --format=json | jq -r '.[].id' | {cli_name} worker cordon -
      \b
    """,
)
@click.argument(
    "worker_ids",
    nargs=-1,
    required=True,
)
@extraclick.pass_service_client
def cordon_worker(service: ServiceClient, worker_ids: List[str]):
    if worker_ids and worker_ids[0] == "-":
        worker_ids = click.get_text_stream("stdin").read().strip().split()

    if not worker_ids:
        return terminal.error("Must provide at least one worker ID.")

    for worker_id in worker_ids:
        res = service.gateway.cordon_worker(CordonWorkerRequest(worker_id=worker_id))
        if not res.ok:
            text = res.err_msg.capitalize()
            terminal.warn(text)
        else:
            terminal.success(f"Worker {worker_id} has been cordoned.")


@management.command(
    name="uncordon",
    help="Uncordon a worker.",
    epilog="""
      Examples:

        # Uncordon a worker
        {cli_name} worker uncordon 675a65c3

        # Uncordon multiple workers
        {cli_name} worker uncordon 675a65c3 9c1b7bae 4c89436cs

        # Uncordon workers from stdin (useful for piping)
        {cli_name} worker list --format=json | jq -r '.[] | select(.status=="disabled") | .id' | {cli_name} worker uncordon -
        \b
    """,
)
@click.argument(
    "worker_ids",
    nargs=-1,
    required=True,
)
@extraclick.pass_service_client
def uncordon_worker(service: ServiceClient, worker_ids: List[str]):
    if worker_ids and worker_ids[0] == "-":
        worker_ids = click.get_text_stream("stdin").read().strip().split()

    if not worker_ids:
        return terminal.error("Must provide at least one worker ID.")

    for worker_id in worker_ids:
        res = service.gateway.uncordon_worker(UncordonWorkerRequest(worker_id=worker_id))
        if not res.ok:
            text = res.err_msg.capitalize()
            terminal.warn(text)
        else:
            terminal.success(f"Worker {worker_id} has been uncordon.")


@management.command(
    name="drain",
    help="""
    Drain a worker.

    When a worker is drained, all running containers on it will be stopped. The
    worker will continue to run until it reaches its idle timeout, but only if
    the worker was cordoned before being drained.
    """,
    epilog="""
      Examples:

        # Drain a worker
        {cli_name} worker drain 675a65c3

        # Drain multiple workers
        {cli_name} worker drain 675a65c3 9c1b7bae 4c89436cs

        # Drain workers from stdin (useful for piping)
        {cli_name} worker list --format=json | jq -r '.[].id' | {cli_name} worker drain -
        \b
    """,
)
@click.argument(
    "worker_ids",
    nargs=-1,
    required=True,
)
@extraclick.pass_service_client
def drain_worker(
    service: ServiceClient,
    worker_ids: List[str],
):
    if worker_ids and worker_ids[0] == "-":
        worker_ids = click.get_text_stream("stdin").read().strip().split()

    if not worker_ids:
        return terminal.error("Must provide at least one worker ID.")

    for worker_id in worker_ids:
        res = service.gateway.drain_worker(DrainWorkerRequest(worker_id=worker_id))
        if not res.ok:
            text = res.err_msg.capitalize()
            terminal.warn(text)
        else:
            terminal.success(f"Worker {worker_id} has been drained.")

================================================================================
# File: client/__init__.py
# Path: src/beta9/client/__init__.py
================================================================================

import json
from functools import lru_cache
from typing import Union

import requests

from ..exceptions import DeploymentNotFoundError, StubNotFoundError


def make_request(
    *, token: str, url: str, method: str, path: str, data: dict = {}, params: dict = None
) -> requests.Response:
    url = f"{url}{path}"
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json",
    }
    return requests.request(
        method, url, headers=headers, data=json.dumps(data) if data else None, params=params
    )


def post(*, token: str, url: str, path: str, data: dict = {}):
    return make_request(token=token, url=url, method="POST", path=path, data=data)


def get(*, token: str, url: str, path: str, params: dict = None):
    return make_request(token=token, url=url, method="GET", path=path, params=params)


@lru_cache(maxsize=128)
def get_stub_url(*, token: str, url: str, id: str) -> Union[str, None]:
    response = make_request(token=token, url=url, method="GET", path=f"/api/v1/stub/{id}/url")
    body = response.json()

    if body and "url" in body:
        return body["url"]

    raise StubNotFoundError(id)


@lru_cache(maxsize=128)
def get_deployment_url(*, token: str, url: str, id: str) -> Union[str, None]:
    response = make_request(token=token, url=url, method="GET", path=f"/api/v1/deployment/{id}/url")

    body = response.json()
    if body and "url" in body:
        return body["url"]

    raise DeploymentNotFoundError(id)

================================================================================
# File: client/client.py
# Path: src/beta9/client/client.py
================================================================================

import http
import uuid
from pathlib import Path

import requests

from ..exceptions import VolumeUploadError, WorkspaceNotFoundError
from . import get, make_request
from .deployment import Deployment
from .task import Task

VOLUME_UPLOAD_PATH = "uploads"


class Client:
    def __init__(
        self,
        token: str,
        gateway_host: str = "0.0.0.0",
        gateway_port: int = 1994,
        tls: bool = False,
    ) -> None:
        self.token: str = token
        self.gateway_host: str = gateway_host
        self.gateway_port: int = gateway_port
        self.tls: bool = tls
        self.base_url: str = self._get_base_url()
        self._load_workspace()

    def _load_workspace(self):
        response = make_request(
            token=self.token,
            url=self.base_url,
            method="GET",
            path="/api/v1/workspace/current",
        )

        body = response.json()
        if body and "external_id" in body:
            self.workspace_id = body["external_id"]
        else:
            raise WorkspaceNotFoundError("Failed to load workspace")

    def _get_base_url(self):
        if self.gateway_host.startswith(("http://", "https://")):
            return f"{self.gateway_host}:{self.gateway_port}"
        return f"{'https' if self.tls else 'http'}://{self.gateway_host}:{self.gateway_port}"

    def upload_file(self, local_path: str = "") -> str:
        """Upload a file to to be used as an input to some function or deployment."""

        path = Path(local_path)
        filename = f"{path.stem}_{uuid.uuid4()}{path.suffix}"
        volume_path = str(path.parent / filename) if path.parent != Path(".") else filename

        try:
            response = get(
                token=self.token,
                url=self.base_url,
                path=f"/volume/{self.workspace_id}/generate-upload-url/{VOLUME_UPLOAD_PATH}/{volume_path}",
            )
            response.raise_for_status()
        except BaseException as e:
            raise VolumeUploadError(f"Failed to get upload URL: {e}")

        if response.status_code == http.HTTPStatus.OK:
            presigned_url = response.json()

            with open(local_path, "rb") as file:
                r = requests.put(presigned_url, data=file)
                if r.status_code != http.HTTPStatus.OK:
                    raise VolumeUploadError(f"Failed to upload file: {r.text}")

        response = get(
            token=self.token,
            url=self.base_url,
            path=f"/volume/{self.workspace_id}/generate-download-url/{VOLUME_UPLOAD_PATH}/{volume_path}",
        )
        response.raise_for_status()
        return response.json()

    def get_task_by_id(self, id: str) -> Task:
        """Retrieve a task by task ID."""
        return Task(
            id=id, url=f"{self.base_url}/api/v1/task/{self.workspace_id}/{id}", token=self.token
        )

    def get_deployment_by_id(self, id: str) -> Deployment:
        """Retrieve a deployment using its deployment ID."""
        return Deployment(
            base_url=self.base_url,
            token=self.token,
            workspace_id=self.workspace_id,
            deployment_id=id,
        )

    def get_deployment_by_stub_id(self, stub_id: str) -> Deployment:
        """Retrieve a deployment using the associated stub ID."""
        return Deployment(
            base_url=self.base_url,
            token=self.token,
            workspace_id=self.workspace_id,
            stub_id=stub_id,
        )

================================================================================
# File: client/deployment.py
# Path: src/beta9/client/deployment.py
================================================================================

from typing import Any, Callable, Optional, Union

from ..exceptions import DeploymentNotFoundError, TaskNotFoundError
from . import get_deployment_url, get_stub_url, post
from .task import Task


class Deployment:
    def __init__(
        self,
        *,
        base_url: str,
        token: str,
        workspace_id: str,
        stub_id: Optional[str] = None,
        deployment_id: Optional[str] = None,
        deployment_url: Optional[str] = None,
    ):
        self.stub_id = stub_id
        self.deployment_id = deployment_id
        self.token = token
        self.base_url = base_url
        self.workspace_id = workspace_id

        if deployment_url:
            self.url = deployment_url
        elif self.stub_id:
            self.url = get_stub_url(token=self.token, url=self.base_url, id=self.stub_id)
        elif self.deployment_id:
            self.url = get_deployment_url(
                token=self.token, url=self.base_url, id=self.deployment_id
            )
        else:
            raise ValueError(
                "Deployment() requires either a deployment ID, stub ID, or deployment URL"
            )

    def submit(self, *, input: dict = {}) -> Union[Task, Any]:
        """Submit a task to the deployment. Returns a Task object if the task runs asynchronously, otherwise blocks until the task is complete and returns a JSON response."""

        if not self.url:
            raise TaskNotFoundError(f"Failed to get retrieve URL for task {self.id}")

        response = post(token=self.token, url=self.url, path="", data=input)
        body = response.json()
        if body is not None and "task_id" in body:
            return Task(
                id=body["task_id"],
                url=f"{self.base_url}/api/v1/task/{self.workspace_id}/{body['task_id']}",
                token=self.token,
            )

        return body

    def subscribe(self, *, input: dict = {}, event_handler: Callable = None) -> Any:
        """Submit a task to the deployment, and subscribe to the task. Yields updates as task status changes. Returns the JSON response from the deployment, or None."""

        if not self.url:
            raise DeploymentNotFoundError(f"Failed to get retrieve URL for task {id}")

        response = post(token=self.token, url=self.url, path="", data=input)
        body = response.json()
        if body is not None and "task_id" not in body:
            return body

        task = Task(
            id=body["task_id"],
            url=f"{self.base_url}/api/v1/task/{self.workspace_id}/{body['task_id']}",
            token=self.token,
        )

        return task.subscribe(event_handler=event_handler)

================================================================================
# File: client/task.py
# Path: src/beta9/client/task.py
================================================================================

import base64
import json
from dataclasses import dataclass, field
from http import HTTPStatus
from typing import Any, Callable, List, Optional

import cloudpickle
import requests

from ..exceptions import TaskNotFoundError
from ..type import TaskStatus


@dataclass
class Task:
    id: str
    url: str
    _status: Optional[TaskStatus] = None
    _result: Optional[Any] = None
    _outputs: Optional[Any] = field(default_factory=list)

    def __init__(self, id: str, url: str, token: str):
        self.id = id
        self.url = url
        self.__token = token
        self._get()

    def _get(self):
        headers = {
            "Authorization": f"Bearer {self.__token}",
            "Content-Type": "application/json",
        }
        response = requests.get(self.url, headers=headers)
        if response.status_code == HTTPStatus.OK:
            body = response.json()
            self._status = TaskStatus(body["status"])
            self._result = body["result"]
            self._outputs = body["outputs"]
        elif response.status_code == HTTPStatus.NOT_FOUND:
            raise TaskNotFoundError(self.id)
        else:
            response.raise_for_status()

        if self._result and self._result.get("base64"):
            self._result = cloudpickle.loads(base64.b64decode(self._result["base64"]))

    def status(self) -> TaskStatus:
        """Returns the status of the task"""
        self._get()
        return self._status

    def result(self, wait: bool = False) -> Any:
        """Returns the JSON output of the task. If wait is True, blocks until the task is complete and returns the result."""
        if wait:
            return self.subscribe()

        self._get()
        return self._result

    def outputs(self) -> List:
        """Returns a list of the Output() objects saved during the duration of the task"""
        return self._outputs

    def is_complete(self) -> bool:
        """Returns True if the task is in a terminal state (COMPLETE, ERROR, TIMEOUT, CANCELLED)"""
        self._get()
        return self._status.is_complete()

    def subscribe(self, event_handler: Callable = None) -> Any:
        """Subscribe to a task and yield updates as task status changes. Returns an iterable of JSON objects."""
        headers = {
            "Authorization": f"Bearer {self.__token}",
            "Content-Type": "application/json",
            "Accept": "text/event-stream",
        }

        try:
            with requests.get(f"{self.url}/subscribe", headers=headers, stream=True) as response:
                response.raise_for_status()
                buffer = ""

                for chunk in response.iter_content(chunk_size=1024, decode_unicode=True):
                    if not chunk:
                        continue

                    buffer += chunk
                    while "\n\n" in buffer:
                        event, buffer = buffer.split("\n\n", 1)
                        event_type = None
                        data = None

                        for line in event.split("\n"):
                            if line.startswith("event: "):
                                event_type = line[7:]
                            elif line.startswith("data: "):
                                data = line[6:]

                        if event_type == "status" and data:
                            try:
                                task_data = json.loads(data)

                                if event_handler:
                                    event_handler(task_data)

                                # Stop iteration if task is done running
                                status = TaskStatus(task_data.get("status"))
                                if status.is_complete():
                                    self._status = status
                                    self._result = task_data.get("result")
                                    self._outputs = task_data.get("outputs")

                                    if self._result and self._result.get("base64"):
                                        self._result = cloudpickle.loads(
                                            base64.b64decode(self._result["base64"])
                                        )

                                    return self._result

                            except json.JSONDecodeError:
                                continue
        except GeneratorExit:
            raise
        except BaseException as e:
            return {"error": str(e)}

================================================================================
# File: clients/__init__.py
# Path: src/beta9/clients/__init__.py
================================================================================


================================================================================
# File: clients/bot/__init__.py
# Path: src/beta9/clients/bot/__init__.py
================================================================================

# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: bot.proto
# plugin: python-betterproto
# This file has been @generated

from dataclasses import dataclass
from typing import (
    TYPE_CHECKING,
    Dict,
    List,
    Optional,
)

import betterproto
import grpc
from betterproto.grpcstub.grpcio_client import SyncServiceStub
from betterproto.grpcstub.grpclib_server import ServiceBase


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpcstub.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


@dataclass(eq=False, repr=False)
class PopBotTaskRequest(betterproto.Message):
    stub_id: str = betterproto.string_field(1)
    session_id: str = betterproto.string_field(2)
    transition_name: str = betterproto.string_field(3)
    task_id: str = betterproto.string_field(4)


@dataclass(eq=False, repr=False)
class PopBotTaskResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    markers: Dict[str, "PopBotTaskResponseMarkerList"] = betterproto.map_field(
        2, betterproto.TYPE_STRING, betterproto.TYPE_MESSAGE
    )


@dataclass(eq=False, repr=False)
class PopBotTaskResponseMarkerList(betterproto.Message):
    markers: List["Marker"] = betterproto.message_field(1)


@dataclass(eq=False, repr=False)
class MarkerField(betterproto.Message):
    field_name: str = betterproto.string_field(1)
    field_value: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class Marker(betterproto.Message):
    location_name: str = betterproto.string_field(1)
    fields: List["MarkerField"] = betterproto.message_field(2)
    source_task_id: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class PushBotMarkersRequest(betterproto.Message):
    stub_id: str = betterproto.string_field(1)
    session_id: str = betterproto.string_field(2)
    markers: Dict[str, "PushBotMarkersRequestMarkerList"] = betterproto.map_field(
        3, betterproto.TYPE_STRING, betterproto.TYPE_MESSAGE
    )
    source_task_id: str = betterproto.string_field(5)


@dataclass(eq=False, repr=False)
class PushBotMarkersRequestMarkerList(betterproto.Message):
    markers: List["Marker"] = betterproto.message_field(4)


@dataclass(eq=False, repr=False)
class PushBotMarkersResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)


@dataclass(eq=False, repr=False)
class PushBotEventRequest(betterproto.Message):
    stub_id: str = betterproto.string_field(1)
    session_id: str = betterproto.string_field(2)
    event_type: str = betterproto.string_field(3)
    event_value: str = betterproto.string_field(4)
    metadata: Dict[str, str] = betterproto.map_field(
        5, betterproto.TYPE_STRING, betterproto.TYPE_STRING
    )


@dataclass(eq=False, repr=False)
class PushBotEventResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    event_id: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class PushBotEventBlockingRequest(betterproto.Message):
    stub_id: str = betterproto.string_field(1)
    session_id: str = betterproto.string_field(2)
    event_type: str = betterproto.string_field(3)
    event_value: str = betterproto.string_field(4)
    metadata: Dict[str, str] = betterproto.map_field(
        5, betterproto.TYPE_STRING, betterproto.TYPE_STRING
    )
    timeout_seconds: int = betterproto.int32_field(6)


@dataclass(eq=False, repr=False)
class PushBotEventBlockingResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    event: "BotEvent" = betterproto.message_field(2)


@dataclass(eq=False, repr=False)
class BotEvent(betterproto.Message):
    type: str = betterproto.string_field(1)
    value: str = betterproto.string_field(2)
    metadata: Dict[str, str] = betterproto.map_field(
        3, betterproto.TYPE_STRING, betterproto.TYPE_STRING
    )


class BotServiceStub(SyncServiceStub):
    def pop_bot_task(
        self, pop_bot_task_request: "PopBotTaskRequest"
    ) -> "PopBotTaskResponse":
        return self._unary_unary(
            "/bot.BotService/PopBotTask",
            PopBotTaskRequest,
            PopBotTaskResponse,
        )(pop_bot_task_request)

    def push_bot_markers(
        self, push_bot_markers_request: "PushBotMarkersRequest"
    ) -> "PushBotMarkersResponse":
        return self._unary_unary(
            "/bot.BotService/PushBotMarkers",
            PushBotMarkersRequest,
            PushBotMarkersResponse,
        )(push_bot_markers_request)

    def push_bot_event(
        self, push_bot_event_request: "PushBotEventRequest"
    ) -> "PushBotEventResponse":
        return self._unary_unary(
            "/bot.BotService/PushBotEvent",
            PushBotEventRequest,
            PushBotEventResponse,
        )(push_bot_event_request)

    def push_bot_event_blocking(
        self, push_bot_event_blocking_request: "PushBotEventBlockingRequest"
    ) -> "PushBotEventBlockingResponse":
        return self._unary_unary(
            "/bot.BotService/PushBotEventBlocking",
            PushBotEventBlockingRequest,
            PushBotEventBlockingResponse,
        )(push_bot_event_blocking_request)

================================================================================
# File: clients/endpoint/__init__.py
# Path: src/beta9/clients/endpoint/__init__.py
================================================================================

# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: endpoint.proto
# plugin: python-betterproto
# This file has been @generated

from dataclasses import dataclass
from typing import (
    TYPE_CHECKING,
    Dict,
    Optional,
)

import betterproto
import grpc
from betterproto.grpcstub.grpcio_client import SyncServiceStub
from betterproto.grpcstub.grpclib_server import ServiceBase


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpcstub.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


@dataclass(eq=False, repr=False)
class StartEndpointServeRequest(betterproto.Message):
    stub_id: str = betterproto.string_field(1)
    timeout: int = betterproto.int32_field(2)


@dataclass(eq=False, repr=False)
class StartEndpointServeResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    container_id: str = betterproto.string_field(2)
    error_msg: str = betterproto.string_field(3)


class EndpointServiceStub(SyncServiceStub):
    def start_endpoint_serve(
        self, start_endpoint_serve_request: "StartEndpointServeRequest"
    ) -> "StartEndpointServeResponse":
        return self._unary_unary(
            "/endpoint.EndpointService/StartEndpointServe",
            StartEndpointServeRequest,
            StartEndpointServeResponse,
        )(start_endpoint_serve_request)

================================================================================
# File: clients/function/__init__.py
# Path: src/beta9/clients/function/__init__.py
================================================================================

# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: function.proto
# plugin: python-betterproto
# This file has been @generated

from dataclasses import dataclass
from typing import (
    TYPE_CHECKING,
    AsyncIterator,
    Dict,
    Iterator,
    Optional,
)

import betterproto
import grpc
from betterproto.grpcstub.grpcio_client import SyncServiceStub
from betterproto.grpcstub.grpclib_server import ServiceBase


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpcstub.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


@dataclass(eq=False, repr=False)
class FunctionInvokeRequest(betterproto.Message):
    stub_id: str = betterproto.string_field(1)
    args: bytes = betterproto.bytes_field(2)
    headless: bool = betterproto.bool_field(3)


@dataclass(eq=False, repr=False)
class FunctionInvokeResponse(betterproto.Message):
    task_id: str = betterproto.string_field(1)
    output: str = betterproto.string_field(2)
    done: bool = betterproto.bool_field(3)
    exit_code: int = betterproto.int32_field(4)
    result: bytes = betterproto.bytes_field(5)


@dataclass(eq=False, repr=False)
class FunctionGetArgsRequest(betterproto.Message):
    task_id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class FunctionGetArgsResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    args: bytes = betterproto.bytes_field(2)


@dataclass(eq=False, repr=False)
class FunctionSetResultRequest(betterproto.Message):
    task_id: str = betterproto.string_field(1)
    result: bytes = betterproto.bytes_field(2)


@dataclass(eq=False, repr=False)
class FunctionSetResultResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)


@dataclass(eq=False, repr=False)
class FunctionMonitorRequest(betterproto.Message):
    task_id: str = betterproto.string_field(1)
    stub_id: str = betterproto.string_field(2)
    container_id: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class FunctionMonitorResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    cancelled: bool = betterproto.bool_field(2)
    complete: bool = betterproto.bool_field(3)
    timed_out: bool = betterproto.bool_field(4)


@dataclass(eq=False, repr=False)
class FunctionScheduleRequest(betterproto.Message):
    stub_id: str = betterproto.string_field(1)
    when: str = betterproto.string_field(2)
    deployment_id: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class FunctionScheduleResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    scheduled_job_id: str = betterproto.string_field(3)


class FunctionServiceStub(SyncServiceStub):
    def function_invoke(
        self, function_invoke_request: "FunctionInvokeRequest"
    ) -> Iterator["FunctionInvokeResponse"]:
        for response in self._unary_stream(
            "/function.FunctionService/FunctionInvoke",
            FunctionInvokeRequest,
            FunctionInvokeResponse,
        )(function_invoke_request):
            yield response

    def function_get_args(
        self, function_get_args_request: "FunctionGetArgsRequest"
    ) -> "FunctionGetArgsResponse":
        return self._unary_unary(
            "/function.FunctionService/FunctionGetArgs",
            FunctionGetArgsRequest,
            FunctionGetArgsResponse,
        )(function_get_args_request)

    def function_set_result(
        self, function_set_result_request: "FunctionSetResultRequest"
    ) -> "FunctionSetResultResponse":
        return self._unary_unary(
            "/function.FunctionService/FunctionSetResult",
            FunctionSetResultRequest,
            FunctionSetResultResponse,
        )(function_set_result_request)

    def function_monitor(
        self, function_monitor_request: "FunctionMonitorRequest"
    ) -> Iterator["FunctionMonitorResponse"]:
        for response in self._unary_stream(
            "/function.FunctionService/FunctionMonitor",
            FunctionMonitorRequest,
            FunctionMonitorResponse,
        )(function_monitor_request):
            yield response

    def function_schedule(
        self, function_schedule_request: "FunctionScheduleRequest"
    ) -> "FunctionScheduleResponse":
        return self._unary_unary(
            "/function.FunctionService/FunctionSchedule",
            FunctionScheduleRequest,
            FunctionScheduleResponse,
        )(function_schedule_request)

================================================================================
# File: clients/gateway/__init__.py
# Path: src/beta9/clients/gateway/__init__.py
================================================================================

# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: gateway.proto
# plugin: python-betterproto
# This file has been @generated

from dataclasses import dataclass
from datetime import datetime
from typing import (
    TYPE_CHECKING,
    AsyncIterable,
    AsyncIterator,
    Dict,
    Iterable,
    Iterator,
    List,
    Optional,
    Union,
)

import betterproto
import grpc
from betterproto.grpcstub.grpcio_client import SyncServiceStub
from betterproto.grpcstub.grpclib_server import ServiceBase

from .. import types as _types__


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpcstub.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


class SyncContainerWorkspaceOperation(betterproto.Enum):
    WRITE = 0
    DELETE = 1
    MOVED = 2


@dataclass(eq=False, repr=False)
class AuthorizeRequest(betterproto.Message):
    pass


@dataclass(eq=False, repr=False)
class AuthorizeResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    workspace_id: str = betterproto.string_field(2)
    new_token: str = betterproto.string_field(3)
    error_msg: str = betterproto.string_field(4)


@dataclass(eq=False, repr=False)
class SignPayloadRequest(betterproto.Message):
    payload: bytes = betterproto.bytes_field(1)


@dataclass(eq=False, repr=False)
class SignPayloadResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    signature: str = betterproto.string_field(2)
    timestamp: int = betterproto.int64_field(3)
    error_msg: str = betterproto.string_field(4)


@dataclass(eq=False, repr=False)
class ObjectMetadata(betterproto.Message):
    name: str = betterproto.string_field(1)
    size: int = betterproto.int64_field(2)


@dataclass(eq=False, repr=False)
class HeadObjectRequest(betterproto.Message):
    hash: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class HeadObjectResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    exists: bool = betterproto.bool_field(2)
    object_id: str = betterproto.string_field(3)
    object_metadata: "ObjectMetadata" = betterproto.message_field(4)
    error_msg: str = betterproto.string_field(5)
    use_workspace_storage: bool = betterproto.bool_field(6)


@dataclass(eq=False, repr=False)
class CreateObjectRequest(betterproto.Message):
    object_metadata: "ObjectMetadata" = betterproto.message_field(1)
    hash: str = betterproto.string_field(2)
    size: int = betterproto.int64_field(3)
    overwrite: bool = betterproto.bool_field(4)


@dataclass(eq=False, repr=False)
class CreateObjectResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    object_id: str = betterproto.string_field(2)
    presigned_url: str = betterproto.string_field(3)
    error_msg: str = betterproto.string_field(4)


@dataclass(eq=False, repr=False)
class PutObjectRequest(betterproto.Message):
    object_content: bytes = betterproto.bytes_field(1)
    object_metadata: "ObjectMetadata" = betterproto.message_field(2)
    hash: str = betterproto.string_field(3)
    overwrite: bool = betterproto.bool_field(4)


@dataclass(eq=False, repr=False)
class PutObjectResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    object_id: str = betterproto.string_field(2)
    error_msg: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class SyncContainerWorkspaceRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    path: str = betterproto.string_field(2)
    new_path: str = betterproto.string_field(3)
    is_dir: bool = betterproto.bool_field(4)
    data: bytes = betterproto.bytes_field(5)
    op: "SyncContainerWorkspaceOperation" = betterproto.enum_field(6)


@dataclass(eq=False, repr=False)
class SyncContainerWorkspaceResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)


@dataclass(eq=False, repr=False)
class ListContainersRequest(betterproto.Message):
    pass


@dataclass(eq=False, repr=False)
class ListContainersResponse(betterproto.Message):
    containers: List["_types__.Container"] = betterproto.message_field(1)
    ok: bool = betterproto.bool_field(2)
    error_msg: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class StopContainerRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class StopContainerResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    error_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class ContainerStreamMessage(betterproto.Message):
    attach_request: "AttachToContainerRequest" = betterproto.message_field(
        1, group="payload"
    )
    sync_container_workspace: "SyncContainerWorkspaceRequest" = (
        betterproto.message_field(2, group="payload")
    )


@dataclass(eq=False, repr=False)
class AttachToContainerRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class AttachToContainerResponse(betterproto.Message):
    output: str = betterproto.string_field(1)
    done: bool = betterproto.bool_field(2)
    exit_code: int = betterproto.int32_field(3)


@dataclass(eq=False, repr=False)
class StartTaskRequest(betterproto.Message):
    """Task messages"""

    task_id: str = betterproto.string_field(1)
    container_id: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class StartTaskResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)


@dataclass(eq=False, repr=False)
class EndTaskRequest(betterproto.Message):
    task_id: str = betterproto.string_field(1)
    task_duration: float = betterproto.float_field(2)
    task_status: str = betterproto.string_field(3)
    container_id: str = betterproto.string_field(4)
    container_hostname: str = betterproto.string_field(5)
    keep_warm_seconds: float = betterproto.float_field(6)
    result: bytes = betterproto.bytes_field(7)


@dataclass(eq=False, repr=False)
class EndTaskResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)


@dataclass(eq=False, repr=False)
class StringList(betterproto.Message):
    values: List[str] = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class ListTasksRequest(betterproto.Message):
    filters: Dict[str, "StringList"] = betterproto.map_field(
        1, betterproto.TYPE_STRING, betterproto.TYPE_MESSAGE
    )
    limit: int = betterproto.uint32_field(2)


@dataclass(eq=False, repr=False)
class Task(betterproto.Message):
    id: str = betterproto.string_field(2)
    status: str = betterproto.string_field(3)
    container_id: str = betterproto.string_field(4)
    started_at: datetime = betterproto.message_field(5)
    ended_at: datetime = betterproto.message_field(6)
    stub_id: str = betterproto.string_field(7)
    stub_name: str = betterproto.string_field(8)
    workspace_id: str = betterproto.string_field(9)
    workspace_name: str = betterproto.string_field(10)
    created_at: datetime = betterproto.message_field(11)
    updated_at: datetime = betterproto.message_field(12)


@dataclass(eq=False, repr=False)
class ListTasksResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    tasks: List["Task"] = betterproto.message_field(3)
    total: int = betterproto.int32_field(4)


@dataclass(eq=False, repr=False)
class StopTasksRequest(betterproto.Message):
    task_ids: List[str] = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class StopTasksResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class Volume(betterproto.Message):
    id: str = betterproto.string_field(1)
    mount_path: str = betterproto.string_field(2)
    config: Optional["_types__.MountPointConfig"] = betterproto.message_field(
        3, optional=True
    )


@dataclass(eq=False, repr=False)
class SecretVar(betterproto.Message):
    name: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class Autoscaler(betterproto.Message):
    type: str = betterproto.string_field(1)
    max_containers: int = betterproto.uint32_field(2)
    tasks_per_container: int = betterproto.uint32_field(3)
    min_containers: int = betterproto.uint32_field(4)


@dataclass(eq=False, repr=False)
class TaskPolicy(betterproto.Message):
    timeout: int = betterproto.int64_field(1)
    max_retries: int = betterproto.uint32_field(2)
    ttl: int = betterproto.uint32_field(3)


@dataclass(eq=False, repr=False)
class Schema(betterproto.Message):
    fields: Dict[str, "SchemaField"] = betterproto.map_field(
        1, betterproto.TYPE_STRING, betterproto.TYPE_MESSAGE
    )


@dataclass(eq=False, repr=False)
class SchemaField(betterproto.Message):
    type: str = betterproto.string_field(1)
    fields: "Schema" = betterproto.message_field(2)


@dataclass(eq=False, repr=False)
class GetOrCreateStubRequest(betterproto.Message):
    object_id: str = betterproto.string_field(1)
    image_id: str = betterproto.string_field(2)
    stub_type: str = betterproto.string_field(3)
    name: str = betterproto.string_field(4)
    python_version: str = betterproto.string_field(5)
    cpu: int = betterproto.int64_field(6)
    memory: int = betterproto.int64_field(7)
    gpu: str = betterproto.string_field(8)
    handler: str = betterproto.string_field(9)
    retries: int = betterproto.uint32_field(10)
    timeout: int = betterproto.int64_field(11)
    keep_warm_seconds: float = betterproto.float_field(12)
    workers: int = betterproto.uint32_field(13)
    max_pending_tasks: int = betterproto.uint32_field(15)
    volumes: List["Volume"] = betterproto.message_field(16)
    force_create: bool = betterproto.bool_field(17)
    on_start: str = betterproto.string_field(18)
    callback_url: str = betterproto.string_field(19)
    authorized: bool = betterproto.bool_field(20)
    secrets: List["SecretVar"] = betterproto.message_field(21)
    autoscaler: "Autoscaler" = betterproto.message_field(22)
    task_policy: "TaskPolicy" = betterproto.message_field(23)
    concurrent_requests: int = betterproto.uint32_field(24)
    extra: str = betterproto.string_field(25)
    checkpoint_enabled: bool = betterproto.bool_field(26)
    gpu_count: int = betterproto.uint32_field(27)
    on_deploy: str = betterproto.string_field(28)
    on_deploy_stub_id: str = betterproto.string_field(29)
    entrypoint: List[str] = betterproto.string_field(30)
    ports: List[int] = betterproto.uint32_field(31)
    env: List[str] = betterproto.string_field(32)
    app_name: str = betterproto.string_field(33)
    pricing: "_types__.PricingPolicy" = betterproto.message_field(34)
    inputs: "Schema" = betterproto.message_field(35)
    outputs: "Schema" = betterproto.message_field(36)


@dataclass(eq=False, repr=False)
class GetOrCreateStubResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    stub_id: str = betterproto.string_field(2)
    err_msg: str = betterproto.string_field(3)
    warn_msg: str = betterproto.string_field(4)


@dataclass(eq=False, repr=False)
class DeployStubRequest(betterproto.Message):
    stub_id: str = betterproto.string_field(1)
    name: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class DeployStubResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    deployment_id: str = betterproto.string_field(2)
    version: int = betterproto.uint32_field(3)
    invoke_url: str = betterproto.string_field(4)


@dataclass(eq=False, repr=False)
class Deployment(betterproto.Message):
    id: str = betterproto.string_field(1)
    name: str = betterproto.string_field(2)
    active: bool = betterproto.bool_field(3)
    stub_id: str = betterproto.string_field(4)
    stub_type: str = betterproto.string_field(5)
    stub_name: str = betterproto.string_field(6)
    version: int = betterproto.uint32_field(7)
    workspace_id: str = betterproto.string_field(8)
    workspace_name: str = betterproto.string_field(9)
    created_at: datetime = betterproto.message_field(10)
    updated_at: datetime = betterproto.message_field(11)
    app_id: str = betterproto.string_field(12)


@dataclass(eq=False, repr=False)
class ListDeploymentsRequest(betterproto.Message):
    filters: Dict[str, "StringList"] = betterproto.map_field(
        1, betterproto.TYPE_STRING, betterproto.TYPE_MESSAGE
    )
    limit: int = betterproto.uint32_field(2)


@dataclass(eq=False, repr=False)
class ListDeploymentsResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    deployments: List["Deployment"] = betterproto.message_field(3)


@dataclass(eq=False, repr=False)
class StopDeploymentRequest(betterproto.Message):
    id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class StopDeploymentResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class StartDeploymentRequest(betterproto.Message):
    id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class StartDeploymentResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class ScaleDeploymentRequest(betterproto.Message):
    id: str = betterproto.string_field(1)
    containers: int = betterproto.uint32_field(2)


@dataclass(eq=False, repr=False)
class ScaleDeploymentResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class DeleteDeploymentRequest(betterproto.Message):
    id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class DeleteDeploymentResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class Pool(betterproto.Message):
    name: str = betterproto.string_field(2)
    active: bool = betterproto.bool_field(3)
    gpu: str = betterproto.string_field(4)
    min_free_gpu: str = betterproto.string_field(5)
    min_free_cpu: str = betterproto.string_field(6)
    min_free_memory: str = betterproto.string_field(7)
    default_worker_cpu: str = betterproto.string_field(8)
    default_worker_memory: str = betterproto.string_field(9)
    default_worker_gpu_count: str = betterproto.string_field(10)
    state: "_types__.WorkerPoolState" = betterproto.message_field(11)


@dataclass(eq=False, repr=False)
class ListPoolsRequest(betterproto.Message):
    filters: Dict[str, "StringList"] = betterproto.map_field(
        1, betterproto.TYPE_STRING, betterproto.TYPE_MESSAGE
    )
    limit: int = betterproto.uint32_field(2)


@dataclass(eq=False, repr=False)
class ListPoolsResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    pools: List["Pool"] = betterproto.message_field(3)


@dataclass(eq=False, repr=False)
class Machine(betterproto.Message):
    id: str = betterproto.string_field(1)
    cpu: int = betterproto.int64_field(2)
    memory: int = betterproto.int64_field(3)
    gpu: str = betterproto.string_field(4)
    gpu_count: int = betterproto.uint32_field(5)
    status: str = betterproto.string_field(6)
    pool_name: str = betterproto.string_field(7)
    provider_name: str = betterproto.string_field(8)
    registration_token: str = betterproto.string_field(9)
    tailscale_url: str = betterproto.string_field(10)
    tailscale_auth: str = betterproto.string_field(11)
    last_keepalive: str = betterproto.string_field(12)
    created: str = betterproto.string_field(13)
    agent_version: str = betterproto.string_field(14)
    machine_metrics: "MachineMetrics" = betterproto.message_field(15)
    user_data: str = betterproto.string_field(16)


@dataclass(eq=False, repr=False)
class MachineMetrics(betterproto.Message):
    total_cpu_available: int = betterproto.int32_field(1)
    total_memory_available: int = betterproto.int32_field(2)
    cpu_utilization_pct: float = betterproto.float_field(3)
    memory_utilization_pct: float = betterproto.float_field(4)
    worker_count: int = betterproto.int32_field(5)
    container_count: int = betterproto.int32_field(6)
    free_gpu_count: int = betterproto.int32_field(7)
    cache_usage_pct: float = betterproto.float_field(8)
    cache_capacity: int = betterproto.int32_field(9)
    cache_memory_usage: int = betterproto.int32_field(10)
    cache_cpu_usage: float = betterproto.float_field(11)


@dataclass(eq=False, repr=False)
class ListMachinesRequest(betterproto.Message):
    pool_name: str = betterproto.string_field(1)
    limit: int = betterproto.uint32_field(2)


@dataclass(eq=False, repr=False)
class ListMachinesResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    machines: List["Machine"] = betterproto.message_field(3)
    gpus: Dict[str, bool] = betterproto.map_field(
        4, betterproto.TYPE_STRING, betterproto.TYPE_BOOL
    )


@dataclass(eq=False, repr=False)
class CreateMachineRequest(betterproto.Message):
    pool_name: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class CreateMachineResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    machine: "Machine" = betterproto.message_field(3)
    agent_upstream_url: str = betterproto.string_field(4)
    agent_upstream_branch: str = betterproto.string_field(5)


@dataclass(eq=False, repr=False)
class DeleteMachineRequest(betterproto.Message):
    machine_id: str = betterproto.string_field(1)
    pool_name: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class DeleteMachineResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class Token(betterproto.Message):
    token_id: str = betterproto.string_field(1)
    key: str = betterproto.string_field(2)
    active: bool = betterproto.bool_field(3)
    reusable: bool = betterproto.bool_field(4)
    workspace_id: Optional[int] = betterproto.uint32_field(5, optional=True)
    token_type: str = betterproto.string_field(6)
    created_at: datetime = betterproto.message_field(7)
    updated_at: datetime = betterproto.message_field(8)


@dataclass(eq=False, repr=False)
class ListTokensRequest(betterproto.Message):
    pass


@dataclass(eq=False, repr=False)
class ListTokensResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    tokens: List["Token"] = betterproto.message_field(3)


@dataclass(eq=False, repr=False)
class CreateTokenRequest(betterproto.Message):
    pass


@dataclass(eq=False, repr=False)
class CreateTokenResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    token: "Token" = betterproto.message_field(3)


@dataclass(eq=False, repr=False)
class ToggleTokenRequest(betterproto.Message):
    token_id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class ToggleTokenResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    token: "Token" = betterproto.message_field(3)


@dataclass(eq=False, repr=False)
class DeleteTokenRequest(betterproto.Message):
    token_id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class DeleteTokenResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class GetUrlRequest(betterproto.Message):
    stub_id: str = betterproto.string_field(1)
    deployment_id: str = betterproto.string_field(2)
    url_type: str = betterproto.string_field(3)
    is_shell: bool = betterproto.bool_field(4)


@dataclass(eq=False, repr=False)
class GetUrlResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    url: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class ListWorkersRequest(betterproto.Message):
    pass


@dataclass(eq=False, repr=False)
class ListWorkersResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    workers: List["_types__.Worker"] = betterproto.message_field(3)


@dataclass(eq=False, repr=False)
class CordonWorkerRequest(betterproto.Message):
    worker_id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class CordonWorkerResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class UncordonWorkerRequest(betterproto.Message):
    worker_id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class UncordonWorkerResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class DrainWorkerRequest(betterproto.Message):
    worker_id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class DrainWorkerResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class ExportWorkspaceConfigRequest(betterproto.Message):
    pass


@dataclass(eq=False, repr=False)
class ExportWorkspaceConfigResponse(betterproto.Message):
    gateway_http_host: str = betterproto.string_field(1)
    gateway_http_port: int = betterproto.int32_field(2)
    gateway_http_tls: bool = betterproto.bool_field(3)
    gateway_grpc_host: str = betterproto.string_field(4)
    gateway_grpc_port: int = betterproto.int32_field(5)
    gateway_grpc_tls: bool = betterproto.bool_field(6)
    workspace_id: str = betterproto.string_field(7)


class GatewayServiceStub(SyncServiceStub):
    def authorize(self, authorize_request: "AuthorizeRequest") -> "AuthorizeResponse":
        return self._unary_unary(
            "/gateway.GatewayService/Authorize",
            AuthorizeRequest,
            AuthorizeResponse,
        )(authorize_request)

    def sign_payload(
        self, sign_payload_request: "SignPayloadRequest"
    ) -> "SignPayloadResponse":
        return self._unary_unary(
            "/gateway.GatewayService/SignPayload",
            SignPayloadRequest,
            SignPayloadResponse,
        )(sign_payload_request)

    def head_object(
        self, head_object_request: "HeadObjectRequest"
    ) -> "HeadObjectResponse":
        return self._unary_unary(
            "/gateway.GatewayService/HeadObject",
            HeadObjectRequest,
            HeadObjectResponse,
        )(head_object_request)

    def create_object(
        self, create_object_request: "CreateObjectRequest"
    ) -> "CreateObjectResponse":
        return self._unary_unary(
            "/gateway.GatewayService/CreateObject",
            CreateObjectRequest,
            CreateObjectResponse,
        )(create_object_request)

    def put_object_stream(
        self, put_object_request_iterator: Iterable["PutObjectRequest"]
    ) -> "PutObjectResponse":
        return (
            self._stream_unary(
                "/gateway.GatewayService/PutObjectStream",
                PutObjectRequest,
                PutObjectResponse,
            )
            .future(put_object_request_iterator)
            .result()
        )

    def list_containers(
        self, list_containers_request: "ListContainersRequest"
    ) -> "ListContainersResponse":
        return self._unary_unary(
            "/gateway.GatewayService/ListContainers",
            ListContainersRequest,
            ListContainersResponse,
        )(list_containers_request)

    def stop_container(
        self, stop_container_request: "StopContainerRequest"
    ) -> "StopContainerResponse":
        return self._unary_unary(
            "/gateway.GatewayService/StopContainer",
            StopContainerRequest,
            StopContainerResponse,
        )(stop_container_request)

    def attach_to_container(
        self, container_stream_message_iterator: Iterable["ContainerStreamMessage"]
    ) -> Iterator["AttachToContainerResponse"]:
        for response in self._stream_stream(
            "/gateway.GatewayService/AttachToContainer",
            ContainerStreamMessage,
            AttachToContainerResponse,
        )(container_stream_message_iterator):
            yield response

    def start_task(self, start_task_request: "StartTaskRequest") -> "StartTaskResponse":
        return self._unary_unary(
            "/gateway.GatewayService/StartTask",
            StartTaskRequest,
            StartTaskResponse,
        )(start_task_request)

    def end_task(self, end_task_request: "EndTaskRequest") -> "EndTaskResponse":
        return self._unary_unary(
            "/gateway.GatewayService/EndTask",
            EndTaskRequest,
            EndTaskResponse,
        )(end_task_request)

    def stop_tasks(self, stop_tasks_request: "StopTasksRequest") -> "StopTasksResponse":
        return self._unary_unary(
            "/gateway.GatewayService/StopTasks",
            StopTasksRequest,
            StopTasksResponse,
        )(stop_tasks_request)

    def list_tasks(self, list_tasks_request: "ListTasksRequest") -> "ListTasksResponse":
        return self._unary_unary(
            "/gateway.GatewayService/ListTasks",
            ListTasksRequest,
            ListTasksResponse,
        )(list_tasks_request)

    def get_or_create_stub(
        self, get_or_create_stub_request: "GetOrCreateStubRequest"
    ) -> "GetOrCreateStubResponse":
        return self._unary_unary(
            "/gateway.GatewayService/GetOrCreateStub",
            GetOrCreateStubRequest,
            GetOrCreateStubResponse,
        )(get_or_create_stub_request)

    def deploy_stub(
        self, deploy_stub_request: "DeployStubRequest"
    ) -> "DeployStubResponse":
        return self._unary_unary(
            "/gateway.GatewayService/DeployStub",
            DeployStubRequest,
            DeployStubResponse,
        )(deploy_stub_request)

    def get_url(self, get_url_request: "GetUrlRequest") -> "GetUrlResponse":
        return self._unary_unary(
            "/gateway.GatewayService/GetURL",
            GetUrlRequest,
            GetUrlResponse,
        )(get_url_request)

    def list_deployments(
        self, list_deployments_request: "ListDeploymentsRequest"
    ) -> "ListDeploymentsResponse":
        return self._unary_unary(
            "/gateway.GatewayService/ListDeployments",
            ListDeploymentsRequest,
            ListDeploymentsResponse,
        )(list_deployments_request)

    def stop_deployment(
        self, stop_deployment_request: "StopDeploymentRequest"
    ) -> "StopDeploymentResponse":
        return self._unary_unary(
            "/gateway.GatewayService/StopDeployment",
            StopDeploymentRequest,
            StopDeploymentResponse,
        )(stop_deployment_request)

    def start_deployment(
        self, start_deployment_request: "StartDeploymentRequest"
    ) -> "StartDeploymentResponse":
        return self._unary_unary(
            "/gateway.GatewayService/StartDeployment",
            StartDeploymentRequest,
            StartDeploymentResponse,
        )(start_deployment_request)

    def scale_deployment(
        self, scale_deployment_request: "ScaleDeploymentRequest"
    ) -> "ScaleDeploymentResponse":
        return self._unary_unary(
            "/gateway.GatewayService/ScaleDeployment",
            ScaleDeploymentRequest,
            ScaleDeploymentResponse,
        )(scale_deployment_request)

    def delete_deployment(
        self, delete_deployment_request: "DeleteDeploymentRequest"
    ) -> "DeleteDeploymentResponse":
        return self._unary_unary(
            "/gateway.GatewayService/DeleteDeployment",
            DeleteDeploymentRequest,
            DeleteDeploymentResponse,
        )(delete_deployment_request)

    def list_pools(self, list_pools_request: "ListPoolsRequest") -> "ListPoolsResponse":
        return self._unary_unary(
            "/gateway.GatewayService/ListPools",
            ListPoolsRequest,
            ListPoolsResponse,
        )(list_pools_request)

    def list_machines(
        self, list_machines_request: "ListMachinesRequest"
    ) -> "ListMachinesResponse":
        return self._unary_unary(
            "/gateway.GatewayService/ListMachines",
            ListMachinesRequest,
            ListMachinesResponse,
        )(list_machines_request)

    def create_machine(
        self, create_machine_request: "CreateMachineRequest"
    ) -> "CreateMachineResponse":
        return self._unary_unary(
            "/gateway.GatewayService/CreateMachine",
            CreateMachineRequest,
            CreateMachineResponse,
        )(create_machine_request)

    def delete_machine(
        self, delete_machine_request: "DeleteMachineRequest"
    ) -> "DeleteMachineResponse":
        return self._unary_unary(
            "/gateway.GatewayService/DeleteMachine",
            DeleteMachineRequest,
            DeleteMachineResponse,
        )(delete_machine_request)

    def list_tokens(
        self, list_tokens_request: "ListTokensRequest"
    ) -> "ListTokensResponse":
        return self._unary_unary(
            "/gateway.GatewayService/ListTokens",
            ListTokensRequest,
            ListTokensResponse,
        )(list_tokens_request)

    def create_token(
        self, create_token_request: "CreateTokenRequest"
    ) -> "CreateTokenResponse":
        return self._unary_unary(
            "/gateway.GatewayService/CreateToken",
            CreateTokenRequest,
            CreateTokenResponse,
        )(create_token_request)

    def toggle_token(
        self, toggle_token_request: "ToggleTokenRequest"
    ) -> "ToggleTokenResponse":
        return self._unary_unary(
            "/gateway.GatewayService/ToggleToken",
            ToggleTokenRequest,
            ToggleTokenResponse,
        )(toggle_token_request)

    def delete_token(
        self, delete_token_request: "DeleteTokenRequest"
    ) -> "DeleteTokenResponse":
        return self._unary_unary(
            "/gateway.GatewayService/DeleteToken",
            DeleteTokenRequest,
            DeleteTokenResponse,
        )(delete_token_request)

    def list_workers(
        self, list_workers_request: "ListWorkersRequest"
    ) -> "ListWorkersResponse":
        return self._unary_unary(
            "/gateway.GatewayService/ListWorkers",
            ListWorkersRequest,
            ListWorkersResponse,
        )(list_workers_request)

    def cordon_worker(
        self, cordon_worker_request: "CordonWorkerRequest"
    ) -> "CordonWorkerResponse":
        return self._unary_unary(
            "/gateway.GatewayService/CordonWorker",
            CordonWorkerRequest,
            CordonWorkerResponse,
        )(cordon_worker_request)

    def uncordon_worker(
        self, uncordon_worker_request: "UncordonWorkerRequest"
    ) -> "UncordonWorkerResponse":
        return self._unary_unary(
            "/gateway.GatewayService/UncordonWorker",
            UncordonWorkerRequest,
            UncordonWorkerResponse,
        )(uncordon_worker_request)

    def drain_worker(
        self, drain_worker_request: "DrainWorkerRequest"
    ) -> "DrainWorkerResponse":
        return self._unary_unary(
            "/gateway.GatewayService/DrainWorker",
            DrainWorkerRequest,
            DrainWorkerResponse,
        )(drain_worker_request)

    def export_workspace_config(
        self, export_workspace_config_request: "ExportWorkspaceConfigRequest"
    ) -> "ExportWorkspaceConfigResponse":
        return self._unary_unary(
            "/gateway.GatewayService/ExportWorkspaceConfig",
            ExportWorkspaceConfigRequest,
            ExportWorkspaceConfigResponse,
        )(export_workspace_config_request)

================================================================================
# File: clients/image/__init__.py
# Path: src/beta9/clients/image/__init__.py
================================================================================

# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: image.proto
# plugin: python-betterproto
# This file has been @generated

from dataclasses import dataclass
from typing import (
    TYPE_CHECKING,
    AsyncIterator,
    Dict,
    Iterator,
    List,
    Optional,
)

import betterproto
import grpc
from betterproto.grpcstub.grpcio_client import SyncServiceStub
from betterproto.grpcstub.grpclib_server import ServiceBase


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpcstub.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


@dataclass(eq=False, repr=False)
class BuildStep(betterproto.Message):
    type: str = betterproto.string_field(1)
    command: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class VerifyImageBuildRequest(betterproto.Message):
    python_version: str = betterproto.string_field(1)
    python_packages: List[str] = betterproto.string_field(2)
    commands: List[str] = betterproto.string_field(3)
    force_rebuild: bool = betterproto.bool_field(4)
    existing_image_uri: str = betterproto.string_field(5)
    build_steps: List["BuildStep"] = betterproto.message_field(6)
    env_vars: List[str] = betterproto.string_field(7)
    dockerfile: str = betterproto.string_field(8)
    build_ctx_object: str = betterproto.string_field(9)
    secrets: List[str] = betterproto.string_field(10)
    gpu: str = betterproto.string_field(11)
    ignore_python: bool = betterproto.bool_field(12)


@dataclass(eq=False, repr=False)
class VerifyImageBuildResponse(betterproto.Message):
    image_id: str = betterproto.string_field(1)
    valid: bool = betterproto.bool_field(2)
    exists: bool = betterproto.bool_field(3)


@dataclass(eq=False, repr=False)
class BuildImageRequest(betterproto.Message):
    python_version: str = betterproto.string_field(1)
    """These parameters are used for a "beta9" managed image"""

    python_packages: List[str] = betterproto.string_field(2)
    commands: List[str] = betterproto.string_field(3)
    existing_image_uri: str = betterproto.string_field(4)
    """These parameters are used for an existing image"""

    existing_image_creds: Dict[str, str] = betterproto.map_field(
        5, betterproto.TYPE_STRING, betterproto.TYPE_STRING
    )
    build_steps: List["BuildStep"] = betterproto.message_field(6)
    env_vars: List[str] = betterproto.string_field(7)
    dockerfile: str = betterproto.string_field(8)
    build_ctx_object: str = betterproto.string_field(9)
    secrets: List[str] = betterproto.string_field(10)
    gpu: str = betterproto.string_field(11)
    ignore_python: bool = betterproto.bool_field(12)


@dataclass(eq=False, repr=False)
class BuildImageResponse(betterproto.Message):
    image_id: str = betterproto.string_field(1)
    msg: str = betterproto.string_field(2)
    done: bool = betterproto.bool_field(3)
    success: bool = betterproto.bool_field(4)
    python_version: str = betterproto.string_field(5)
    warning: bool = betterproto.bool_field(6)


class ImageServiceStub(SyncServiceStub):
    def verify_image_build(
        self, verify_image_build_request: "VerifyImageBuildRequest"
    ) -> "VerifyImageBuildResponse":
        return self._unary_unary(
            "/image.ImageService/VerifyImageBuild",
            VerifyImageBuildRequest,
            VerifyImageBuildResponse,
        )(verify_image_build_request)

    def build_image(
        self, build_image_request: "BuildImageRequest"
    ) -> Iterator["BuildImageResponse"]:
        for response in self._unary_stream(
            "/image.ImageService/BuildImage",
            BuildImageRequest,
            BuildImageResponse,
        )(build_image_request):
            yield response

================================================================================
# File: clients/map/__init__.py
# Path: src/beta9/clients/map/__init__.py
================================================================================

# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: map.proto
# plugin: python-betterproto
# This file has been @generated

from dataclasses import dataclass
from typing import (
    TYPE_CHECKING,
    Dict,
    List,
    Optional,
)

import betterproto
import grpc
from betterproto.grpcstub.grpcio_client import SyncServiceStub
from betterproto.grpcstub.grpclib_server import ServiceBase


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpcstub.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


@dataclass(eq=False, repr=False)
class MapSetRequest(betterproto.Message):
    name: str = betterproto.string_field(1)
    key: str = betterproto.string_field(2)
    value: bytes = betterproto.bytes_field(3)
    ttl: int = betterproto.int64_field(4)


@dataclass(eq=False, repr=False)
class MapSetResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class MapGetRequest(betterproto.Message):
    name: str = betterproto.string_field(1)
    key: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class MapGetResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    value: bytes = betterproto.bytes_field(2)


@dataclass(eq=False, repr=False)
class MapDeleteRequest(betterproto.Message):
    name: str = betterproto.string_field(1)
    key: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class MapDeleteResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)


@dataclass(eq=False, repr=False)
class MapCountRequest(betterproto.Message):
    name: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class MapCountResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    count: int = betterproto.uint32_field(2)


@dataclass(eq=False, repr=False)
class MapKeysRequest(betterproto.Message):
    name: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class MapKeysResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    keys: List[str] = betterproto.string_field(2)


class MapServiceStub(SyncServiceStub):
    def map_set(self, map_set_request: "MapSetRequest") -> "MapSetResponse":
        return self._unary_unary(
            "/map.MapService/MapSet",
            MapSetRequest,
            MapSetResponse,
        )(map_set_request)

    def map_get(self, map_get_request: "MapGetRequest") -> "MapGetResponse":
        return self._unary_unary(
            "/map.MapService/MapGet",
            MapGetRequest,
            MapGetResponse,
        )(map_get_request)

    def map_delete(self, map_delete_request: "MapDeleteRequest") -> "MapDeleteResponse":
        return self._unary_unary(
            "/map.MapService/MapDelete",
            MapDeleteRequest,
            MapDeleteResponse,
        )(map_delete_request)

    def map_count(self, map_count_request: "MapCountRequest") -> "MapCountResponse":
        return self._unary_unary(
            "/map.MapService/MapCount",
            MapCountRequest,
            MapCountResponse,
        )(map_count_request)

    def map_keys(self, map_keys_request: "MapKeysRequest") -> "MapKeysResponse":
        return self._unary_unary(
            "/map.MapService/MapKeys",
            MapKeysRequest,
            MapKeysResponse,
        )(map_keys_request)

================================================================================
# File: clients/output/__init__.py
# Path: src/beta9/clients/output/__init__.py
================================================================================

# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: output.proto
# plugin: python-betterproto
# This file has been @generated

from dataclasses import dataclass
from datetime import datetime
from typing import (
    TYPE_CHECKING,
    AsyncIterable,
    AsyncIterator,
    Dict,
    Iterable,
    Iterator,
    Optional,
    Union,
)

import betterproto
import grpc
from betterproto.grpcstub.grpcio_client import SyncServiceStub
from betterproto.grpcstub.grpclib_server import ServiceBase


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpcstub.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


@dataclass(eq=False, repr=False)
class OutputSaveRequest(betterproto.Message):
    task_id: str = betterproto.string_field(1)
    filename: str = betterproto.string_field(2)
    content: bytes = betterproto.bytes_field(3)


@dataclass(eq=False, repr=False)
class OutputSaveResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    id: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class OutputStatRequest(betterproto.Message):
    id: str = betterproto.string_field(1)
    task_id: str = betterproto.string_field(2)
    filename: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class OutputStat(betterproto.Message):
    mode: str = betterproto.string_field(1)
    size: int = betterproto.int64_field(2)
    atime: datetime = betterproto.message_field(3)
    mtime: datetime = betterproto.message_field(4)


@dataclass(eq=False, repr=False)
class OutputStatResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    stat: "OutputStat" = betterproto.message_field(3)


@dataclass(eq=False, repr=False)
class OutputPublicUrlRequest(betterproto.Message):
    id: str = betterproto.string_field(1)
    task_id: str = betterproto.string_field(2)
    filename: str = betterproto.string_field(3)
    expires: int = betterproto.uint32_field(4)


@dataclass(eq=False, repr=False)
class OutputPublicUrlResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    public_url: str = betterproto.string_field(3)


class OutputServiceStub(SyncServiceStub):
    def output_save_stream(
        self, output_save_request_iterator: Iterable["OutputSaveRequest"]
    ) -> "OutputSaveResponse":
        return (
            self._stream_unary(
                "/output.OutputService/OutputSaveStream",
                OutputSaveRequest,
                OutputSaveResponse,
            )
            .future(output_save_request_iterator)
            .result()
        )

    def output_stat(
        self, output_stat_request: "OutputStatRequest"
    ) -> "OutputStatResponse":
        return self._unary_unary(
            "/output.OutputService/OutputStat",
            OutputStatRequest,
            OutputStatResponse,
        )(output_stat_request)

    def output_public_url(
        self, output_public_url_request: "OutputPublicUrlRequest"
    ) -> "OutputPublicUrlResponse":
        return self._unary_unary(
            "/output.OutputService/OutputPublicURL",
            OutputPublicUrlRequest,
            OutputPublicUrlResponse,
        )(output_public_url_request)

================================================================================
# File: clients/pod/__init__.py
# Path: src/beta9/clients/pod/__init__.py
================================================================================

# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: pod.proto
# plugin: python-betterproto
# This file has been @generated

from dataclasses import dataclass
from typing import (
    TYPE_CHECKING,
    Dict,
    List,
    Optional,
)

import betterproto
import grpc
from betterproto.grpcstub.grpcio_client import SyncServiceStub
from betterproto.grpcstub.grpclib_server import ServiceBase

from .. import types as _types__


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpcstub.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


@dataclass(eq=False, repr=False)
class CreatePodRequest(betterproto.Message):
    stub_id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class CreatePodResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    container_id: str = betterproto.string_field(2)
    error_msg: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class PodSandboxExecRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    command: str = betterproto.string_field(2)
    cwd: str = betterproto.string_field(3)
    env: Dict[str, str] = betterproto.map_field(
        4, betterproto.TYPE_STRING, betterproto.TYPE_STRING
    )


@dataclass(eq=False, repr=False)
class PodSandboxExecResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    error_msg: str = betterproto.string_field(2)
    pid: int = betterproto.int32_field(3)


@dataclass(eq=False, repr=False)
class PodSandboxStatusRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    pid: int = betterproto.int32_field(2)


@dataclass(eq=False, repr=False)
class PodSandboxStatusResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    error_msg: str = betterproto.string_field(2)
    status: str = betterproto.string_field(3)
    exit_code: int = betterproto.int32_field(4)


@dataclass(eq=False, repr=False)
class PodSandboxStdoutRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    pid: int = betterproto.int32_field(2)


@dataclass(eq=False, repr=False)
class PodSandboxStdoutResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    error_msg: str = betterproto.string_field(2)
    stdout: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class PodSandboxStderrRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    pid: int = betterproto.int32_field(2)


@dataclass(eq=False, repr=False)
class PodSandboxStderrResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    error_msg: str = betterproto.string_field(2)
    stderr: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class PodSandboxKillRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    pid: int = betterproto.int32_field(2)


@dataclass(eq=False, repr=False)
class PodSandboxKillResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    error_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class PodSandboxListProcessesRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class PodSandboxListProcessesResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    error_msg: str = betterproto.string_field(2)
    pids: List[int] = betterproto.int32_field(3)


@dataclass(eq=False, repr=False)
class PodSandboxUploadFileRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    container_path: str = betterproto.string_field(2)
    mode: int = betterproto.int32_field(3)
    data: bytes = betterproto.bytes_field(4)


@dataclass(eq=False, repr=False)
class PodSandboxUploadFileResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    error_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class PodSandboxDownloadFileRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    container_path: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class PodSandboxDownloadFileResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    error_msg: str = betterproto.string_field(2)
    data: bytes = betterproto.bytes_field(3)


@dataclass(eq=False, repr=False)
class PodSandboxListFilesRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    container_path: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class PodSandboxListFilesResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    error_msg: str = betterproto.string_field(2)
    files: List["PodSandboxFileInfo"] = betterproto.message_field(3)


@dataclass(eq=False, repr=False)
class PodSandboxDeleteFileRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    container_path: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class PodSandboxDeleteFileResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    error_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class PodSandboxCreateDirectoryRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    container_path: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class PodSandboxCreateDirectoryResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    error_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class PodSandboxDeleteDirectoryRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    container_path: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class PodSandboxDeleteDirectoryResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    error_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class PodSandboxStatFileRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    container_path: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class PodSandboxStatFileResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    error_msg: str = betterproto.string_field(2)
    file_info: "PodSandboxFileInfo" = betterproto.message_field(3)


@dataclass(eq=False, repr=False)
class PodSandboxFileInfo(betterproto.Message):
    mode: int = betterproto.int32_field(1)
    size: int = betterproto.int64_field(2)
    mod_time: int = betterproto.int64_field(3)
    owner: str = betterproto.string_field(4)
    group: str = betterproto.string_field(5)
    is_dir: bool = betterproto.bool_field(6)
    name: str = betterproto.string_field(7)
    permissions: int = betterproto.uint32_field(8)


@dataclass(eq=False, repr=False)
class PodSandboxReplaceInFilesRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    container_path: str = betterproto.string_field(2)
    pattern: str = betterproto.string_field(3)
    new_string: str = betterproto.string_field(4)


@dataclass(eq=False, repr=False)
class PodSandboxReplaceInFilesResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    error_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class PodSandboxExposePortRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    stub_id: str = betterproto.string_field(2)
    port: int = betterproto.int32_field(3)


@dataclass(eq=False, repr=False)
class PodSandboxExposePortResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    url: str = betterproto.string_field(2)
    error_msg: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class PodSandboxFindFilesRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    container_path: str = betterproto.string_field(2)
    pattern: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class PodSandboxFindFilesResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    error_msg: str = betterproto.string_field(2)
    results: List["_types__.FileSearchResult"] = betterproto.message_field(3)


class PodServiceStub(SyncServiceStub):
    def create_pod(self, create_pod_request: "CreatePodRequest") -> "CreatePodResponse":
        return self._unary_unary(
            "/pod.PodService/CreatePod",
            CreatePodRequest,
            CreatePodResponse,
        )(create_pod_request)

    def sandbox_exec(
        self, pod_sandbox_exec_request: "PodSandboxExecRequest"
    ) -> "PodSandboxExecResponse":
        return self._unary_unary(
            "/pod.PodService/SandboxExec",
            PodSandboxExecRequest,
            PodSandboxExecResponse,
        )(pod_sandbox_exec_request)

    def sandbox_status(
        self, pod_sandbox_status_request: "PodSandboxStatusRequest"
    ) -> "PodSandboxStatusResponse":
        return self._unary_unary(
            "/pod.PodService/SandboxStatus",
            PodSandboxStatusRequest,
            PodSandboxStatusResponse,
        )(pod_sandbox_status_request)

    def sandbox_stdout(
        self, pod_sandbox_stdout_request: "PodSandboxStdoutRequest"
    ) -> "PodSandboxStdoutResponse":
        return self._unary_unary(
            "/pod.PodService/SandboxStdout",
            PodSandboxStdoutRequest,
            PodSandboxStdoutResponse,
        )(pod_sandbox_stdout_request)

    def sandbox_stderr(
        self, pod_sandbox_stderr_request: "PodSandboxStderrRequest"
    ) -> "PodSandboxStderrResponse":
        return self._unary_unary(
            "/pod.PodService/SandboxStderr",
            PodSandboxStderrRequest,
            PodSandboxStderrResponse,
        )(pod_sandbox_stderr_request)

    def sandbox_kill(
        self, pod_sandbox_kill_request: "PodSandboxKillRequest"
    ) -> "PodSandboxKillResponse":
        return self._unary_unary(
            "/pod.PodService/SandboxKill",
            PodSandboxKillRequest,
            PodSandboxKillResponse,
        )(pod_sandbox_kill_request)

    def sandbox_list_processes(
        self, pod_sandbox_list_processes_request: "PodSandboxListProcessesRequest"
    ) -> "PodSandboxListProcessesResponse":
        return self._unary_unary(
            "/pod.PodService/SandboxListProcesses",
            PodSandboxListProcessesRequest,
            PodSandboxListProcessesResponse,
        )(pod_sandbox_list_processes_request)

    def sandbox_upload_file(
        self, pod_sandbox_upload_file_request: "PodSandboxUploadFileRequest"
    ) -> "PodSandboxUploadFileResponse":
        return self._unary_unary(
            "/pod.PodService/SandboxUploadFile",
            PodSandboxUploadFileRequest,
            PodSandboxUploadFileResponse,
        )(pod_sandbox_upload_file_request)

    def sandbox_download_file(
        self, pod_sandbox_download_file_request: "PodSandboxDownloadFileRequest"
    ) -> "PodSandboxDownloadFileResponse":
        return self._unary_unary(
            "/pod.PodService/SandboxDownloadFile",
            PodSandboxDownloadFileRequest,
            PodSandboxDownloadFileResponse,
        )(pod_sandbox_download_file_request)

    def sandbox_stat_file(
        self, pod_sandbox_stat_file_request: "PodSandboxStatFileRequest"
    ) -> "PodSandboxStatFileResponse":
        return self._unary_unary(
            "/pod.PodService/SandboxStatFile",
            PodSandboxStatFileRequest,
            PodSandboxStatFileResponse,
        )(pod_sandbox_stat_file_request)

    def sandbox_list_files(
        self, pod_sandbox_list_files_request: "PodSandboxListFilesRequest"
    ) -> "PodSandboxListFilesResponse":
        return self._unary_unary(
            "/pod.PodService/SandboxListFiles",
            PodSandboxListFilesRequest,
            PodSandboxListFilesResponse,
        )(pod_sandbox_list_files_request)

    def sandbox_delete_file(
        self, pod_sandbox_delete_file_request: "PodSandboxDeleteFileRequest"
    ) -> "PodSandboxDeleteFileResponse":
        return self._unary_unary(
            "/pod.PodService/SandboxDeleteFile",
            PodSandboxDeleteFileRequest,
            PodSandboxDeleteFileResponse,
        )(pod_sandbox_delete_file_request)

    def sandbox_create_directory(
        self, pod_sandbox_create_directory_request: "PodSandboxCreateDirectoryRequest"
    ) -> "PodSandboxCreateDirectoryResponse":
        return self._unary_unary(
            "/pod.PodService/SandboxCreateDirectory",
            PodSandboxCreateDirectoryRequest,
            PodSandboxCreateDirectoryResponse,
        )(pod_sandbox_create_directory_request)

    def sandbox_delete_directory(
        self, pod_sandbox_delete_directory_request: "PodSandboxDeleteDirectoryRequest"
    ) -> "PodSandboxDeleteDirectoryResponse":
        return self._unary_unary(
            "/pod.PodService/SandboxDeleteDirectory",
            PodSandboxDeleteDirectoryRequest,
            PodSandboxDeleteDirectoryResponse,
        )(pod_sandbox_delete_directory_request)

    def sandbox_expose_port(
        self, pod_sandbox_expose_port_request: "PodSandboxExposePortRequest"
    ) -> "PodSandboxExposePortResponse":
        return self._unary_unary(
            "/pod.PodService/SandboxExposePort",
            PodSandboxExposePortRequest,
            PodSandboxExposePortResponse,
        )(pod_sandbox_expose_port_request)

    def sandbox_replace_in_files(
        self, pod_sandbox_replace_in_files_request: "PodSandboxReplaceInFilesRequest"
    ) -> "PodSandboxReplaceInFilesResponse":
        return self._unary_unary(
            "/pod.PodService/SandboxReplaceInFiles",
            PodSandboxReplaceInFilesRequest,
            PodSandboxReplaceInFilesResponse,
        )(pod_sandbox_replace_in_files_request)

    def sandbox_find_files(
        self, pod_sandbox_find_files_request: "PodSandboxFindFilesRequest"
    ) -> "PodSandboxFindFilesResponse":
        return self._unary_unary(
            "/pod.PodService/SandboxFindFiles",
            PodSandboxFindFilesRequest,
            PodSandboxFindFilesResponse,
        )(pod_sandbox_find_files_request)

================================================================================
# File: clients/secret/__init__.py
# Path: src/beta9/clients/secret/__init__.py
================================================================================

# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: secret.proto
# plugin: python-betterproto
# This file has been @generated

from dataclasses import dataclass
from datetime import datetime
from typing import (
    TYPE_CHECKING,
    Dict,
    List,
    Optional,
)

import betterproto
import grpc
from betterproto.grpcstub.grpcio_client import SyncServiceStub
from betterproto.grpcstub.grpclib_server import ServiceBase


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpcstub.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


@dataclass(eq=False, repr=False)
class Secret(betterproto.Message):
    id: str = betterproto.string_field(1)
    name: str = betterproto.string_field(2)
    value: str = betterproto.string_field(3)
    last_updated_by: str = betterproto.string_field(5)
    created_at: datetime = betterproto.message_field(6)
    updated_at: datetime = betterproto.message_field(7)


@dataclass(eq=False, repr=False)
class CreateSecretRequest(betterproto.Message):
    name: str = betterproto.string_field(2)
    value: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class CreateSecretResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    id: str = betterproto.string_field(3)
    name: str = betterproto.string_field(4)


@dataclass(eq=False, repr=False)
class DeleteSecretRequest(betterproto.Message):
    name: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class DeleteSecretResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class UpdateSecretRequest(betterproto.Message):
    name: str = betterproto.string_field(2)
    value: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class UpdateSecretResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class GetSecretRequest(betterproto.Message):
    name: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class GetSecretResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    secret: "Secret" = betterproto.message_field(3)


@dataclass(eq=False, repr=False)
class ListSecretsRequest(betterproto.Message):
    pass


@dataclass(eq=False, repr=False)
class ListSecretsResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    secrets: List["Secret"] = betterproto.message_field(3)


class SecretServiceStub(SyncServiceStub):
    def create_secret(
        self, create_secret_request: "CreateSecretRequest"
    ) -> "CreateSecretResponse":
        return self._unary_unary(
            "/secret.SecretService/CreateSecret",
            CreateSecretRequest,
            CreateSecretResponse,
        )(create_secret_request)

    def delete_secret(
        self, delete_secret_request: "DeleteSecretRequest"
    ) -> "DeleteSecretResponse":
        return self._unary_unary(
            "/secret.SecretService/DeleteSecret",
            DeleteSecretRequest,
            DeleteSecretResponse,
        )(delete_secret_request)

    def update_secret(
        self, update_secret_request: "UpdateSecretRequest"
    ) -> "UpdateSecretResponse":
        return self._unary_unary(
            "/secret.SecretService/UpdateSecret",
            UpdateSecretRequest,
            UpdateSecretResponse,
        )(update_secret_request)

    def get_secret(self, get_secret_request: "GetSecretRequest") -> "GetSecretResponse":
        return self._unary_unary(
            "/secret.SecretService/GetSecret",
            GetSecretRequest,
            GetSecretResponse,
        )(get_secret_request)

    def list_secrets(
        self, list_secrets_request: "ListSecretsRequest"
    ) -> "ListSecretsResponse":
        return self._unary_unary(
            "/secret.SecretService/ListSecrets",
            ListSecretsRequest,
            ListSecretsResponse,
        )(list_secrets_request)

================================================================================
# File: clients/shell/__init__.py
# Path: src/beta9/clients/shell/__init__.py
================================================================================

# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: shell.proto
# plugin: python-betterproto
# This file has been @generated

from dataclasses import dataclass
from typing import (
    TYPE_CHECKING,
    Dict,
    Optional,
)

import betterproto
import grpc
from betterproto.grpcstub.grpcio_client import SyncServiceStub
from betterproto.grpcstub.grpclib_server import ServiceBase


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpcstub.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


@dataclass(eq=False, repr=False)
class CreateStandaloneShellRequest(betterproto.Message):
    stub_id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class CreateStandaloneShellResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    container_id: str = betterproto.string_field(2)
    username: str = betterproto.string_field(3)
    password: str = betterproto.string_field(4)
    err_msg: str = betterproto.string_field(5)


@dataclass(eq=False, repr=False)
class CreateShellInExistingContainerRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class CreateShellInExistingContainerResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    username: str = betterproto.string_field(2)
    password: str = betterproto.string_field(3)
    stub_id: str = betterproto.string_field(4)
    err_msg: str = betterproto.string_field(5)


class ShellServiceStub(SyncServiceStub):
    def create_standalone_shell(
        self, create_standalone_shell_request: "CreateStandaloneShellRequest"
    ) -> "CreateStandaloneShellResponse":
        return self._unary_unary(
            "/shell.ShellService/CreateStandaloneShell",
            CreateStandaloneShellRequest,
            CreateStandaloneShellResponse,
        )(create_standalone_shell_request)

    def create_shell_in_existing_container(
        self,
        create_shell_in_existing_container_request: "CreateShellInExistingContainerRequest",
    ) -> "CreateShellInExistingContainerResponse":
        return self._unary_unary(
            "/shell.ShellService/CreateShellInExistingContainer",
            CreateShellInExistingContainerRequest,
            CreateShellInExistingContainerResponse,
        )(create_shell_in_existing_container_request)

================================================================================
# File: clients/signal/__init__.py
# Path: src/beta9/clients/signal/__init__.py
================================================================================

# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: signal.proto
# plugin: python-betterproto
# This file has been @generated

from dataclasses import dataclass
from typing import (
    TYPE_CHECKING,
    AsyncIterator,
    Dict,
    Iterator,
    Optional,
)

import betterproto
import grpc
from betterproto.grpcstub.grpcio_client import SyncServiceStub
from betterproto.grpcstub.grpclib_server import ServiceBase


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpcstub.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


@dataclass(eq=False, repr=False)
class SignalSetRequest(betterproto.Message):
    name: str = betterproto.string_field(1)
    ttl: int = betterproto.int64_field(2)


@dataclass(eq=False, repr=False)
class SignalSetResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)


@dataclass(eq=False, repr=False)
class SignalClearRequest(betterproto.Message):
    name: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class SignalClearResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)


@dataclass(eq=False, repr=False)
class SignalMonitorRequest(betterproto.Message):
    name: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class SignalMonitorResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    set: bool = betterproto.bool_field(2)


class SignalServiceStub(SyncServiceStub):
    def signal_set(self, signal_set_request: "SignalSetRequest") -> "SignalSetResponse":
        return self._unary_unary(
            "/signal.SignalService/SignalSet",
            SignalSetRequest,
            SignalSetResponse,
        )(signal_set_request)

    def signal_clear(
        self, signal_clear_request: "SignalClearRequest"
    ) -> "SignalClearResponse":
        return self._unary_unary(
            "/signal.SignalService/SignalClear",
            SignalClearRequest,
            SignalClearResponse,
        )(signal_clear_request)

    def signal_monitor(
        self, signal_monitor_request: "SignalMonitorRequest"
    ) -> Iterator["SignalMonitorResponse"]:
        for response in self._unary_stream(
            "/signal.SignalService/SignalMonitor",
            SignalMonitorRequest,
            SignalMonitorResponse,
        )(signal_monitor_request):
            yield response

================================================================================
# File: clients/simplequeue/__init__.py
# Path: src/beta9/clients/simplequeue/__init__.py
================================================================================

# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: queue.proto
# plugin: python-betterproto
# This file has been @generated

from dataclasses import dataclass
from typing import (
    TYPE_CHECKING,
    Dict,
    Optional,
)

import betterproto
import grpc
from betterproto.grpcstub.grpcio_client import SyncServiceStub
from betterproto.grpcstub.grpclib_server import ServiceBase


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpcstub.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


@dataclass(eq=False, repr=False)
class SimpleQueuePutRequest(betterproto.Message):
    name: str = betterproto.string_field(1)
    value: bytes = betterproto.bytes_field(2)


@dataclass(eq=False, repr=False)
class SimpleQueuePutResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)


@dataclass(eq=False, repr=False)
class SimpleQueuePopRequest(betterproto.Message):
    name: str = betterproto.string_field(1)
    value: bytes = betterproto.bytes_field(2)


@dataclass(eq=False, repr=False)
class SimpleQueuePopResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    value: bytes = betterproto.bytes_field(2)


@dataclass(eq=False, repr=False)
class SimpleQueuePeekResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    value: bytes = betterproto.bytes_field(2)


@dataclass(eq=False, repr=False)
class SimpleQueueEmptyResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    empty: bool = betterproto.bool_field(2)


@dataclass(eq=False, repr=False)
class SimpleQueueSizeResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    size: int = betterproto.uint64_field(2)


@dataclass(eq=False, repr=False)
class SimpleQueueRequest(betterproto.Message):
    name: str = betterproto.string_field(1)


class SimpleQueueServiceStub(SyncServiceStub):
    def simple_queue_put(
        self, simple_queue_put_request: "SimpleQueuePutRequest"
    ) -> "SimpleQueuePutResponse":
        return self._unary_unary(
            "/simplequeue.SimpleQueueService/SimpleQueuePut",
            SimpleQueuePutRequest,
            SimpleQueuePutResponse,
        )(simple_queue_put_request)

    def simple_queue_pop(
        self, simple_queue_pop_request: "SimpleQueuePopRequest"
    ) -> "SimpleQueuePopResponse":
        return self._unary_unary(
            "/simplequeue.SimpleQueueService/SimpleQueuePop",
            SimpleQueuePopRequest,
            SimpleQueuePopResponse,
        )(simple_queue_pop_request)

    def simple_queue_peek(
        self, simple_queue_request: "SimpleQueueRequest"
    ) -> "SimpleQueuePeekResponse":
        return self._unary_unary(
            "/simplequeue.SimpleQueueService/SimpleQueuePeek",
            SimpleQueueRequest,
            SimpleQueuePeekResponse,
        )(simple_queue_request)

    def simple_queue_empty(
        self, simple_queue_request: "SimpleQueueRequest"
    ) -> "SimpleQueueEmptyResponse":
        return self._unary_unary(
            "/simplequeue.SimpleQueueService/SimpleQueueEmpty",
            SimpleQueueRequest,
            SimpleQueueEmptyResponse,
        )(simple_queue_request)

    def simple_queue_size(
        self, simple_queue_request: "SimpleQueueRequest"
    ) -> "SimpleQueueSizeResponse":
        return self._unary_unary(
            "/simplequeue.SimpleQueueService/SimpleQueueSize",
            SimpleQueueRequest,
            SimpleQueueSizeResponse,
        )(simple_queue_request)

================================================================================
# File: clients/taskqueue/__init__.py
# Path: src/beta9/clients/taskqueue/__init__.py
================================================================================

# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: taskqueue.proto
# plugin: python-betterproto
# This file has been @generated

from dataclasses import dataclass
from typing import (
    TYPE_CHECKING,
    AsyncIterator,
    Dict,
    Iterator,
    Optional,
)

import betterproto
import grpc
from betterproto.grpcstub.grpcio_client import SyncServiceStub
from betterproto.grpcstub.grpclib_server import ServiceBase


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpcstub.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


@dataclass(eq=False, repr=False)
class TaskQueuePutRequest(betterproto.Message):
    stub_id: str = betterproto.string_field(1)
    payload: bytes = betterproto.bytes_field(2)


@dataclass(eq=False, repr=False)
class TaskQueuePutResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    task_id: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class TaskQueuePopRequest(betterproto.Message):
    stub_id: str = betterproto.string_field(1)
    container_id: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class TaskQueuePopResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    task_msg: bytes = betterproto.bytes_field(2)


@dataclass(eq=False, repr=False)
class TaskQueueLengthRequest(betterproto.Message):
    stub_id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class TaskQueueLengthResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    length: int = betterproto.int64_field(2)


@dataclass(eq=False, repr=False)
class TaskQueueCompleteRequest(betterproto.Message):
    task_id: str = betterproto.string_field(1)
    stub_id: str = betterproto.string_field(2)
    task_duration: float = betterproto.float_field(3)
    task_status: str = betterproto.string_field(4)
    container_id: str = betterproto.string_field(5)
    container_hostname: str = betterproto.string_field(6)
    keep_warm_seconds: float = betterproto.float_field(7)
    result: bytes = betterproto.bytes_field(8)


@dataclass(eq=False, repr=False)
class TaskQueueCompleteResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    message: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class TaskQueueMonitorRequest(betterproto.Message):
    task_id: str = betterproto.string_field(1)
    stub_id: str = betterproto.string_field(2)
    container_id: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class TaskQueueMonitorResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    cancelled: bool = betterproto.bool_field(2)
    complete: bool = betterproto.bool_field(3)
    timed_out: bool = betterproto.bool_field(4)


@dataclass(eq=False, repr=False)
class StartTaskQueueServeRequest(betterproto.Message):
    stub_id: str = betterproto.string_field(1)
    timeout: int = betterproto.int32_field(2)


@dataclass(eq=False, repr=False)
class StartTaskQueueServeResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    container_id: str = betterproto.string_field(2)
    error_msg: str = betterproto.string_field(3)


class TaskQueueServiceStub(SyncServiceStub):
    def task_queue_put(
        self, task_queue_put_request: "TaskQueuePutRequest"
    ) -> "TaskQueuePutResponse":
        return self._unary_unary(
            "/taskqueue.TaskQueueService/TaskQueuePut",
            TaskQueuePutRequest,
            TaskQueuePutResponse,
        )(task_queue_put_request)

    def task_queue_pop(
        self, task_queue_pop_request: "TaskQueuePopRequest"
    ) -> "TaskQueuePopResponse":
        return self._unary_unary(
            "/taskqueue.TaskQueueService/TaskQueuePop",
            TaskQueuePopRequest,
            TaskQueuePopResponse,
        )(task_queue_pop_request)

    def task_queue_monitor(
        self, task_queue_monitor_request: "TaskQueueMonitorRequest"
    ) -> Iterator["TaskQueueMonitorResponse"]:
        for response in self._unary_stream(
            "/taskqueue.TaskQueueService/TaskQueueMonitor",
            TaskQueueMonitorRequest,
            TaskQueueMonitorResponse,
        )(task_queue_monitor_request):
            yield response

    def task_queue_complete(
        self, task_queue_complete_request: "TaskQueueCompleteRequest"
    ) -> "TaskQueueCompleteResponse":
        return self._unary_unary(
            "/taskqueue.TaskQueueService/TaskQueueComplete",
            TaskQueueCompleteRequest,
            TaskQueueCompleteResponse,
        )(task_queue_complete_request)

    def task_queue_length(
        self, task_queue_length_request: "TaskQueueLengthRequest"
    ) -> "TaskQueueLengthResponse":
        return self._unary_unary(
            "/taskqueue.TaskQueueService/TaskQueueLength",
            TaskQueueLengthRequest,
            TaskQueueLengthResponse,
        )(task_queue_length_request)

    def start_task_queue_serve(
        self, start_task_queue_serve_request: "StartTaskQueueServeRequest"
    ) -> "StartTaskQueueServeResponse":
        return self._unary_unary(
            "/taskqueue.TaskQueueService/StartTaskQueueServe",
            StartTaskQueueServeRequest,
            StartTaskQueueServeResponse,
        )(start_task_queue_serve_request)

================================================================================
# File: clients/types/__init__.py
# Path: src/beta9/clients/types/__init__.py
================================================================================

# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: types.proto
# plugin: python-betterproto
# This file has been @generated

from dataclasses import dataclass
from datetime import datetime
from typing import List

import betterproto


@dataclass(eq=False, repr=False)
class App(betterproto.Message):
    id: int = betterproto.uint32_field(1)
    external_id: str = betterproto.string_field(2)
    name: str = betterproto.string_field(3)
    description: str = betterproto.string_field(4)
    workspace_id: int = betterproto.uint32_field(5)
    created_at: datetime = betterproto.message_field(6)
    updated_at: datetime = betterproto.message_field(7)
    deleted_at: "NullTime" = betterproto.message_field(8)


@dataclass(eq=False, repr=False)
class BuildOptions(betterproto.Message):
    source_image: str = betterproto.string_field(1)
    dockerfile: str = betterproto.string_field(2)
    build_ctx_object: str = betterproto.string_field(3)
    source_image_creds: str = betterproto.string_field(4)
    build_secrets: List[str] = betterproto.string_field(5)


@dataclass(eq=False, repr=False)
class CheckpointState(betterproto.Message):
    stub_id: str = betterproto.string_field(1)
    container_id: str = betterproto.string_field(2)
    status: str = betterproto.string_field(3)
    remote_key: str = betterproto.string_field(4)


@dataclass(eq=False, repr=False)
class ConcurrencyLimit(betterproto.Message):
    id: int = betterproto.uint32_field(1)
    external_id: str = betterproto.string_field(2)
    gpu_limit: int = betterproto.uint32_field(3)
    cpu_millicore_limit: int = betterproto.uint32_field(4)
    created_at: datetime = betterproto.message_field(5)
    updated_at: datetime = betterproto.message_field(6)


@dataclass(eq=False, repr=False)
class Container(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    stub_id: str = betterproto.string_field(2)
    status: str = betterproto.string_field(3)
    scheduled_at: datetime = betterproto.message_field(4)
    started_at: datetime = betterproto.message_field(5)
    workspace_id: str = betterproto.string_field(6)
    worker_id: str = betterproto.string_field(7)
    machine_id: str = betterproto.string_field(8)
    deployment_id: str = betterproto.string_field(9)


@dataclass(eq=False, repr=False)
class ContainerRequest(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    entry_point: List[str] = betterproto.string_field(2)
    env: List[str] = betterproto.string_field(3)
    cpu: int = betterproto.int64_field(4)
    memory: int = betterproto.int64_field(5)
    gpu: str = betterproto.string_field(6)
    gpu_request: List[str] = betterproto.string_field(7)
    gpu_count: int = betterproto.uint32_field(8)
    image_id: str = betterproto.string_field(9)
    stub_id: str = betterproto.string_field(10)
    workspace_id: str = betterproto.string_field(11)
    workspace: "Workspace" = betterproto.message_field(12)
    stub: "StubWithRelated" = betterproto.message_field(13)
    timestamp: datetime = betterproto.message_field(14)
    mounts: List["Mount"] = betterproto.message_field(15)
    retry_count: int = betterproto.int64_field(16)
    pool_selector: str = betterproto.string_field(17)
    preemptable: bool = betterproto.bool_field(18)
    checkpoint_enabled: bool = betterproto.bool_field(19)
    build_options: "BuildOptions" = betterproto.message_field(20)
    ports: List[int] = betterproto.uint32_field(21)
    cost_per_ms: float = betterproto.double_field(22)
    app_id: str = betterproto.string_field(23)


@dataclass(eq=False, repr=False)
class ContainerState(betterproto.Message):
    container_id: str = betterproto.string_field(1)
    stub_id: str = betterproto.string_field(2)
    status: str = betterproto.string_field(3)
    scheduled_at: int = betterproto.int64_field(4)
    workspace_id: str = betterproto.string_field(5)
    gpu: str = betterproto.string_field(6)
    gpu_count: int = betterproto.uint32_field(7)
    cpu: int = betterproto.int64_field(8)
    memory: int = betterproto.int64_field(9)
    started_at: int = betterproto.int64_field(10)


@dataclass(eq=False, repr=False)
class FileInfo(betterproto.Message):
    name: str = betterproto.string_field(1)
    is_dir: bool = betterproto.bool_field(2)
    size: int = betterproto.int64_field(3)
    mode: int = betterproto.int32_field(4)
    mod_time: int = betterproto.int64_field(5)
    owner: str = betterproto.string_field(6)
    group: str = betterproto.string_field(7)
    path: str = betterproto.string_field(8)
    permissions: int = betterproto.uint32_field(9)


@dataclass(eq=False, repr=False)
class FileSearchMatch(betterproto.Message):
    range: "FileSearchRange" = betterproto.message_field(1)
    content: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class FileSearchPosition(betterproto.Message):
    line: int = betterproto.int32_field(1)
    column: int = betterproto.int32_field(2)


@dataclass(eq=False, repr=False)
class FileSearchRange(betterproto.Message):
    start: "FileSearchPosition" = betterproto.message_field(1)
    end: "FileSearchPosition" = betterproto.message_field(2)


@dataclass(eq=False, repr=False)
class FileSearchResult(betterproto.Message):
    path: str = betterproto.string_field(1)
    matches: List["FileSearchMatch"] = betterproto.message_field(2)


@dataclass(eq=False, repr=False)
class Mount(betterproto.Message):
    local_path: str = betterproto.string_field(1)
    mount_path: str = betterproto.string_field(2)
    link_path: str = betterproto.string_field(3)
    read_only: bool = betterproto.bool_field(4)
    mount_type: str = betterproto.string_field(5)
    mount_point_config: "MountPointConfig" = betterproto.message_field(6)


@dataclass(eq=False, repr=False)
class MountPointConfig(betterproto.Message):
    bucket_name: str = betterproto.string_field(1)
    access_key: str = betterproto.string_field(2)
    secret_key: str = betterproto.string_field(3)
    endpoint_url: str = betterproto.string_field(4)
    region: str = betterproto.string_field(5)
    read_only: bool = betterproto.bool_field(6)
    force_path_style: bool = betterproto.bool_field(7)


@dataclass(eq=False, repr=False)
class NullTime(betterproto.Message):
    null_time: "NullTime" = betterproto.message_field(1)


@dataclass(eq=False, repr=False)
class Object(betterproto.Message):
    id: int = betterproto.uint32_field(1)
    external_id: str = betterproto.string_field(2)
    hash: str = betterproto.string_field(3)
    size: int = betterproto.int64_field(4)
    workspace_id: int = betterproto.uint32_field(5)
    created_at: datetime = betterproto.message_field(6)


@dataclass(eq=False, repr=False)
class PricingPolicy(betterproto.Message):
    max_in_flight: int = betterproto.int64_field(1)
    cost_model: str = betterproto.string_field(2)
    cost_per_task: float = betterproto.double_field(3)
    cost_per_task_duration_ms: float = betterproto.double_field(4)


@dataclass(eq=False, repr=False)
class Stub(betterproto.Message):
    id: int = betterproto.uint32_field(1)
    external_id: str = betterproto.string_field(2)
    name: str = betterproto.string_field(3)
    type: str = betterproto.string_field(4)
    config: str = betterproto.string_field(5)
    config_version: int = betterproto.uint32_field(6)
    object_id: int = betterproto.uint32_field(7)
    workspace_id: int = betterproto.uint32_field(8)
    created_at: datetime = betterproto.message_field(9)
    updated_at: datetime = betterproto.message_field(10)
    public: bool = betterproto.bool_field(11)
    app_id: int = betterproto.uint32_field(12)


@dataclass(eq=False, repr=False)
class StubWithRelated(betterproto.Message):
    stub: "Stub" = betterproto.message_field(1)
    workspace: "Workspace" = betterproto.message_field(2)
    object: "Object" = betterproto.message_field(3)
    app: "App" = betterproto.message_field(4)


@dataclass(eq=False, repr=False)
class Worker(betterproto.Message):
    id: str = betterproto.string_field(1)
    status: str = betterproto.string_field(2)
    total_cpu: int = betterproto.int64_field(3)
    total_memory: int = betterproto.int64_field(4)
    total_gpu_count: int = betterproto.uint32_field(5)
    free_cpu: int = betterproto.int64_field(6)
    free_memory: int = betterproto.int64_field(7)
    free_gpu_count: int = betterproto.uint32_field(8)
    gpu: str = betterproto.string_field(9)
    pool_name: str = betterproto.string_field(10)
    machine_id: str = betterproto.string_field(11)
    resource_version: int = betterproto.int64_field(12)
    requires_pool_selector: bool = betterproto.bool_field(13)
    priority: int = betterproto.int32_field(14)
    preemptable: bool = betterproto.bool_field(15)
    build_version: str = betterproto.string_field(16)
    active_containers: List["Container"] = betterproto.message_field(17)


@dataclass(eq=False, repr=False)
class WorkerPoolState(betterproto.Message):
    status: str = betterproto.string_field(1)
    scheduling_latency: int = betterproto.int64_field(2)
    free_gpu: int = betterproto.uint32_field(3)
    free_cpu: int = betterproto.int64_field(4)
    free_memory: int = betterproto.int64_field(5)
    pending_workers: int = betterproto.int64_field(6)
    available_workers: int = betterproto.int64_field(7)
    pending_containers: int = betterproto.int64_field(8)
    running_containers: int = betterproto.int64_field(9)
    registered_machines: int = betterproto.int64_field(10)
    pending_machines: int = betterproto.int64_field(11)
    ready_machines: int = betterproto.int64_field(12)


@dataclass(eq=False, repr=False)
class Workspace(betterproto.Message):
    id: int = betterproto.uint32_field(1)
    external_id: str = betterproto.string_field(2)
    name: str = betterproto.string_field(3)
    created_at: datetime = betterproto.message_field(4)
    updated_at: datetime = betterproto.message_field(5)
    signing_key: str = betterproto.string_field(6)
    volume_cache_enabled: bool = betterproto.bool_field(7)
    multi_gpu_enabled: bool = betterproto.bool_field(8)
    concurrency_limit_id: int = betterproto.uint32_field(9)
    concurrency_limit: "ConcurrencyLimit" = betterproto.message_field(10)
    storage_id: int = betterproto.uint32_field(11)
    storage: "WorkspaceStorage" = betterproto.message_field(12)


@dataclass(eq=False, repr=False)
class WorkspaceStorage(betterproto.Message):
    id: int = betterproto.uint32_field(1)
    external_id: str = betterproto.string_field(2)
    bucket_name: str = betterproto.string_field(3)
    access_key: str = betterproto.string_field(4)
    secret_key: str = betterproto.string_field(5)
    endpoint_url: str = betterproto.string_field(6)
    region: str = betterproto.string_field(7)
    created_at: datetime = betterproto.message_field(8)
    updated_at: datetime = betterproto.message_field(9)

================================================================================
# File: clients/volume/__init__.py
# Path: src/beta9/clients/volume/__init__.py
================================================================================

# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: volume.proto
# plugin: python-betterproto
# This file has been @generated

from dataclasses import dataclass
from datetime import datetime
from typing import (
    TYPE_CHECKING,
    AsyncIterable,
    AsyncIterator,
    Dict,
    Iterable,
    Iterator,
    List,
    Optional,
    Union,
)

import betterproto
import grpc
from betterproto.grpcstub.grpcio_client import SyncServiceStub
from betterproto.grpcstub.grpclib_server import ServiceBase


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpcstub.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


class PresignedUrlMethod(betterproto.Enum):
    GetObject = 0
    PutObject = 1
    HeadObject = 2
    UploadPart = 3


@dataclass(eq=False, repr=False)
class VolumeInstance(betterproto.Message):
    id: str = betterproto.string_field(1)
    name: str = betterproto.string_field(2)
    size: int = betterproto.uint64_field(3)
    created_at: datetime = betterproto.message_field(4)
    updated_at: datetime = betterproto.message_field(5)
    workspace_id: str = betterproto.string_field(6)
    workspace_name: str = betterproto.string_field(7)


@dataclass(eq=False, repr=False)
class GetOrCreateVolumeRequest(betterproto.Message):
    name: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class GetOrCreateVolumeResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    volume: "VolumeInstance" = betterproto.message_field(3)


@dataclass(eq=False, repr=False)
class DeleteVolumeRequest(betterproto.Message):
    name: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class DeleteVolumeResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class PathInfo(betterproto.Message):
    path: str = betterproto.string_field(1)
    size: int = betterproto.uint64_field(2)
    mod_time: datetime = betterproto.message_field(3)
    is_dir: bool = betterproto.bool_field(4)


@dataclass(eq=False, repr=False)
class ListPathRequest(betterproto.Message):
    path: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class ListPathResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    path_infos: List["PathInfo"] = betterproto.message_field(3)


@dataclass(eq=False, repr=False)
class DeletePathRequest(betterproto.Message):
    path: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class DeletePathResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    deleted: List[str] = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class CopyPathRequest(betterproto.Message):
    path: str = betterproto.string_field(1)
    content: bytes = betterproto.bytes_field(2)


@dataclass(eq=False, repr=False)
class CopyPathResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    object_id: str = betterproto.string_field(2)
    err_msg: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class ListVolumesRequest(betterproto.Message):
    pass


@dataclass(eq=False, repr=False)
class ListVolumesResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    volumes: List["VolumeInstance"] = betterproto.message_field(3)


@dataclass(eq=False, repr=False)
class MovePathRequest(betterproto.Message):
    original_path: str = betterproto.string_field(1)
    new_path: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class MovePathResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    new_path: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class StatPathRequest(betterproto.Message):
    path: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class StatPathResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    path_info: "PathInfo" = betterproto.message_field(3)


@dataclass(eq=False, repr=False)
class PresignedUrlParams(betterproto.Message):
    upload_id: str = betterproto.string_field(1)
    part_number: int = betterproto.uint32_field(2)
    content_length: int = betterproto.uint64_field(3)
    content_type: str = betterproto.string_field(4)


@dataclass(eq=False, repr=False)
class GetFileServiceInfoRequest(betterproto.Message):
    pass


@dataclass(eq=False, repr=False)
class GetFileServiceInfoResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    enabled: bool = betterproto.bool_field(3)
    command_version: int = betterproto.uint32_field(4)


@dataclass(eq=False, repr=False)
class CreatePresignedUrlRequest(betterproto.Message):
    volume_name: str = betterproto.string_field(1)
    volume_path: str = betterproto.string_field(2)
    expires: int = betterproto.uint32_field(3)
    method: "PresignedUrlMethod" = betterproto.enum_field(4)
    params: "PresignedUrlParams" = betterproto.message_field(5)


@dataclass(eq=False, repr=False)
class CreatePresignedUrlResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    url: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class CreateMultipartUploadRequest(betterproto.Message):
    volume_name: str = betterproto.string_field(1)
    volume_path: str = betterproto.string_field(2)
    chunk_size: int = betterproto.uint64_field(3)
    file_size: int = betterproto.uint64_field(4)


@dataclass(eq=False, repr=False)
class FileUploadPart(betterproto.Message):
    number: int = betterproto.uint32_field(1)
    start: int = betterproto.uint64_field(2)
    end: int = betterproto.uint64_field(3)
    url: str = betterproto.string_field(4)


@dataclass(eq=False, repr=False)
class CreateMultipartUploadResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)
    upload_id: str = betterproto.string_field(3)
    file_upload_parts: List["FileUploadPart"] = betterproto.message_field(4)


@dataclass(eq=False, repr=False)
class CompletedPart(betterproto.Message):
    number: int = betterproto.uint32_field(1)
    etag: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class CompleteMultipartUploadRequest(betterproto.Message):
    upload_id: str = betterproto.string_field(1)
    volume_name: str = betterproto.string_field(2)
    volume_path: str = betterproto.string_field(3)
    completed_parts: List["CompletedPart"] = betterproto.message_field(4)


@dataclass(eq=False, repr=False)
class CompleteMultipartUploadResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class AbortMultipartUploadRequest(betterproto.Message):
    upload_id: str = betterproto.string_field(1)
    volume_name: str = betterproto.string_field(2)
    volume_path: str = betterproto.string_field(3)


@dataclass(eq=False, repr=False)
class AbortMultipartUploadResponse(betterproto.Message):
    ok: bool = betterproto.bool_field(1)
    err_msg: str = betterproto.string_field(2)


class VolumeServiceStub(SyncServiceStub):
    def get_or_create_volume(
        self, get_or_create_volume_request: "GetOrCreateVolumeRequest"
    ) -> "GetOrCreateVolumeResponse":
        return self._unary_unary(
            "/volume.VolumeService/GetOrCreateVolume",
            GetOrCreateVolumeRequest,
            GetOrCreateVolumeResponse,
        )(get_or_create_volume_request)

    def delete_volume(
        self, delete_volume_request: "DeleteVolumeRequest"
    ) -> "DeleteVolumeResponse":
        return self._unary_unary(
            "/volume.VolumeService/DeleteVolume",
            DeleteVolumeRequest,
            DeleteVolumeResponse,
        )(delete_volume_request)

    def list_volumes(
        self, list_volumes_request: "ListVolumesRequest"
    ) -> "ListVolumesResponse":
        return self._unary_unary(
            "/volume.VolumeService/ListVolumes",
            ListVolumesRequest,
            ListVolumesResponse,
        )(list_volumes_request)

    def list_path(self, list_path_request: "ListPathRequest") -> "ListPathResponse":
        return self._unary_unary(
            "/volume.VolumeService/ListPath",
            ListPathRequest,
            ListPathResponse,
        )(list_path_request)

    def delete_path(
        self, delete_path_request: "DeletePathRequest"
    ) -> "DeletePathResponse":
        return self._unary_unary(
            "/volume.VolumeService/DeletePath",
            DeletePathRequest,
            DeletePathResponse,
        )(delete_path_request)

    def copy_path_stream(
        self, copy_path_request_iterator: Iterable["CopyPathRequest"]
    ) -> "CopyPathResponse":
        return (
            self._stream_unary(
                "/volume.VolumeService/CopyPathStream",
                CopyPathRequest,
                CopyPathResponse,
            )
            .future(copy_path_request_iterator)
            .result()
        )

    def move_path(self, move_path_request: "MovePathRequest") -> "MovePathResponse":
        return self._unary_unary(
            "/volume.VolumeService/MovePath",
            MovePathRequest,
            MovePathResponse,
        )(move_path_request)

    def stat_path(self, stat_path_request: "StatPathRequest") -> "StatPathResponse":
        return self._unary_unary(
            "/volume.VolumeService/StatPath",
            StatPathRequest,
            StatPathResponse,
        )(stat_path_request)

    def get_file_service_info(
        self, get_file_service_info_request: "GetFileServiceInfoRequest"
    ) -> "GetFileServiceInfoResponse":
        return self._unary_unary(
            "/volume.VolumeService/GetFileServiceInfo",
            GetFileServiceInfoRequest,
            GetFileServiceInfoResponse,
        )(get_file_service_info_request)

    def create_presigned_url(
        self, create_presigned_url_request: "CreatePresignedUrlRequest"
    ) -> "CreatePresignedUrlResponse":
        return self._unary_unary(
            "/volume.VolumeService/CreatePresignedURL",
            CreatePresignedUrlRequest,
            CreatePresignedUrlResponse,
        )(create_presigned_url_request)

    def create_multipart_upload(
        self, create_multipart_upload_request: "CreateMultipartUploadRequest"
    ) -> "CreateMultipartUploadResponse":
        return self._unary_unary(
            "/volume.VolumeService/CreateMultipartUpload",
            CreateMultipartUploadRequest,
            CreateMultipartUploadResponse,
        )(create_multipart_upload_request)

    def complete_multipart_upload(
        self, complete_multipart_upload_request: "CompleteMultipartUploadRequest"
    ) -> "CompleteMultipartUploadResponse":
        return self._unary_unary(
            "/volume.VolumeService/CompleteMultipartUpload",
            CompleteMultipartUploadRequest,
            CompleteMultipartUploadResponse,
        )(complete_multipart_upload_request)

    def abort_multipart_upload(
        self, abort_multipart_upload_request: "AbortMultipartUploadRequest"
    ) -> "AbortMultipartUploadResponse":
        return self._unary_unary(
            "/volume.VolumeService/AbortMultipartUpload",
            AbortMultipartUploadRequest,
            AbortMultipartUploadResponse,
        )(abort_multipart_upload_request)

================================================================================
# File: config.py
# Path: src/beta9/config.py
================================================================================

import configparser
import functools
import inspect
import ipaddress
import os
import socket
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any, Mapping, MutableMapping, Optional, Tuple, Union

from . import terminal

DEFAULT_CLI_NAME = "Beta9"
DEFAULT_CONTEXT_NAME = "default"
DEFAULT_GATEWAY_HOST = "0.0.0.0"
DEFAULT_GATEWAY_PORT = 1993
DEFAULT_API_HOST = "0.0.0.0"
DEFAULT_API_PORT = 1994
_SETTINGS: Optional["SDKSettings"] = None
DEFAULT_ASCII_LOGO = """
           ,#@@&&&&&&&&&@&/
        @&&&&&&&&&&&&&&&&&&&&@#
         *@&&&&&&&&&&&&&&&&&&&&&@/
   ##      /&&&&&&&&&&&&&@&&&&&&&&@,
  @&&&&&.    (&&&&&&@/    &&&&&&&&&&/
 &&&&&&&&&@*   %&@.      @& ,@&&&&&&&,
.@&&&&&&&&&&&&#        &&*  ,@&&&&&&&&
*&&&&&&&&&&&@,   %&@/@&*    @&&&&&&&&@
.@&&&&&&&&&*      *&@     .@&&&&&&&&&&
 %&&&&&&&&     /@@*     .@&&&&&&&&&&@,
  &&&&&&&/.#@&&.     .&&&    %&&&&&@,
   /&&&&&&&@%*,,*#@&&(         ,@&&
     /&&&&&&&&&&&&&&,
        #@&&&&&&&&&&,
            ,(&@@&&&,
"""


@dataclass
class SDKSettings:
    name: str = DEFAULT_CLI_NAME
    gateway_host: str = DEFAULT_GATEWAY_HOST
    gateway_port: int = DEFAULT_GATEWAY_PORT
    api_host: str = DEFAULT_API_HOST
    api_port: int = DEFAULT_API_PORT
    config_path: Path = Path("~/.beta9/config.ini").expanduser()
    ascii_logo: str = DEFAULT_ASCII_LOGO
    use_defaults_in_prompt: bool = False

    def __post_init__(self, **kwargs):
        if p := os.getenv("CONFIG_PATH"):
            self.config_path = Path(p).expanduser()


@dataclass
class ConfigContext:
    token: Optional[str] = None
    gateway_host: Optional[str] = None
    gateway_port: Optional[int] = None

    @classmethod
    def from_dict(cls, data: Mapping[str, Any]) -> "ConfigContext":
        return cls(**{k: v for k, v in data.items() if k in inspect.signature(cls).parameters})

    def to_dict(self) -> MutableMapping[str, Any]:
        return {k: ("" if not v else v) for k, v in asdict(self).items()}

    def use_ssl(self) -> bool:
        if self.gateway_port in [443, "443"]:
            return True
        return False

    def is_valid(self) -> bool:
        return all([self.token, self.gateway_host, self.gateway_port])


def set_settings(s: Optional[SDKSettings] = None) -> None:
    if s is None:
        s = SDKSettings()

    global _SETTINGS
    _SETTINGS = s


def get_settings() -> SDKSettings:
    if not _SETTINGS:
        set_settings()

    return _SETTINGS  # type: ignore


def load_config(path: Optional[Union[str, Path]] = None) -> MutableMapping[str, ConfigContext]:
    if path is None:
        path = get_settings().config_path

    path = Path(path)
    if not path.exists():
        return {}

    parser = configparser.ConfigParser(default_section=DEFAULT_CONTEXT_NAME)
    parser.read(path)

    return {k: ConfigContext.from_dict(v) for k, v in parser.items()}  # type:ignore


def save_config(
    contexts: Mapping[str, ConfigContext], path: Optional[Union[Path, str]] = None
) -> None:
    if not contexts:
        return

    if path is None:
        path = get_settings().config_path

    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)

    parser = configparser.ConfigParser(default_section=DEFAULT_CONTEXT_NAME)
    parser.read_dict({k: v.to_dict() for k, v in contexts.items()})

    with open(path, "w") as file:
        parser.write(file)


def is_config_empty(path: Optional[Union[Path, str]] = None) -> bool:
    if path is None:
        path = get_settings().config_path

    path = Path(path)
    if path.exists():
        return False

    parser = configparser.ConfigParser()
    parser.read(path)
    if any(v.get("gateway_host") for v in parser.values()):
        return False

    return True


def get_config_context(name: str = DEFAULT_CONTEXT_NAME) -> ConfigContext:
    contexts = load_config()
    if name in contexts:
        return contexts[name]

    gateway_host = os.getenv("BETA9_GATEWAY_HOST", None)
    gateway_port = os.getenv("BETA9_GATEWAY_PORT", None)
    token = os.getenv("BETA9_TOKEN", None)

    if gateway_host and gateway_port and token:
        return ConfigContext(
            token=token,
            gateway_host=gateway_host,
            gateway_port=gateway_port,
        )

    terminal.header(f"Context '{name}' does not exist. Let's try setting it up.")
    contexts[name] = prompt_for_config_context(name=name, require_token=True)[1]
    save_config(contexts)
    return contexts[name]


def prompt_for_config_context(
    name: Optional[str] = None,
    token: Optional[str] = None,
    gateway_host: Optional[str] = None,
    gateway_port: Optional[int] = None,
    require_token: bool = False,
) -> Tuple[str, ConfigContext]:
    settings = get_settings()

    prompt_name = functools.partial(
        terminal.prompt, text="Context Name", default=name or DEFAULT_CONTEXT_NAME
    )
    prompt_gateway_host = functools.partial(
        terminal.prompt, text="Gateway Host", default=gateway_host or settings.gateway_host
    )
    prompt_gateway_port = functools.partial(
        terminal.prompt, text="Gateway Port", default=gateway_port or settings.gateway_port
    )

    try:
        while not name and not (name := prompt_name()):
            terminal.warn("Name is invalid.")

        if settings.use_defaults_in_prompt:
            gateway_host = settings.gateway_host
            gateway_port = settings.gateway_port
        else:
            while not (gateway_host := prompt_gateway_host()) or not validate_ip_or_dns(
                gateway_host
            ):
                terminal.warn("Gateway host is invalid or unreachable.")

            while not (gateway_port := prompt_gateway_port()) or not validate_port(gateway_port):
                terminal.warn("Gateway port is invalid.")

        if require_token:
            while not (token := terminal.prompt(text="Token", default=None)) or len(token) < 64:
                terminal.warn("Token is invalid.")
        else:
            token = terminal.prompt(text="Token", default=None)

    except (KeyboardInterrupt, EOFError):
        os._exit(1)

    return name, ConfigContext(
        token=token,
        gateway_host=gateway_host,
        gateway_port=gateway_port,
    )


def validate_ip_or_dns(value) -> bool:
    try:
        ipaddress.ip_address(value)
        return True
    except ValueError:
        pass

    try:
        socket.gethostbyname(value)
        return True
    except socket.error:
        pass

    return False


def validate_port(value: Any) -> bool:
    try:
        if 0 < int(value) <= 65535:
            return True
    except ValueError:
        pass

    return False

================================================================================
# File: env.py
# Path: src/beta9/env.py
================================================================================

import os
import sys
from functools import wraps
from typing import Callable, TypeVar, Union, overload


def called_on_import() -> bool:
    """Check if we are currently in the process of importing the users code."""
    return os.getenv("BETA9_IMPORTING_USER_CODE", "false") == "true"


def is_local() -> bool:
    """Check if we are currently running in a remote container"""
    return os.getenv("CONTAINER_ID", "") == ""


def is_remote() -> bool:
    return not is_local()


def local_entrypoint(func: Callable) -> None:
    """Decorator that executes the decorated function if the environment is local (i.e. not a remote container)"""

    @wraps(func)
    def wrapper(*args, **kwargs):
        if is_local():
            func(*args, **kwargs)

    wrapper()


EnvValue = TypeVar("EnvValue", str, int, float, bool)


@overload
def try_env(env: str, default: str) -> bool: ...


@overload
def try_env(env: str, default: EnvValue) -> EnvValue: ...


def try_env(env: str, default: EnvValue) -> Union[EnvValue, bool]:
    """
    Gets an environment variable and converts it to the correct type based on
    the default value.

    The environment variable name is prefixed with the name of the project if
    it is set in the settings.

    Args:
        env: The name of the environment variable.
        default: The default value to return if the environment variable doesn't exist.

    Returns:
        The value of the environment variable or the default value if it doesn't exist.
    """
    from .config import get_settings

    name = get_settings().name.upper()
    env_var = env.upper()
    env_val = os.getenv(f"{name}_{env_var}", "")
    target_type = type(default)

    try:
        if target_type is bool:
            return env_val.lower() in ["true", "1", "yes", "on"]

        return target_type(env_val) or default
    except (ValueError, TypeError):
        return default


def is_notebook_env() -> bool:
    if "google.colab" in sys.modules or "marimo" in sys.modules:
        return True

    try:
        import hex

        return hex.context.is_notebook()
    except (ImportError, NameError, AttributeError):
        pass

    try:
        from ipykernel.zmqshell import ZMQInteractiveShell
        from IPython import get_ipython

        shell = get_ipython().__class__.__name__
        return shell == ZMQInteractiveShell.__name__
    except (NameError, ImportError):
        return False

================================================================================
# File: exceptions.py
# Path: src/beta9/exceptions.py
================================================================================

class RunnerException(SystemExit):
    def __init__(self, message="", *args):
        self.message = message
        self.code = 1
        super().__init__(*args)


class InvalidFunctionArgumentsError(RuntimeError):
    def __init__(self):
        super().__init__("Invalid function arguments")


class FunctionSetResultError(RunnerException):
    def __init__(self):
        super().__init__("Unable to set function result")


class TaskStartError(RunnerException):
    def __init__(self):
        super().__init__("Unable to start task")


class TaskEndError(RunnerException):
    def __init__(self):
        super().__init__("Unable to end task")


class InvalidRunnerEnvironmentError(RunnerException):
    def __init__(self):
        super().__init__("Invalid runner environment")


class CreatePresignedUrlError(RuntimeError):
    def __init__(self, message: str):
        self.message = message
        super().__init__(f"Unable to create presigned URL: {message}")


class CreateMultipartUploadError(RuntimeError):
    def __init__(self, message: str):
        self.message = message
        super().__init__(f"Unable to create multipart upload: {message}")


class CompleteMultipartUploadError(RuntimeError):
    def __init__(self, message: str):
        self.message = message
        super().__init__(f"Unable to complete multipart upload: {message}")


class UploadPartError(RuntimeError):
    def __init__(self, part_number: int, message: str):
        self.message = message
        self.part_number = part_number
        super().__init__(f"Unable to upload part: {part_number=} {message=}")


class DownloadChunkError(RuntimeError):
    def __init__(self, number: int, start: int, end: int, message: str):
        self.message = message
        self.number = number
        self.start = start
        self.end = end
        super().__init__(f"Unable to download chunk: {number=} {start=} {end=} {message=}")


class RetryableError(Exception):
    def __init__(self, tries: int, message: str):
        self.message = message
        super().__init__(f"Retryable error after {tries} tries: {message}")


class GetFileSizeError(RuntimeError):
    def __init__(self, status_code: int, message: str):
        self.message = message
        self.status_code = status_code
        super().__init__(f"Unable to get file size: {status_code=} {message=}")


class ListPathError(RuntimeError):
    def __init__(self, path: str, message: str):
        self.message = message.capitalize() if message else ""
        self.path = path
        super().__init__(f"Unable to list path: {path=} {message=}")


class StatPathError(RuntimeError):
    def __init__(self, path: str, message: str):
        self.message = message.capitalize() if message else ""
        self.path = path
        super().__init__(f"Unable to stat path: {path=} {message=}")


class TaskNotFoundError(RuntimeError):
    def __init__(self, task_id: str):
        self.task_id = task_id
        super().__init__(f"Task not found: {task_id}")


class WorkspaceNotFoundError(RuntimeError):
    def __init__(self, workspace_id: str):
        self.workspace_id = workspace_id
        super().__init__(f"Workspace not found: {workspace_id}")


class StubNotFoundError(RuntimeError):
    def __init__(self, stub_id: str):
        self.stub_id = stub_id
        super().__init__(f"Stub not found: {stub_id=}")


class DeploymentNotFoundError(RuntimeError):
    def __init__(self, deployment_id: str):
        self.deployment_id = deployment_id
        super().__init__(f"Deployment not found: {deployment_id=}")


class VolumeUploadError(RuntimeError):
    def __init__(self, message: str):
        self.message = message
        super().__init__(f"Unable to upload volume: {message}")


class SandboxProcessError(RuntimeError):
    def __init__(self, message: str):
        self.message = message
        super().__init__(f"Unable to launch sandbox process: {message}")


class SandboxFileSystemError(RuntimeError):
    def __init__(self, message: str):
        self.message = message
        super().__init__(f"Unable to perform sandbox filesystem operation: {message}")

================================================================================
# File: integrations.py
# Path: src/beta9/integrations.py
================================================================================

from .abstractions.integrations import VLLM, MCPServer, MCPServerArgs, VLLMArgs

__all__ = ["VLLM", "VLLMArgs", "MCPServer", "MCPServerArgs"]

================================================================================
# File: logging.py
# Path: src/beta9/logging.py
================================================================================

import functools
import io
import json
import sys
from typing import Any


class StdoutJsonInterceptor(io.TextIOBase):
    def __init__(self, stream=sys.__stdout__, **ctx: Any):
        self.ctx = ctx
        self.stream = stream

    def __enter__(self):
        sys.stdout = self
        sys.stderr = self
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        sys.stdout = sys.__stdout__
        sys.stderr = sys.__stderr__
        return False

    def write(self, buf: str):
        try:
            for line in buf.splitlines():
                if line == "":
                    continue

                log_record = {
                    "message": f"{line}\n",
                    **self.ctx,
                }

                self.stream.write(json.dumps(log_record))
        except BaseException:
            self.stream.write(buf)

    def flush(self):
        return self.stream.flush()

    def fileno(self) -> int:
        return -1


def json_output_interceptor(**ctx: Any):
    """
    A class decorator that intercepts stdout and stderr and writes the output
    as JSON objects to the original stdout.
    """

    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            with StdoutJsonInterceptor(**ctx):
                return func(*args, **kwargs)

        return wrapper

    return decorator

================================================================================
# File: middleware.py
# Path: src/beta9/middleware.py
================================================================================

import os
import time
from dataclasses import dataclass
from http import HTTPStatus
from typing import Any, Optional

from fastapi import HTTPException, Request
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import HTTPConnection
from starlette.types import ASGIApp, Receive, Scope, Send

from .clients.gateway import (
    EndTaskRequest,
    StartTaskRequest,
)
from .logging import StdoutJsonInterceptor
from .runner.common import config as cfg
from .runner.common import end_task_and_send_callback
from .type import TaskStatus


@dataclass
class TaskLifecycleData:
    status: TaskStatus
    result: Any
    override_callback_url: Optional[str] = None


async def run_task(request, func, func_args):
    start_time = time.time()
    task_id = request.headers.get("X-TASK-ID")
    if not task_id:
        raise HTTPException(status_code=HTTPStatus.BAD_REQUEST, detail="Task ID missing")

    os.environ["TASK_ID"] = task_id
    with StdoutJsonInterceptor(task_id=task_id):
        print(f"Received task <{task_id}>")

        for attempt in range(3):
            try:
                start_response = request.app.state.gateway_stub.start_task(
                    StartTaskRequest(task_id=task_id, container_id=cfg.container_id)
                )
                if start_response.ok:
                    break
                else:
                    raise HTTPException(
                        status_code=HTTPStatus.INTERNAL_SERVER_ERROR, detail="Failed to start task"
                    )
            except BaseException:
                if attempt == 2:
                    raise

        task_lifecycle_data = TaskLifecycleData(
            status=TaskStatus.Complete, result=None, override_callback_url=None
        )

        try:
            request.state.task_lifecycle_data = task_lifecycle_data
            response = await func(*func_args)
            print(f"Task <{task_id}> finished")
            return response
        finally:
            if "TASK_ID" in os.environ:
                del os.environ["TASK_ID"]

            end_task_and_send_callback(
                gateway_stub=request.app.state.gateway_stub,
                payload=task_lifecycle_data.result,
                end_task_request=EndTaskRequest(
                    task_id=task_id,
                    container_id=cfg.container_id,
                    keep_warm_seconds=cfg.keep_warm_seconds,
                    task_status=task_lifecycle_data.status,
                    task_duration=time.time() - start_time,
                ),
                override_callback_url=task_lifecycle_data.override_callback_url,
            )


class WebsocketTaskLifecycleMiddleware:
    def __init__(self, app: ASGIApp) -> None:
        self.app = app

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] != "websocket":
            await self.app(scope, receive, send)
            return

        request = HTTPConnection(scope, receive)
        return await run_task(request, self.app, (scope, receive, send))


class TaskLifecycleMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        if request.url.path == "/health":
            return await call_next(request)

        return await run_task(request, call_next, (request,))

================================================================================
# File: multipart.py
# Path: src/beta9/multipart.py
================================================================================

import abc
import concurrent.futures
import io
import math
import os
import signal
import tempfile
import time
from concurrent.futures import ProcessPoolExecutor
from contextlib import ExitStack, contextmanager
from functools import wraps
from multiprocessing import Manager
from os import PathLike
from pathlib import Path
from queue import Empty, Queue
from threading import Thread
from typing import (
    Any,
    Callable,
    ContextManager,
    Final,
    Generator,
    List,
    NamedTuple,
    Optional,
    Protocol,
    Sequence,
    Tuple,
    TypeVar,
    Union,
)

import click
import requests
from requests import Session
from typing_extensions import ParamSpec

from .clients.volume import (
    AbortMultipartUploadRequest,
    CompletedPart,
    CompleteMultipartUploadRequest,
    CreateMultipartUploadRequest,
    CreatePresignedUrlRequest,
    FileUploadPart,
    ListPathRequest,
    PresignedUrlMethod,
    StatPathRequest,
    VolumeServiceStub,
)
from .env import try_env
from .exceptions import (
    CompleteMultipartUploadError,
    CreateMultipartUploadError,
    CreatePresignedUrlError,
    DownloadChunkError,
    GetFileSizeError,
    ListPathError,
    RetryableError,
    StatPathError,
    UploadPartError,
)
from .terminal import CustomProgress

# Value of 0 means the number of workers is calculated based on the file size
_MAX_WORKERS: Final = try_env("MULTIPART_MAX_WORKERS", 0)
_REQUEST_TIMEOUT: Final = try_env("MULTIPART_REQUEST_TIMEOUT", 5)
_DEBUG_RETRY: Final = try_env("MULTIPART_DEBUG_RETRY", False)


class ProgressCallbackType(Protocol):
    def __call__(self, total: int, advance: int) -> None: ...


class CompletionCallbackType(Protocol):
    def __call__(self) -> ContextManager: ...


P = ParamSpec("P")
R = TypeVar("R")


def retry(
    times: int, delay: float = 0.1, max_delay: float = 10.0
) -> Callable[[Callable[P, R]], Callable[P, R]]:
    """
    Retry a function multiple times with exponential backoff.

    The exponential backoff starts with the initial delay and
    doubles with each retry.

    Args:
        times: The number of times to retry the function.
        delay: The initial delay between retries. Defaults to 0.1.
        max_delay: The maximum delay between retries. Defaults to 10.0.

    Raises:
        RetryableError: If the function fails after all retries.

    Returns:
        A decorator that wraps the function.
    """

    def decorator(func: Callable[P, R]) -> Callable[P, R]:
        @wraps(func)
        def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:
            current_delay = delay
            last_exception = None

            for attempt in range(times):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if _DEBUG_RETRY:
                        print(e)

                    last_exception = e
                    if attempt < times - 1:
                        time.sleep(min(current_delay, max_delay))
                        current_delay *= 2

            raise RetryableError(times, str(last_exception))

        return wrapper

    return decorator


@contextmanager
def _progress_updater(
    file_size: int,
    queue: Queue,
    callback: Optional[ProgressCallbackType] = None,
    timeout: float = 1.0,
) -> Generator[Thread, None, None]:
    """
    Calls a callback with the progress of a multipart upload or download.

    Args:
        file_size: The total size of the file.
        queue: A queue that receives the number of bytes processed.
        callback: A callback that receives the total size and the number of bytes processed. Defaults to None.
        timeout: The time to wait for the thread to finish. Defaults to 1.0.

    Yields:
        A thread that updates the progress.
    """

    def target():
        def cb(total: int, advance: int):
            if callback is not None:
                callback(total=total, advance=advance)

        finished = 0
        while finished < file_size:
            try:
                processed = queue.get(timeout=1)
            except Empty:
                continue
            except Exception:
                break

            if processed:
                finished += processed
                cb(total=file_size, advance=processed)

        cb(total=file_size, advance=0)

    thread = Thread(target=target, daemon=True)
    thread.start()

    yield thread

    thread.join(timeout=timeout)


# Global session for making HTTP requests
_session = None


def _get_session() -> Session:
    """
    Get a session for making HTTP requests.

    This is not thread safe, but should be process safe.
    """
    global _session
    if _session is None:
        _session = requests.Session()
    return _session


def _init():
    """
    Initialize the process by setting a signal handler.
    """
    _get_session()

    signal.signal(signal.SIGINT, lambda *_: os.kill(os.getpid(), signal.SIGTERM))


@retry(times=10)
def _upload_part(file_path: Path, file_part: FileUploadPart, queue: Queue) -> CompletedPart:
    """
    Read a chunk of a file and upload it to a URL.

    Args:
        file_path: Path to the file.
        file_part: Information about the part to upload.
        queue: A queue to send the number of bytes processed.

    Raises:
        UploadPartError: If the upload fails.

    Returns:
        Information about the completed part.
    """
    session = _get_session()
    chunk = _get_file_chunk(file_path, file_part.start, file_part.end)

    class QueueBuffer(io.BytesIO):
        def read(self, size: Optional[int] = -1) -> bytes:
            b = super().read(size)
            queue.put_nowait(len(b))
            return b

    try:
        response = session.put(
            url=file_part.url,
            data=QueueBuffer(chunk) if chunk else None,
            headers={
                "Content-Length": str(len(chunk)),
            },
            timeout=_REQUEST_TIMEOUT,
        )
        response.raise_for_status()
        etag = response.headers["ETag"].replace('"', "")
    except Exception as e:
        raise UploadPartError(file_part.number, str(e))

    return CompletedPart(number=file_part.number, etag=etag)


def _get_file_chunk(file_path: Path, start: int, end: int) -> bytes:
    with open(file_path, "rb") as f:
        f.seek(start)
        return f.read(end - start)


def beta9_upload(
    service: VolumeServiceStub,
    file_path: Path,
    remote_path: "RemotePath",
    progress_callback: Optional[ProgressCallbackType] = None,
    completion_callback: Optional[CompletionCallbackType] = None,
):
    """
    Upload a file to a volume using multipart upload.

    Args:
        service: The volume service stub.
        file_path: Path to the file to upload.
        remote_path: Path to save the file on the volume.
        progress_callback: A callback that receives the total size and the number of
            bytes processed. Defaults to None.
        completion_callback: A context manager that wraps the completion of the upload.
            Defaults to None.

    Raises:
        CreateMultipartUploadError: If initializing the upload fails.
        CompleteMultipartUploadError: If completing the upload fails.
        KeyboardInterrupt: If the upload is interrupted by the user.
        Exception: If any other error occurs.
    """
    # Initialize multipart upload
    file_size = file_path.stat().st_size
    chunk_size, max_workers = _calculate_chunk_size(file_size)
    initial = retry(times=3, delay=1.0)(service.create_multipart_upload)(
        CreateMultipartUploadRequest(
            volume_name=remote_path.volume_name,
            volume_path=remote_path.volume_path,
            chunk_size=chunk_size,
            file_size=file_size,
        )
    )
    if not initial.ok:
        raise CreateMultipartUploadError(initial.err_msg)

    # Start multipart upload
    try:
        with ExitStack() as stack:
            manager = stack.enter_context(Manager())
            workers = max(max_workers, _MAX_WORKERS)
            executor = stack.enter_context(ProcessPoolExecutor(workers, initializer=_init))

            queue = manager.Queue()
            stack.enter_context(_progress_updater(file_size, queue, progress_callback))

            futures = (
                executor.submit(_upload_part, file_path, part, queue)
                for part in initial.file_upload_parts
            )

            parts = [future.result() for future in concurrent.futures.as_completed(futures)]
            parts.sort(key=lambda part: part.number)

        # Complete multipart upload
        def complete_upload():
            completed = retry(times=3, delay=1.0)(service.complete_multipart_upload)(
                CompleteMultipartUploadRequest(
                    upload_id=initial.upload_id,
                    volume_name=remote_path.volume_name,
                    volume_path=remote_path.volume_path,
                    completed_parts=parts,
                )
            )
            if not completed.ok:
                raise CompleteMultipartUploadError(completed.err_msg)

        if completion_callback is not None:
            with completion_callback():
                complete_upload()
        else:
            complete_upload()

    except (Exception, KeyboardInterrupt):
        service.abort_multipart_upload(
            AbortMultipartUploadRequest(
                upload_id=initial.upload_id,
                volume_name=remote_path.volume_name,
                volume_path=remote_path.volume_path,
            )
        )
        raise


class FileChunk(NamedTuple):
    number: int
    path: Path


class FileRange(NamedTuple):
    number: int
    start: int
    end: int


@retry(times=3, delay=1.0)
def _get_file_size(url: str) -> int:
    session = _get_session()

    response = session.head(url)
    if response.status_code != 200:
        raise GetFileSizeError(response.status_code, response.text)

    return int(response.headers["Content-Length"])


def _calculate_file_ranges(file_size: int, chunk_size: int) -> List[FileRange]:
    """
    Calculate byte ranges for a file based on the chunk size.

    Args:
        file_size: Size of the file in bytes.
        chunk_size: Size of each chunk in bytes.

    Returns:
        List of byte ranges.
    """
    ranges = math.ceil(file_size / (chunk_size or 1))
    return [
        FileRange(
            number=i + 1,
            start=i * chunk_size,
            end=min(file_size - 1, (i + 1) * chunk_size - 1),
        )
        for i in range(ranges)
    ]


def _calculate_chunk_size(
    file_size: int,
    min_chunk_size: int = 5 * 1024 * 1024,
    max_chunk_size: int = 500 * 1024 * 1024,
    chunk_size_divisor: int = 10,
) -> Tuple[int, int]:
    """
    Calculate chunk size using log2 scaling.
    """
    if file_size <= min_chunk_size:
        return file_size, 1

    # Use log2 directly for chunk size
    chunk_size = 2 ** math.floor(math.log2(file_size / chunk_size_divisor))
    chunk_size = min(max(chunk_size, min_chunk_size), max_chunk_size, file_size)

    # Workers scale with log2
    workers = min(max(int(math.log2(file_size / min_chunk_size)), 1), 16)

    return chunk_size, workers


@retry(times=10)
def _download_chunk(
    url: str,
    file_range: FileRange,
    output_dir: Path,
    queue: Queue,
) -> FileChunk:
    """
    Download a byte range of a file to a temporary directory.

    Args:
        url: URL of the file.
        file_range: Byte range to download.
        output_dir: Directory to save the file.
        queue: A queue to send the number of bytes processed.

    Raises:
        DownloadChunkError: If the download fails.

    Returns:
        Information about the downloaded chunk.
    """
    session = _get_session()
    headers = {"Range": f"bytes={file_range.start}-{file_range.end}"}

    try:
        response = session.get(url=url, headers=headers, stream=True, timeout=_REQUEST_TIMEOUT)
        response.raise_for_status()

        path = output_dir / f"data_{file_range.number}"
        with open(path, "wb") as file:
            for chunk in response.iter_content(chunk_size=1024 * 1024):
                queue.put_nowait(file.write(chunk))
    except Exception as e:
        raise DownloadChunkError(file_range.number, file_range.start, file_range.end, str(e))

    return FileChunk(number=file_range.number, path=path)


def _merge_file_chunks(file_path: PathLike, file_chunks: Sequence[FileChunk]) -> None:
    """
    Merge file chunks into a single file then delete the chunks.
    """
    with open(file_path, "wb") as merged_file:
        for chunk in file_chunks:
            with open(chunk.path, "rb") as chunk_file:
                merged_file.write(chunk_file.read())
            os.remove(chunk.path)


def beta9_download(
    service: VolumeServiceStub,
    remote_path: "RemotePath",
    file_path: Path,
    callback: Optional[ProgressCallbackType] = None,
) -> None:
    """
    Download a file from a volume using multipart download.

    Args:
        service: The volume service stub.
        remote_path: Path to the file on the volume.
        file_path: Path to save the file.
        callback: A callback that receives the total size and the number of bytes processed. Defaults to None.

    Raises:
        CreatePresignedUrlError: If a presigned URL cannot be created.
    """
    file_path.parent.mkdir(parents=True, exist_ok=True)

    # Calculate byte ranges
    presigned = retry(times=3, delay=1.0)(service.create_presigned_url)(
        CreatePresignedUrlRequest(
            volume_name=remote_path.volume_name,
            volume_path=remote_path.volume_path,
            expires=30,
            method=PresignedUrlMethod.HeadObject,
        )
    )
    if not presigned.ok:
        raise CreatePresignedUrlError(presigned.err_msg)

    file_size = _get_file_size(presigned.url)
    chunk_size, max_workers = _calculate_chunk_size(file_size)
    file_ranges = _calculate_file_ranges(file_size, chunk_size)

    # Download and merge file ranges
    presigned = retry(times=3, delay=1.0)(service.create_presigned_url)(
        CreatePresignedUrlRequest(
            volume_name=remote_path.volume_name,
            volume_path=remote_path.volume_path,
            expires=7200,
            method=PresignedUrlMethod.GetObject,
        )
    )
    if not presigned.ok:
        raise CreatePresignedUrlError(presigned.err_msg)

    with ExitStack() as stack:
        manager = stack.enter_context(Manager())
        temp_dir = stack.enter_context(tempfile.TemporaryDirectory())
        workers = max(max_workers, _MAX_WORKERS)
        executor = stack.enter_context(ProcessPoolExecutor(workers, initializer=_init))

        queue = manager.Queue()
        stack.enter_context(_progress_updater(file_size, queue, callback))

        futures = (
            executor.submit(_download_chunk, presigned.url, file_range, Path(temp_dir), queue)
            for file_range in file_ranges
        )

        chunks = [future.result() for future in concurrent.futures.as_completed(futures)]
        chunks.sort(key=lambda chunk: chunk.number)
        _merge_file_chunks(file_path, chunks)


class RemotePath:
    def __init__(
        self, scheme: str, volume_name: str, volume_path: str, is_dir: Optional[bool] = None
    ):
        self.scheme = scheme
        self.volume_name = volume_name
        self.volume_path = volume_path
        self.is_dir = is_dir

    @property
    def volume_path(self) -> str:
        return self._volume_path

    @volume_path.setter
    def volume_path(self, value: str) -> None:
        self._volume_path = value.replace("//", "/")

    @classmethod
    def parse(cls, value: str) -> "RemotePath":
        scheme, volume_path = value.split("://", 1)
        try:
            volume_name, volume_path = volume_path.split("/", 1)
            if not volume_path:
                volume_path = "/"
        except ValueError:
            volume_name = volume_path
            volume_path = ""
        return cls(scheme, volume_name, volume_path)

    def __str__(self) -> str:
        return f"{self.scheme}://{self.volume_name}/{self.volume_path}"

    def __truediv__(self, other: Union["RemotePath", str]) -> "RemotePath":
        path = ""
        if isinstance(other, str):
            path = other
        elif isinstance(other, RemotePath):
            path = other.volume_path

        return RemotePath(
            self.scheme,
            self.volume_name,
            os.path.join(self.volume_path, path),
            other.is_dir if isinstance(other, RemotePath) else self.is_dir,
        )

    @property
    def name(self) -> str:
        return os.path.basename(self.volume_path)

    @property
    def path(self) -> str:
        if self.volume_path == "":
            return self.volume_name + "/"
        return os.path.join(self.volume_name, self.volume_path)


class PathTypeConverter(click.ParamType):
    """
    VolumePath Click type converts a string into a str, Path, or RemotePath object.
    """

    def __init__(self, version_option_name: str = "version") -> None:
        self.version_option_name = version_option_name

    def convert(
        self,
        value: str,
        param: Optional[click.Parameter] = None,
        ctx: Optional[click.Context] = None,
    ) -> Union[str, Path, RemotePath]:
        if "://" in value:
            return RemotePath.parse(value)
        return Path(value)


class RemoteHandler(abc.ABC):
    def __init__(self, **kwargs) -> None:
        pass

    @abc.abstractmethod
    def upload(self, local_path: Path, remote_path: RemotePath) -> None:
        raise NotImplementedError

    @abc.abstractmethod
    def download(self, remote_path: RemotePath, local_path: Path) -> None:
        raise NotImplementedError


class Beta9Handler(RemoteHandler):
    def __init__(
        self, service: VolumeServiceStub, progress: Optional[CustomProgress] = None, **kwargs
    ) -> None:
        self.service = service
        self.progress = progress

    def list_dir(self, remote_path: RemotePath, recursive: bool = False) -> List[RemotePath]:
        path = remote_path.path
        if recursive:
            path = f"{path}/**".replace("//", "/")

        res = self.service.list_path(ListPathRequest(path=path))
        if not res.ok:
            raise ListPathError(remote_path.path, res.err_msg)

        return [
            RemotePath(remote_path.scheme, remote_path.volume_name, p.path, is_dir=p.is_dir)
            for p in res.path_infos
        ]

    def is_dir(self, remote_path: RemotePath) -> bool:
        if remote_path.volume_path == "":
            return True

        res = self.service.stat_path(StatPathRequest(path=remote_path.path))
        if not res.ok:
            raise StatPathError(remote_path.path, res.err_msg)

        return res.path_info.is_dir if not res.err_msg else False

    def upload(self, local_path: Path, remote_path: RemotePath) -> None:
        # Recursively upload directories
        if local_path.is_dir():
            for item in local_path.iterdir():
                if remote_path.volume_path.endswith("/"):
                    volume_path = f"{remote_path.volume_path}{local_path.name}/{item.name}"
                else:
                    volume_path = f"{remote_path.volume_path}/{item.name}"

                volume_path = volume_path.lstrip("/")
                remote_subpath = RemotePath(
                    scheme=remote_path.scheme,
                    volume_name=remote_path.volume_name,
                    volume_path=volume_path,
                )
                self.upload(item, remote_subpath)
        else:
            # Adjust remote_path for single-file upload
            if remote_path.volume_path.endswith("/"):
                volume_path = f"{remote_path.volume_path}{local_path.name}".lstrip("/")
                remote_path.volume_path = volume_path
            elif self.is_dir(remote_path):
                volume_path = f"{remote_path.volume_path}/{local_path.name}".lstrip("/")
                remote_path.volume_path = volume_path
            elif local_path.suffix and "." not in remote_path.volume_path:
                ext = ".".join(local_path.suffixes)
                volume_path = f"{remote_path.volume_path}{ext}".lstrip("/")
                remote_path.volume_path = volume_path

            progress_callback, completion_callback = self._setup_callbacks(remote_path)
            beta9_upload(
                service=self.service,
                file_path=local_path,
                remote_path=remote_path,
                progress_callback=progress_callback,
                completion_callback=completion_callback,
            )

    def download(self, remote_path: RemotePath, local_path: Path) -> None:
        # Recursively download directories
        if self.is_dir(remote_path):
            local_path.mkdir(parents=True, exist_ok=True)

            for rpath in self.list_dir(remote_path, recursive=True):
                lpath = local_path / rpath.volume_path
                if not remote_path.volume_path.endswith("/"):
                    lpath = local_path / rpath.name

                self.download(rpath, lpath)
        else:
            # Adjust local_path for single-file download
            if local_path.is_dir():
                local_path = local_path / Path(remote_path.volume_path).name
            local_path.parent.mkdir(parents=True, exist_ok=True)

            callback, _ = self._setup_callbacks(local_path.relative_to(Path.cwd()))
            beta9_download(
                service=self.service,
                remote_path=remote_path,
                file_path=local_path,
                callback=callback,
            )

    def _setup_callbacks(
        self,
        task_name: Any,
    ) -> Tuple[Optional[ProgressCallbackType], Optional[CompletionCallbackType]]:
        if self.progress is None:
            return None, None

        p = self.progress
        task_id = p.add_task(task_name)

        def progress_callback(total: int, advance: int):
            """
            Updates the progress bar with the current status of the upload.
            """
            p.update(task_id=task_id, total=total, advance=advance)

        @contextmanager
        def completion_callback():
            """
            Shows status while the upload is being processed server-side.
            """
            p.stop()

            from . import terminal

            with terminal.progress("Working...") as s:
                yield s

            # Move cursor up 2x, clear line, and redraw the progress bar
            terminal.print("\033[A\033[A\r", highlight=False)
            p.start()

        return progress_callback, completion_callback


class S3Handler(RemoteHandler):
    def upload(self, local_path: Path, remote_path: RemotePath) -> None:
        raise NotImplementedError

    def download(self, remote_path: RemotePath, local_path: Path) -> None:
        raise NotImplementedError


class HuggingFaceHandler(RemoteHandler):
    def upload(self, local_path: Path, remote_path: RemotePath) -> None:
        raise NotImplementedError

    def download(self, remote_path: RemotePath, local_path: Path) -> None:
        raise NotImplementedError


def get_remote_handler(scheme: str, **kwargs: Any) -> RemoteHandler:
    from .config import get_settings

    this = get_settings().name.lower()
    handlers = {
        this: Beta9Handler,
        "hf": HuggingFaceHandler,
        "s3": S3Handler,
    }

    if scheme not in handlers:
        raise ValueError(
            f"Protocol '{scheme}://' is not supported. Supported protocols are {', '.join(handlers)}."
        )

    return handlers[scheme](**kwargs)


def copy(src, dst, **kwargs: Any):
    # Resolve local paths
    dst_path = Path(dst).resolve() if not isinstance(dst, RemotePath) else dst
    if isinstance(dst_path, Path) and str(dst) == ".":
        dst_path = Path.cwd()

    # Determine handler based on source or destination type
    if isinstance(src, RemotePath) and isinstance(dst_path, Path):
        handler = get_remote_handler(src.scheme, **kwargs)
        handler.download(src, dst_path)
    elif isinstance(src, Path) and isinstance(dst_path, RemotePath):
        handler = get_remote_handler(dst.scheme, **kwargs)
        handler.upload(src, dst_path)
    else:
        raise ValueError("Invalid source and destination types.")

================================================================================
# File: runner/__init__.py
# Path: src/beta9/runner/__init__.py
================================================================================


================================================================================
# File: runner/bot/__init__.py
# Path: src/beta9/runner/bot/__init__.py
================================================================================


================================================================================
# File: runner/bot/transition.py
# Path: src/beta9/runner/bot/transition.py
================================================================================

import json
import os
import time
import traceback
from typing import Any, Dict, get_args, get_origin

from ...abstractions.experimental.bot.bot import BotEventType
from ...abstractions.experimental.bot.types import BotContext
from ...channel import Channel, handle_error, pass_channel
from ...clients.bot import (
    BotServiceStub,
    Marker,
    MarkerField,
    PopBotTaskRequest,
    PopBotTaskResponse,
    PushBotEventRequest,
    PushBotMarkersRequest,
    PushBotMarkersRequestMarkerList,
    PushBotMarkersResponse,
)
from ...clients.gateway import (
    EndTaskRequest,
    GatewayServiceStub,
    StartTaskRequest,
    StartTaskResponse,
)
from ...exceptions import RunnerException, TaskEndError
from ...logging import json_output_interceptor
from ...runner.common import FunctionHandler, config, end_task_and_send_callback
from ...type import TaskStatus


class BotTransitionResult:
    def __init__(self, outputs: Dict[str, Any], exception: BaseException):
        self.outputs = outputs
        self.exception = exception


class BotTransition:
    def __init__(self, session_id: str, transition_name: str, bot_stub: BotServiceStub) -> None:
        self.handler = FunctionHandler(handler_path=config.handler)
        self.session_id = session_id
        self.transition_name = transition_name
        self.bot_stub = bot_stub

    def _format_inputs(self, markers: Dict[str, Any]) -> Dict[str, Any]:
        expected_inputs = self.handler.handler.config.get("inputs", {})
        formatted_inputs = {}

        for marker_class in expected_inputs.keys():
            marker_name = marker_class.__name__

            if marker_name in markers.keys():
                marker_data = markers[marker_name]

                if marker_class not in formatted_inputs.keys():
                    formatted_inputs[marker_class] = []

                for marker in marker_data.markers:
                    fields_dict = {}

                    for field in marker.fields:
                        field_name = field.field_name
                        field_value = field.field_value

                        # Get field type from marker_class
                        if field_name in marker_class.model_fields:
                            field_type = marker_class.model_fields[field_name].annotation
                        else:
                            field_type = str  # default to string if field is not defined

                        # Try to convert field_value to field_type
                        try:
                            if get_origin(field_type) is dict and isinstance(field_value, str):
                                converted_value = json.loads(field_value)
                            else:
                                converted_value = field_type(field_value)
                        except (ValueError, TypeError, json.JSONDecodeError):
                            converted_value = field_value

                        fields_dict[field_name] = converted_value

                    formatted_inputs[marker_class].append(marker_class(**fields_dict))

        return formatted_inputs

    def _format_outputs(self, outputs: Dict[str, Any]) -> Dict[str, Any]:
        expected_outputs = self.handler.handler.config.get("outputs", [])
        formatted_outputs = {}

        if not isinstance(outputs, dict):
            return formatted_outputs

        for output, markers in outputs.items():
            if output not in expected_outputs:
                continue

            location_name = output.__name__
            marker_list = []

            if not isinstance(markers, list):
                if hasattr(markers, "model_dump"):
                    markers = [markers]

            for marker in markers:
                fields = []
                marker_dict = marker.model_dump()
                marker_annotations = marker.__annotations__

                for field_name, field_value in marker_dict.items():
                    field_type = marker_annotations.get(field_name, str)

                    # Decompose the field_type to handle complex types like Optional[int]
                    origin_type = get_origin(field_type) or field_type
                    args = get_args(field_type)

                    # Determine if the field_type is a subclass of the desired types
                    if isinstance(origin_type, type) and issubclass(
                        origin_type, (bool, int, float, list, dict)
                    ):
                        converted_value = json.dumps(field_value)
                    elif args and all(
                        isinstance(arg, type) and issubclass(arg, (bool, int, float, list, dict))
                        for arg in args
                    ):
                        converted_value = json.dumps(field_value)
                    else:
                        converted_value = str(field_value)

                    fields.append(MarkerField(field_name=field_name, field_value=converted_value))

                # Create a Marker for each set of fields
                marker_list.append(Marker(location_name=location_name, fields=fields))

            # Assign a single PushBotMarkersRequestMarkerList for all markers at this location
            formatted_outputs[location_name] = PushBotMarkersRequestMarkerList(markers=marker_list)

        return formatted_outputs

    def run(self, inputs: Dict[str, Any]) -> BotTransitionResult:
        result = BotTransitionResult(outputs={}, exception=None)
        context: BotContext = BotContext.new(
            config=config,
            task_id=config.task_id,
            session_id=self.session_id,
            transition_name=self.transition_name,
            bot_stub=self.bot_stub,
        )

        context.push_event(
            event_type=BotEventType.TRANSITION_STARTED, event_value=self.transition_name
        )

        try:
            outputs = self.handler(context=context, inputs=self._format_inputs(inputs))
            outputs = self._format_outputs(outputs)
            result.outputs = outputs
        except BaseException as exc:
            error_message = (
                f"Error occurred in transition<{context.transition_name}>: {traceback.format_exc()}"
            )
            print(error_message)
            context.push_event(event_type=BotEventType.AGENT_MESSAGE, event_value=error_message)
            result.exception = exc

        return result


@json_output_interceptor(task_id=config.task_id)
@handle_error()
@pass_channel
def main(channel: Channel):
    bot_stub: BotServiceStub = BotServiceStub(channel)
    session_id: str = os.environ.get("SESSION_ID")
    transition_name: str = os.environ.get("TRANSITION_NAME")

    bt: BotTransition = BotTransition(
        session_id=session_id, transition_name=transition_name, bot_stub=bot_stub
    )
    gateway_stub: GatewayServiceStub = GatewayServiceStub(channel)
    task_id: str = config.task_id

    task_args: PopBotTaskResponse = bot_stub.pop_bot_task(
        PopBotTaskRequest(
            stub_id=config.stub_id,
            session_id=session_id,
            transition_name=transition_name,
            task_id=task_id,
        )
    )
    if not task_args.ok:
        raise RunnerException("Failed to retrieve task.")

    start_time = time.time()
    start_task_response: StartTaskResponse = gateway_stub.start_task(
        StartTaskRequest(task_id=task_id, container_id=config.container_id)
    )
    if not start_task_response.ok:
        raise RunnerException("Failed to start task.")

    task_status = TaskStatus.Complete

    # Run the transition
    inputs = task_args.markers
    result = bt.run(inputs=inputs)
    if result.exception:
        task_status = TaskStatus.Error
    else:
        push_bot_markers_response: PushBotMarkersResponse = bot_stub.push_bot_markers(
            PushBotMarkersRequest(
                stub_id=config.stub_id,
                session_id=session_id,
                markers=result.outputs,
                source_task_id=task_id,
            )
        )
        if not push_bot_markers_response.ok:
            raise RunnerException("Failed to push markers.")

    end_task_response = end_task_and_send_callback(
        gateway_stub=gateway_stub,
        payload={},
        end_task_request=EndTaskRequest(
            task_id=task_id,
            task_duration=time.time() - start_time,
            task_status=task_status,
            container_id=config.container_id,
            container_hostname=config.container_hostname,
            keep_warm_seconds=config.keep_warm_seconds,
        ),
    )

    if not end_task_response.ok:
        raise TaskEndError

    bot_stub.push_bot_event(
        PushBotEventRequest(
            stub_id=config.stub_id,
            session_id=session_id,
            event_type=BotEventType.TRANSITION_COMPLETED
            if task_status == TaskStatus.Complete
            else BotEventType.TRANSITION_FAILED,
            event_value=transition_name,
            metadata={
                "task_id": task_id,
                "session_id": session_id,
                "transition_name": transition_name,
            },
        )
    )


if __name__ == "__main__":
    main()

================================================================================
# File: runner/common.py
# Path: src/beta9/runner/common.py
================================================================================

import asyncio
import importlib
import inspect
import json
import os
import sys
import time
import traceback
from concurrent.futures import ThreadPoolExecutor
from contextlib import contextmanager
from dataclasses import dataclass
from functools import wraps
from multiprocessing import Value
from pathlib import Path
from typing import Any, Callable, Dict, Optional, Union

import cloudpickle
import requests
from starlette.responses import Response

from ..clients.gateway import (
    EndTaskRequest,
    EndTaskResponse,
    GatewayServiceStub,
    SignPayloadRequest,
    SignPayloadResponse,
)
from ..env import is_remote
from ..exceptions import RunnerException
from ..schema import Schema, ValidationError

USER_CODE_DIR = "/mnt/code"
USER_VOLUMES_DIR = "/volumes"
USER_OUTPUTS_DIR = "/outputs"
USER_CACHE_DIR = "/cache"

PICKLE_SUFFIX = ".pkl"


@dataclass
class Config:
    container_id: str
    container_hostname: str
    stub_id: str
    stub_type: str
    workers: int
    keep_warm_seconds: int
    timeout: int
    python_version: str
    handler: str
    on_start: str
    callback_url: str
    task_id: str
    bind_port: int
    checkpoint_enabled: bool
    volume_cache_map: Dict
    inputs: Dict
    outputs: Dict

    @classmethod
    def load_from_env(cls) -> "Config":
        container_id = os.getenv("CONTAINER_ID")
        container_hostname = os.getenv("CONTAINER_HOSTNAME")
        stub_id = os.getenv("STUB_ID")
        stub_type = os.getenv("STUB_TYPE")
        workers = int(os.getenv("WORKERS", 1))
        keep_warm_seconds = float(os.getenv("KEEP_WARM_SECONDS", 10))
        python_version = os.getenv("PYTHON_VERSION")
        handler = os.getenv("HANDLER")
        on_start = os.getenv("ON_START")
        callback_url = os.getenv("CALLBACK_URL")
        task_id = os.getenv("TASK_ID")
        bind_port = int(os.getenv("BIND_PORT"))
        timeout = int(os.getenv("TIMEOUT", 180))
        checkpoint_enabled = os.getenv("CHECKPOINT_ENABLED", "false").lower() == "true"
        volume_cache_map = json.loads(os.getenv("VOLUME_CACHE_MAP", "{}"))
        inputs = json.loads(os.getenv("BETA9_INPUTS", "{}"))
        outputs = json.loads(os.getenv("BETA9_OUTPUTS", "{}"))

        if workers <= 0:
            workers = 1

        if not container_id or not stub_id:
            raise RunnerException("Invalid runner environment")

        return cls(
            container_id=container_id,
            container_hostname=container_hostname,
            stub_id=stub_id,
            stub_type=stub_type,
            workers=workers,
            keep_warm_seconds=keep_warm_seconds,
            python_version=python_version,
            handler=handler,
            on_start=on_start,
            callback_url=callback_url,
            task_id=task_id,
            bind_port=bind_port,
            timeout=timeout,
            checkpoint_enabled=checkpoint_enabled,
            volume_cache_map=volume_cache_map,
            inputs=inputs,
            outputs=outputs,
        )


config: Union[Config, None] = None
if is_remote():
    config: Config = Config.load_from_env()


class ParentAbstractionProxy:
    """
    Class to allow handlers to access parent class variables through attribute or dictionary access
    """

    def __init__(self, parent):
        self._parent = parent

    def __getitem__(self, key):
        return getattr(self._parent, key)

    def __setitem__(self, key, value):
        setattr(self._parent, key, value)

    def __getattr__(self, key):
        return getattr(self._parent, key)

    def __setattr__(self, key, value):
        if key == "_parent":
            super().__setattr__(key, value)
        else:
            setattr(self._parent, key, value)


@dataclass
class FunctionContext:
    """
    A dataclass used to store various useful fields you might want to access in your entry point logic
    """

    container_id: Optional[str] = None
    stub_id: Optional[str] = None
    stub_type: Optional[str] = None
    callback_url: Optional[str] = None
    task_id: Optional[str] = None
    timeout: Optional[int] = None
    on_start_value: Optional[Any] = None
    bind_port: int = 0
    python_version: str = ""

    @classmethod
    def new(
        cls,
        *,
        config: Config,
        task_id: Optional[str],
        on_start_value: Optional[Any] = None,
    ) -> "FunctionContext":
        """
        Create a new instance of FunctionContext, to be passed directly into a function handler
        """
        return cls(
            container_id=config.container_id,
            stub_id=config.stub_id,
            stub_type=config.stub_type,
            callback_url=config.callback_url,
            python_version=config.python_version,
            task_id=task_id,
            bind_port=config.bind_port,
            timeout=config.timeout,
            on_start_value=on_start_value,
        )


workers_ready = None
if is_remote():
    workers_ready = Value("i", 0)


class FunctionHandler:
    """
    Helper class for loading user entry point functions
    """

    def __init__(self, handler_path: Optional[str] = None) -> None:
        self.pass_context: bool = False
        self.handler_path: Optional[str] = handler_path
        self.handler: Optional[Callable] = None
        self.is_async: bool = False
        self.inputs: Optional[Schema] = None
        self.outputs: Optional[Schema] = None
        self._load()

    @contextmanager
    def importing_user_code(self):
        os.environ["BETA9_IMPORTING_USER_CODE"] = "true"
        yield
        del os.environ["BETA9_IMPORTING_USER_CODE"]

    def _load_pickled_function(self, module_path: str) -> Callable:
        """Load a pickled function using cloudpickle."""
        try:
            with open(module_path, "rb") as f:
                func = cloudpickle.load(f)
                if not callable(func):
                    raise RunnerException("Loaded object is not callable")
                return func
        except RunnerException:
            raise
        except BaseException as e:
            raise RunnerException(
                f"Failed to load pickled function: {traceback.format_exc()}"
            ) from e

    def _load(self):
        if sys.path[0] != USER_CODE_DIR:
            sys.path.insert(0, USER_CODE_DIR)

        if config.inputs:
            self.inputs = Schema.from_dict(config.inputs)

        if config.outputs:
            self.outputs = Schema.from_dict(config.outputs)

        try:
            module = None
            func = None

            if self.handler_path is not None:
                module, func = self.handler_path.split(":")
            else:
                module, func = config.handler.split(":")

            with self.importing_user_code():
                if Path(module).suffix == PICKLE_SUFFIX:
                    # Handle pickled functions
                    self.handler = self._load_pickled_function(module)
                else:
                    # Handle standard modules (i.e. .py files)
                    target_module = importlib.import_module(module)
                    self.handler = getattr(target_module, func)

            # Check if handler is a wrapped function or direct function
            target_func = getattr(self.handler, "func", self.handler)
            self.signature = inspect.signature(target_func)
            self.pass_context = "context" in self.signature.parameters
            self.is_async = asyncio.iscoroutinefunction(target_func)
        except BaseException:
            raise RunnerException(f"Error loading handler: {traceback.format_exc()}")

    def __call__(self, context: FunctionContext, *args: Any, **kwargs: Any) -> Any:
        if self.handler is None:
            raise Exception("Handler not configured.")

        handler_args = args
        handler_kwargs = kwargs

        if self.inputs is not None:
            if len(kwargs) == 1:
                key, value = next(iter(kwargs.items()))
                if isinstance(value, dict):
                    input_data = value
                else:
                    # Wrap the value in a dict with the expected field name
                    input_data = {key: value}
            else:
                input_data = kwargs

            try:
                parsed_inputs = self.inputs.new(input_data)
            except ValidationError as e:
                print(f"Input validation error: {e}")
                return e.to_dict()

            handler_args = (parsed_inputs,)
            handler_kwargs = {}

        if self.pass_context:
            handler_kwargs["context"] = context

        os.environ["TASK_ID"] = context.task_id or ""
        result = self.handler(*handler_args, **handler_kwargs)

        if self.outputs is not None:
            if result is None:
                result = {}

            try:
                parsed_outputs = self.outputs.new(result)
            except ValidationError as e:
                print(f"Output validation error: {e}")
                return e.to_dict()

            return parsed_outputs.dump()

        return result

    @property
    def parent_abstraction(self) -> ParentAbstractionProxy:
        if not hasattr(self, "_parent_abstraction"):
            self._parent_abstraction = ParentAbstractionProxy(self.handler.parent)
        return self._parent_abstraction


def execute_lifecycle_method(name: str) -> Union[Any, None]:
    """Executes a container lifecycle method defined by the user and return it's value"""

    if sys.path[0] != USER_CODE_DIR:
        sys.path.insert(0, USER_CODE_DIR)

    func: str = getattr(config, name)
    if func == "" or func is None:
        return None

    start_time = time.time()
    print(f"Running {name} func: {func}")
    try:
        module, func = func.split(":")
        target_module = importlib.import_module(module)
        method = getattr(target_module, func)
        result = method()
        duration = time.time() - start_time

        print(f"{name} func complete, took: {duration}s")
        return result
    except BaseException:
        raise RunnerException()


# TODO: add retry behavior directly in dynamically generated GRPC stubs
def retry_grpc_call(
    *, exception_to_check: Exception, tries: int = 4, delay: int = 5, backoff: int = 2
) -> Any:
    def _retry_decorator(f):
        @wraps(f)
        def f_to_retry(*args, **kwargs):
            mtries, mdelay = tries, delay

            while mtries > 1:
                try:
                    return f(*args, **kwargs)
                except exception_to_check:
                    print(f"Unexpected GRPC error, retrying in {mdelay} seconds...")
                    time.sleep(mdelay)
                    mtries -= 1
                    mdelay *= backoff

            return f(*args, **kwargs)

        return f_to_retry

    return _retry_decorator


@retry_grpc_call(exception_to_check=BaseException, tries=4, delay=5, backoff=2)
def end_task_and_send_callback(
    *,
    gateway_stub: GatewayServiceStub,
    payload: Any,
    end_task_request: EndTaskRequest,
    override_callback_url: Optional[str] = None,
) -> EndTaskResponse:
    resp = gateway_stub.end_task(end_task_request)

    send_callback(
        gateway_stub=gateway_stub,
        context=FunctionContext.new(
            config=config,
            task_id=end_task_request.task_id,
            on_start_value=None,
        ),
        payload=payload,
        task_status=end_task_request.task_status,
        override_callback_url=override_callback_url,
    )

    return resp


def send_callback(
    *,
    gateway_stub: GatewayServiceStub,
    context: FunctionContext,
    payload: Any,
    task_status: str,
    override_callback_url: Optional[str] = None,
) -> None:
    """
    Send a signed callback request to an external host defined by the user
    """

    callback_url = override_callback_url or context.callback_url
    if not callback_url:
        return

    body = {}
    headers = {}

    # Serialize callback payload to correct format
    use_json = True
    body = {"data": payload}
    if isinstance(payload, Response):
        body = {"data": payload.body}
        headers = payload.headers
        use_json = False

    # Sign callback payload
    sign_payload_resp: SignPayloadResponse = gateway_stub.sign_payload(
        SignPayloadRequest(payload=bytes(json.dumps(body), "utf-8"))
    )

    print(f"Sending data to callback: {callback_url}")
    headers = {}
    headers = {
        **headers,
        "X-Task-ID": str(context.task_id),
        "X-Task-Status": str(task_status),
        "X-Task-Signature": sign_payload_resp.signature,
        "X-Task-Timestamp": str(sign_payload_resp.timestamp),
    }

    try:
        start = time.time()
        if use_json:
            requests.post(callback_url, json=body, headers=headers)
        else:
            requests.post(callback_url, data=body, headers=headers)

        print(f"Callback request took {time.time() - start} seconds")
    except BaseException:
        print(f"Unable to send callback: {traceback.format_exc()}")


def serialize_result(result: Any) -> bytes:
    try:
        return json.dumps(result).encode("utf-8")
    except Exception:
        print(f"Warning - Error serializing task result: {traceback.format_exc()}")
        return None


def has_asgi3_signature(func) -> bool:
    sig = inspect.signature(func)
    own_parameters = {name for name in sig.parameters if name != "self"}
    return own_parameters == {"scope", "receive", "send"}


def is_asgi3(app: Any) -> bool:
    """Return whether 'app' corresponds to an ASGI3 callable."""
    if inspect.isclass(app):
        constructor = app.__init__
        return has_asgi3_signature(constructor) and hasattr(app, "__await__")

    if inspect.isfunction(app):
        return inspect.iscoroutinefunction(app) and has_asgi3_signature(app)

    try:
        call = app.__call__
    except AttributeError:
        return False
    else:
        return inspect.iscoroutinefunction(call) and has_asgi3_signature(call)


class ThreadPoolExecutorOverride(ThreadPoolExecutor):
    def __exit__(self, *_, **__):
        try:
            # cancel_futures added in 3.9
            self.shutdown(cancel_futures=True)
        except Exception:
            pass


CHECKPOINT_SIGNAL_FILE = "/criu/READY_FOR_CHECKPOINT"
CHECKPOINT_COMPLETE_FILE = "/criu/CHECKPOINT_COMPLETE"
CHECKPOINT_CONTAINER_ID_FILE = "/criu/CONTAINER_ID"
CHECKPOINT_CONTAINER_HOSTNAME_FILE = "/criu/CONTAINER_HOSTNAME"


def wait_for_checkpoint():
    def _reload_config():
        # Once we have set the checkpoint signal file, wait for checkpoint to be complete before reloading the config
        while not Path(CHECKPOINT_COMPLETE_FILE).exists():
            time.sleep(1)

        # Reload config that may have changed during restore
        config.container_id = Path(CHECKPOINT_CONTAINER_ID_FILE).read_text()
        config.container_hostname = Path(CHECKPOINT_CONTAINER_HOSTNAME_FILE).read_text()

    with workers_ready.get_lock():
        workers_ready.value += 1

    if workers_ready.value == config.workers:
        Path(CHECKPOINT_SIGNAL_FILE).touch(exist_ok=True)
        return _reload_config()

    while True:
        with workers_ready.get_lock():
            if workers_ready.value == config.workers:
                break
        time.sleep(1)

    return _reload_config()

================================================================================
# File: runner/container.py
# Path: src/beta9/runner/container.py
================================================================================

import base64
import os
import signal
import subprocess
import sys
from typing import Union

from ..aio import run_sync
from ..channel import Channel, with_runner_context
from ..clients.gateway import EndTaskRequest, GatewayServiceStub, StartTaskRequest
from ..logging import StdoutJsonInterceptor
from ..runner.common import config
from ..type import TaskStatus
from .common import FunctionContext, end_task_and_send_callback, send_callback


class ContainerManager:
    def __init__(self, cmd: str) -> None:
        self.process: Union[subprocess.Popen, None] = None
        self.pid: int = os.getpid()
        self.exit_code: int = 0
        self.task_id: str = os.getenv("TASK_ID")
        self.killed: bool = False

        signal.signal(signal.SIGTERM, self.shutdown)

    @with_runner_context
    def start(self, channel: Channel):
        async def _run():
            with StdoutJsonInterceptor(task_id=self.task_id):
                stub = GatewayServiceStub(channel)
                stub.start_task(
                    StartTaskRequest(
                        task_id=self.task_id,
                        container_id=config.container_id,
                    )
                )

                self.process = subprocess.Popen(
                    ["/bin/bash", "-c", cmd],
                    shell=False,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    env=os.environ,
                )

                while self.process.poll() is None:
                    line = self.process.stdout.readline()
                    if not line:
                        continue
                    print(line.strip().decode("utf-8"))

                if not self.killed:
                    end_task_and_send_callback(
                        gateway_stub=stub,
                        payload={},
                        end_task_request=EndTaskRequest(
                            task_id=self.task_id,
                            container_id=config.container_id,
                            task_status=TaskStatus.Complete,
                        ),
                    )
                else:
                    send_callback(
                        gateway_stub=stub,
                        context=FunctionContext.new(
                            config=config,
                            task_id=self.task_id,
                            on_start_value=None,
                        ),
                        payload={},
                        task_status=TaskStatus.Cancelled,
                    )

        run_sync(_run())

    def shutdown(self, *_, **__):
        if self.process:
            self.killed = True
            self.process.kill()


if __name__ == "__main__":
    cmd = base64.b64decode(sys.argv[1]).decode("utf-8")
    print(f"Running command: {cmd}")
    container = ContainerManager(cmd)
    container.start()

================================================================================
# File: runner/endpoint.py
# Path: src/beta9/runner/endpoint.py
================================================================================

import asyncio
import logging
import os
import signal
import traceback
from contextlib import asynccontextmanager
from http import HTTPStatus
from typing import Any, Dict, Optional, Tuple, Union

from fastapi import Depends, FastAPI, Request
from fastapi.responses import JSONResponse, Response
from gunicorn.app.base import Arbiter, BaseApplication
from starlette.applications import Starlette
from starlette.types import ASGIApp
from uvicorn.workers import UvicornWorker

from ..abstractions.base.runner import (
    ASGI_SERVE_STUB_TYPE,
    ASGI_STUB_TYPE,
    ENDPOINT_SERVE_STUB_TYPE,
)
from ..channel import runner_context
from ..clients.gateway import (
    GatewayServiceStub,
)
from ..middleware import (
    TaskLifecycleData,
    TaskLifecycleMiddleware,
    WebsocketTaskLifecycleMiddleware,
)
from ..runner.common import (
    FunctionContext,
    FunctionHandler,
    execute_lifecycle_method,
    wait_for_checkpoint,
)
from ..runner.common import config as cfg
from ..type import LifeCycleMethod, TaskStatus
from .common import is_asgi3


class EndpointFilter(logging.Filter):
    def filter(self, record: logging.LogRecord) -> bool:
        if record.args:
            return record.args and len(record.args) >= 3 and record.args[2] != "/health"
        return True


class GunicornArbiter(Arbiter):
    def init_signals(self):
        super(GunicornArbiter, self).init_signals()

        # Add a custom signal handler to kill gunicorn master process with non-zero exit code.
        signal.signal(signal.SIGUSR1, self.handle_usr1)
        signal.siginterrupt(signal.SIGUSR1, True)

    # Override default usr1 handler to force shutdown server when forked processes crash
    # during startup
    def handle_usr1(self, sig, frame):
        os._exit(1)


class GunicornApplication(BaseApplication):
    def __init__(self, app: ASGIApp, options: Optional[Dict] = None) -> None:
        self.options = options or {}
        self.application = app
        super().__init__()

    def load_config(self) -> None:
        for key, value in self.options.items():
            if value is not None:
                self.cfg.set(key.lower(), value)

    def load(self) -> ASGIApp:
        return Starlette()  # Return a base Starlette app -- which will be replaced post-fork

    def run(self):
        GunicornArbiter(self).run()

    @staticmethod
    def post_fork_initialize(_, worker: UvicornWorker):
        logger = logging.getLogger("uvicorn.access")
        logger.addFilter(EndpointFilter())

        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
        logger.addHandler(handler)
        logger.propagate = False

        try:
            mg = EndpointManager(logger=logger, worker=worker)
            asgi_app: ASGIApp = mg.app

            # Override the default starlette app
            worker.app.callable = asgi_app

            # If checkpointing is enabled, wait for all workers to be ready before creating a checkpoint
            if cfg.checkpoint_enabled:
                wait_for_checkpoint()

        except EOFError:
            return
        except BaseException:
            logger.exception("Exiting worker due to startup error")
            if cfg.stub_type in [ENDPOINT_SERVE_STUB_TYPE, ASGI_SERVE_STUB_TYPE]:
                return

            # We send SIGUSR1 to indicate to the gunicorn master that the server should shut down completely
            # since our asgi_app callable is erroring out.
            os.kill(os.getppid(), signal.SIGUSR1)
            os._exit(1)


class OnStartMethodHandler:
    def __init__(self, worker: UvicornWorker) -> None:
        self._is_running = True
        self._worker = worker

    async def start(self):
        loop = asyncio.get_running_loop()
        task = loop.create_task(self._keep_worker_alive())
        result = await loop.run_in_executor(None, execute_lifecycle_method, LifeCycleMethod.OnStart)
        self._is_running = False
        await task
        return result

    async def _keep_worker_alive(self) -> None:
        while self._is_running:
            self._worker.notify()
            await asyncio.sleep(1)


def get_task_lifecycle_data(request: Request):
    return request.state.task_lifecycle_data


class EndpointManager:
    @asynccontextmanager
    async def lifespan(self, _: FastAPI):
        with runner_context() as channel:
            self.app.state.gateway_stub = GatewayServiceStub(channel)
            yield

    def __init__(self, logger: logging.Logger, worker: UvicornWorker) -> None:
        self.logger = logger
        self.pid: int = os.getpid()
        self.exit_code: int = 0

        self.handler: FunctionHandler = FunctionHandler()
        self.on_start_value = asyncio.run(OnStartMethodHandler(worker).start())

        self.is_asgi_stub = ASGI_STUB_TYPE in cfg.stub_type
        if self.is_asgi_stub:
            context = FunctionContext.new(
                config=cfg,
                task_id=None,
                on_start_value=self.on_start_value,
            )

            app: Union[FastAPI, None] = None
            internal_asgi_app = getattr(self.handler.handler.func, "internal_asgi_app", None)
            if internal_asgi_app is not None:
                app = internal_asgi_app
                app.context = context
                app.handler = self.handler
            else:
                app = self.handler(context)

            self.app = app
            if not is_asgi3(self.app):
                raise ValueError("Invalid ASGI app returned from handler")

            self.app.router.lifespan_context = self.lifespan
        else:
            self.app = FastAPI(lifespan=self.lifespan)

        self.app.add_middleware(TaskLifecycleMiddleware)
        self.app.add_middleware(WebsocketTaskLifecycleMiddleware)

        # Register signal handlers
        signal.signal(signal.SIGTERM, self.shutdown)

        if hasattr(self.app, "get"):

            @self.app.get("/health")
            async def health():
                return Response(status_code=HTTPStatus.OK)
        else:
            from starlette.responses import Response as StarletteResponse
            from starlette.routing import Route

            async def health(request):
                return StarletteResponse(status_code=HTTPStatus.OK)

            self.app.router.routes.append(Route("/health", health))

        if self.is_asgi_stub:
            return

        @self.app.get("/")
        @self.app.post("/")
        async def function(
            request: Request,
            task_lifecycle_data: TaskLifecycleData = Depends(get_task_lifecycle_data),
        ):
            task_id = request.headers.get("X-TASK-ID")
            payload = await request.json()

            status_code = HTTPStatus.OK
            task_lifecycle_data.result, error = await self._call_function(
                task_id=task_id, payload=payload
            )

            if error:
                task_lifecycle_data.status = TaskStatus.Error
                status_code = HTTPStatus.INTERNAL_SERVER_ERROR

            kwargs = payload.get("kwargs", {})
            if kwargs:
                task_lifecycle_data.override_callback_url = kwargs.get("callback_url")

            return self._create_response(body=task_lifecycle_data.result, status_code=status_code)

    def _create_response(self, *, body: Any, status_code: int = HTTPStatus.OK) -> Response:
        if isinstance(body, Response):
            return body

        try:
            return JSONResponse(body, status_code=status_code)
        except BaseException:
            self.logger.exception("Response serialization failed")
            return JSONResponse(
                {"error": traceback.format_exc()},
                status_code=HTTPStatus.INTERNAL_SERVER_ERROR,
            )

    async def _call_function(self, task_id: str, payload: dict) -> Tuple[Response, Any]:
        response_body = {}
        error = None

        args = payload.get("args", [])
        if args is None:
            args = []

        kwargs = payload.get("kwargs", {})
        if kwargs is None:
            kwargs = {}

        context = FunctionContext.new(
            config=cfg,
            task_id=task_id,
            on_start_value=self.on_start_value,
        )

        try:
            if self.handler.is_async:
                response_body = await self.handler(
                    context,
                    *args,
                    **kwargs,
                )
            else:
                response_body = self.handler(
                    context,
                    *args,
                    **kwargs,
                )
        except BaseException:
            exception = traceback.format_exc()
            print(exception)
            response_body = {"error": exception}
            error = exception

        return response_body, error

    def shutdown(self, signum=None, frame=None):
        os._exit(self.exit_code)


if __name__ == "__main__":
    options = {
        "bind": [f"[::]:{cfg.bind_port}"],
        "workers": cfg.workers,
        "worker_class": "uvicorn.workers.UvicornWorker",
        "loglevel": "info",
        "post_fork": GunicornApplication.post_fork_initialize,
        "timeout": cfg.timeout,
    }

    if os.environ.get("GUNICORN_NO_WAIT") == "true":
        options["graceful_timeout"] = 0

    GunicornApplication(Starlette(), options).run()

================================================================================
# File: runner/function.py
# Path: src/beta9/runner/function.py
================================================================================

import json
import os
import signal
import time
import traceback
from dataclasses import dataclass
from multiprocessing import Process
from typing import Any, Optional

import cloudpickle
import grpc

from ..channel import Channel, get_channel, handle_error, pass_channel
from ..clients.function import (
    FunctionGetArgsRequest,
    FunctionMonitorRequest,
    FunctionServiceStub,
    FunctionSetResultRequest,
)
from ..clients.gateway import (
    EndTaskRequest,
    GatewayServiceStub,
    StartTaskRequest,
    StartTaskResponse,
)
from ..config import get_config_context
from ..exceptions import (
    FunctionSetResultError,
    InvalidFunctionArgumentsError,
    InvalidRunnerEnvironmentError,
    TaskEndError,
    TaskStartError,
)
from ..logging import json_output_interceptor
from ..runner.common import (
    FunctionContext,
    FunctionHandler,
    config,
    end_task_and_send_callback,
    send_callback,
    serialize_result,
)
from ..type import TaskExitCode, TaskStatus


@dataclass
class InvokeResult:
    result: Optional[str] = None
    pickled_result: Optional[bytes] = None
    callback_url: Optional[str] = None
    exception: Optional[BaseException] = None


def _load_args(args: bytes) -> dict:
    try:
        return cloudpickle.loads(args)
    except BaseException:
        # If cloudpickle fails, fall back to JSON
        try:
            return json.loads(args.decode("utf-8"))
        except json.JSONDecodeError:
            raise InvalidFunctionArgumentsError


def _monitor_task(
    *,
    function_context: FunctionContext,
    env: dict,
) -> None:
    os.environ.update(env)
    config = get_config_context()
    parent_pid = os.getppid()

    def _monitor_stream() -> bool:
        """
        Returns True if the stream ended with no errors (and should be restarted),
        or False if a exit event occurred (cancellation, completion, timeout,
        or a connection issue that caused us to kill the parent process)
        """
        with get_channel(config) as channel:
            function_stub = FunctionServiceStub(channel)
            gateway_stub = GatewayServiceStub(channel)

            initial_backoff = 5
            max_retries = 5
            backoff = initial_backoff
            retry = 0

            while retry <= max_retries:
                try:
                    for response in function_stub.function_monitor(
                        FunctionMonitorRequest(
                            task_id=function_context.task_id,
                            stub_id=function_context.stub_id,
                            container_id=function_context.container_id,
                        )
                    ):
                        # If the task is cancelled then send a callback and exit
                        if response.cancelled:
                            print(f"Task cancelled: {function_context.task_id}")
                            send_callback(
                                gateway_stub=gateway_stub,
                                context=function_context,
                                payload={},
                                task_status=TaskStatus.Cancelled,
                            )
                            os.kill(parent_pid, signal.SIGTERM)
                            return False

                        # If the task is complete, exit
                        if response.complete:
                            return False

                        # If the task has timed out, send a timeout callback and exit
                        if response.timed_out:
                            print(f"Task timed out: {function_context.task_id}")
                            send_callback(
                                gateway_stub=gateway_stub,
                                context=function_context,
                                payload={},
                                task_status=TaskStatus.Timeout,
                            )
                            os.kill(parent_pid, signal.SIGTERM)
                            return False

                        # Reset retry state if a valid response was received
                        retry = 0
                        backoff = initial_backoff

                    # Reaching here means that the stream ended with no errors,
                    # which can occur during a rollout restart of the gateway
                    # returning True here tells the outer loop to restart the stream
                    return True

                except (grpc.RpcError, ConnectionRefusedError):
                    if retry == max_retries:
                        print("Lost connection to task monitor, exiting")
                        os.kill(parent_pid, signal.SIGABRT)
                        return False

                    time.sleep(backoff)
                    backoff *= 2
                    retry += 1

                except BaseException:
                    print(f"Unexpected error occurred in task monitor: {traceback.format_exc()}")
                    os.kill(parent_pid, signal.SIGABRT)
                    return False

    # Outer loop: restart only if the stream ended with no errors
    while True:
        should_restart = _monitor_stream()
        if not should_restart:
            # Exit condition encountered; exit the monitor task completely
            return

        # If we reached here, the stream ended with no errors;
        # so we should restart the monitoring stream


def _handle_sigterm(*args: Any, **kwargs: Any) -> None:
    os._exit(TaskExitCode.Success)


def _handle_sigabort(*args: Any, **kwargs: Any) -> None:
    os._exit(TaskExitCode.Error)


@json_output_interceptor(task_id=config.task_id)
@handle_error()
@pass_channel
def main(channel: Channel):
    function_stub: FunctionServiceStub = FunctionServiceStub(channel)
    gateway_stub: GatewayServiceStub = GatewayServiceStub(channel)
    task_id = config.task_id

    if not task_id:
        raise InvalidRunnerEnvironmentError

    container_id = config.container_id
    container_hostname = config.container_hostname

    # Start the task
    start_time = time.time()
    start_task_response = start_task(gateway_stub, task_id, container_id)
    if not start_task_response.ok:
        raise TaskStartError

    context = FunctionContext.new(config=config, task_id=task_id)

    # Start monitor_task process to send health checks
    env = os.environ.copy()

    signal.signal(signal.SIGTERM, _handle_sigterm)
    signal.signal(signal.SIGABRT, _handle_sigabort)

    monitor_process = Process(
        target=_monitor_task,
        kwargs={
            "function_context": context,
            "env": env,
        },
        daemon=True,
    )
    monitor_process.start()

    try:
        # Invoke the function and handle its result
        result = invoke_function(function_stub, context, task_id)
        if result.exception:
            handle_task_failure(gateway_stub, result, task_id, container_id, container_hostname)
            raise result.exception

        # End the task and send callback
        complete_task(
            gateway_stub,
            result,
            task_id,
            container_id,
            container_hostname,
            start_time,
        )
    finally:
        monitor_process.terminate()
        monitor_process.join(timeout=1)


def start_task(
    gateway_stub: GatewayServiceStub, task_id: str, container_id: str
) -> StartTaskResponse:
    return gateway_stub.start_task(StartTaskRequest(task_id=task_id, container_id=container_id))


def invoke_function(
    function_stub: FunctionServiceStub, context: FunctionContext, task_id: str
) -> InvokeResult:
    result: Any = None
    callback_url = None
    pickled_result = None

    try:
        get_args_resp = function_stub.function_get_args(FunctionGetArgsRequest(task_id=task_id))
        if not get_args_resp.ok:
            raise InvalidFunctionArgumentsError

        payload = _load_args(get_args_resp.args)
        args = payload.get("args") or []
        kwargs = payload.get("kwargs") or {}
        callback_url = kwargs.pop("callback_url", None)

        handler = FunctionHandler()
        result = handler(context, *args, **kwargs)

        pickled_result = cloudpickle.dumps(result)
        set_result_resp = function_stub.function_set_result(
            FunctionSetResultRequest(task_id=task_id, result=pickled_result)
        )
        if not set_result_resp.ok:
            raise FunctionSetResultError

        return InvokeResult(
            result=result,
            pickled_result=pickled_result,
            callback_url=callback_url,
        )
    except BaseException as e:
        return InvokeResult(
            result=result,
            pickled_result=pickled_result,
            exception=e,
            callback_url=callback_url,
        )


def complete_task(
    gateway_stub: GatewayServiceStub,
    result: InvokeResult,
    task_id: str,
    container_id: str,
    container_hostname: str,
    start_time: float,
):
    task_status = TaskStatus.Complete
    task_duration = time.time() - start_time
    keep_warm_seconds = 0

    end_task_response = end_task_and_send_callback(
        gateway_stub=gateway_stub,
        payload=result.result,
        end_task_request=EndTaskRequest(
            task_id=task_id,
            task_duration=task_duration,
            task_status=task_status,
            container_id=container_id,
            container_hostname=container_hostname,
            keep_warm_seconds=keep_warm_seconds,
            result=result.pickled_result,
        ),
        override_callback_url=result.callback_url,
    )

    if not end_task_response.ok:
        raise TaskEndError


def handle_task_failure(
    gateway_stub: GatewayServiceStub,
    result: InvokeResult,
    task_id: str,
    container_id: str,
    container_hostname: str,
):
    payload = {"error": str(result.exception)}
    task_status = TaskStatus.Error
    task_duration = 0
    keep_warm_seconds = 0

    end_task_and_send_callback(
        gateway_stub=gateway_stub,
        payload=payload,
        end_task_request=EndTaskRequest(
            task_id=task_id,
            task_duration=task_duration,
            task_status=task_status,
            container_id=container_id,
            container_hostname=container_hostname,
            keep_warm_seconds=keep_warm_seconds,
            result=serialize_result(payload),
        ),
        override_callback_url=result.callback_url,
    )


if __name__ == "__main__":
    main()

================================================================================
# File: runner/serve.py
# Path: src/beta9/runner/serve.py
================================================================================

import os
import signal
import subprocess
import sys
import traceback
from threading import Event
from typing import List, Union

from watchdog.events import FileSystemEventHandler
from watchdog.observers.polling import PollingObserver

from ..abstractions.base.runner import (
    ASGI_SERVE_STUB_TYPE,
    ENDPOINT_SERVE_STUB_TYPE,
    FUNCTION_SERVE_STUB_TYPE,
    TASKQUEUE_SERVE_STUB_TYPE,
)
from ..exceptions import RunnerException
from ..runner.common import USER_CODE_DIR
from ..runner.common import config as cfg


class SyncEventHandler(FileSystemEventHandler):
    def __init__(self, restart_callback):
        super().__init__()
        self.restart_callback = restart_callback
        self.on_created = self._trigger_reload
        self.on_deleted = self._trigger_reload
        self.on_modified = self._trigger_reload
        self.on_moved = self._trigger_reload

    def _trigger_reload(self, event):
        if not event.is_directory and event.src_path.endswith(".py"):
            self.restart_callback()


class ServeGateway:
    def __init__(self) -> None:
        self.process: Union[subprocess.Popen, None] = None
        self.exit_code: int = 0
        self.watch_dir: str = USER_CODE_DIR
        self.restart_event = Event()
        self.exit_event = Event()

        # Register signal handlers
        signal.signal(signal.SIGTERM, self.shutdown)

        # Set up the file change event handler & observer
        self.event_handler = SyncEventHandler(self.trigger_restart)
        self.observer = PollingObserver()
        self.observer.schedule(self.event_handler, self.watch_dir, recursive=True)
        self.observer.start()

    def shutdown(self, signum=None, frame=None) -> None:
        self.kill_subprocess()
        if self.observer.is_alive():
            self.observer.stop()
            self.observer.join(timeout=0.1)
        self.exit_event.set()
        self.restart_event.set()

    def kill_subprocess(self, signum=None, frame=None) -> None:
        if self.process:
            try:
                os.killpg(
                    os.getpgid(self.process.pid), signal.SIGTERM
                )  # Send SIGTERM to the process group
                self.process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                os.killpg(
                    os.getpgid(self.process.pid), signal.SIGKILL
                )  # SIGTERM isn't working, kill the process group
                self.process.wait()
            except BaseException:
                pass

        self.process = None
        self.restart_event.set()

    def run(self, *, command: List[str]) -> None:
        while not self.exit_event.is_set():
            if self.process:
                self.kill_subprocess()

            self.process = subprocess.Popen(
                " ".join(command),
                shell=True,
                preexec_fn=os.setsid,
                env={
                    **os.environ,
                    "GUNICORN_NO_WAIT": "true",
                },
                stdout=sys.stdout,
                stderr=sys.stdout,
            )

            self.restart_event.clear()
            self.restart_event.wait()

    def trigger_restart(self) -> None:
        self.restart_event.set()


def _command() -> List[str]:
    command = []

    if cfg.stub_type in [ENDPOINT_SERVE_STUB_TYPE, ASGI_SERVE_STUB_TYPE]:
        command = [cfg.python_version, "-m", "beta9.runner.endpoint"]
    elif cfg.stub_type == TASKQUEUE_SERVE_STUB_TYPE:
        command = [cfg.python_version, "-m", "beta9.runner.taskqueue"]
    elif cfg.stub_type == FUNCTION_SERVE_STUB_TYPE:
        command = [cfg.python_version, "-m", "beta9.runner.function"]
    else:
        raise RunnerException(f"Invalid stub type: {cfg.stub_type}")

    return command


if __name__ == "__main__":
    sg = ServeGateway()

    try:
        sg.run(command=_command())
    except BaseException:
        print(f"Error occurred: {traceback.format_exc()}")
        sg.exit_code = 1

    if sg.exit_code != 0:
        sys.exit(sg.exit_code)

================================================================================
# File: runner/taskqueue.py
# Path: src/beta9/runner/taskqueue.py
================================================================================

import json
import os
import signal
import sys
import threading
import time
import traceback
from concurrent import futures
from multiprocessing import Event, Process, set_start_method
from multiprocessing.synchronize import Event as TEvent
from typing import Any, List, NamedTuple, Type, Union

import grpc

from ..channel import Channel, with_runner_context
from ..clients.gateway import GatewayServiceStub
from ..clients.taskqueue import (
    TaskQueueCompleteRequest,
    TaskQueueCompleteResponse,
    TaskQueueMonitorRequest,
    TaskQueueMonitorResponse,
    TaskQueuePopRequest,
    TaskQueuePopResponse,
    TaskQueueServiceStub,
)
from ..exceptions import RunnerException
from ..logging import StdoutJsonInterceptor
from ..runner.common import (
    FunctionContext,
    FunctionHandler,
    ThreadPoolExecutorOverride,
    config,
    execute_lifecycle_method,
    send_callback,
    serialize_result,
    wait_for_checkpoint,
)
from ..runner.common import config as cfg
from ..type import LifeCycleMethod, TaskExitCode, TaskStatus

TASK_PROCESS_WATCHDOG_INTERVAL = 0.01
TASK_POLLING_INTERVAL = 0.1
TASK_MANAGER_INTERVAL = 0.1


class TaskQueueManager:
    def __init__(self) -> None:
        # Manager attributes
        self.pid: int = os.getpid()
        self.exit_code: int = 0
        self.shutdown_event = Event()

        self._setup_signal_handlers()
        set_start_method("spawn", force=True)

        # Task worker attributes
        self.task_worker_count: int = config.workers
        self.task_processes: List[Process] = []
        self.task_workers: List[TaskQueueWorker] = []
        self.task_worker_startup_events: List[TEvent] = [
            Event() for _ in range(self.task_worker_count)
        ]
        self.task_worker_watchdog_threads: List[threading.Thread] = []

    def _setup_signal_handlers(self):
        if os.getpid() == self.pid:
            signal.signal(signal.SIGTERM, self._init_shutdown)

    def _init_shutdown(self, signum=None, frame=None):
        self.shutdown_event.set()

    def run(self):
        for worker_index in range(self.task_worker_count):
            print(f"Starting task worker[{worker_index}]")
            self._start_worker(worker_index)

        while not self.shutdown_event.is_set():
            time.sleep(TASK_MANAGER_INTERVAL)

        self.shutdown()

    def shutdown(self):
        print("Spinning down taskqueue")

        # Terminate all worker processes
        for task_process in self.task_processes:
            task_process.terminate()
            task_process.join(timeout=5)

        for task_process in self.task_processes:
            if task_process.is_alive():
                print("Task process did not join within the timeout. Terminating...")
                task_process.terminate()
                task_process.join(timeout=0)

            if task_process.exitcode != 0:
                self.exit_code = task_process.exitcode

    def _start_worker(self, worker_index: int):
        # Initialize task worker
        self.task_workers.append(
            TaskQueueWorker(
                worker_index=worker_index,
                parent_pid=self.pid,
                worker_startup_event=self.task_worker_startup_events[worker_index],
            )
        )

        # Spawn the task process
        self.task_processes.append(Process(target=self.task_workers[-1].process_tasks))
        self.task_processes[-1].start()

        # Initialize and start watchdog thread
        self.task_worker_watchdog_threads.append(
            threading.Thread(target=self._watchdog, args=(worker_index,), daemon=True)
        )
        self.task_worker_watchdog_threads[-1].start()

    def _watchdog(self, worker_index: int):
        self.task_worker_startup_events[worker_index].wait()

        while True:
            if not self.task_processes[worker_index].is_alive():
                exit_code = self.task_processes[worker_index].exitcode

                # Restart worker if the exit code indicates worker exit
                # was due a task timeout, task cancellation, or gateway disconnect
                if exit_code in [
                    TaskExitCode.Cancelled,
                    TaskExitCode.Timeout,
                    TaskExitCode.Disconnect,
                ]:
                    self.task_processes[worker_index] = Process(
                        target=self.task_workers[worker_index].process_tasks
                    )
                    self.task_processes[worker_index].start()

                    continue

                self.exit_code = exit_code
                if self.exit_code == TaskExitCode.SigKill:
                    print(
                        "Task worker ran out memory! Try deploying again with higher memory limits."
                    )

                os.kill(self.pid, signal.SIGTERM)
                break

            time.sleep(TASK_PROCESS_WATCHDOG_INTERVAL)


class Task(NamedTuple):
    id: str = ""
    args: Any = ()
    kwargs: Any = ()


class TaskQueueWorker:
    def __init__(
        self,
        *,
        worker_index: int,
        parent_pid: int,
        worker_startup_event: TEvent,
    ) -> None:
        self.worker_index: int = worker_index
        self.parent_pid: int = parent_pid
        self.worker_startup_event: TEvent = worker_startup_event

    def _get_next_task(
        self, taskqueue_stub: TaskQueueServiceStub, stub_id: str, container_id: str
    ) -> Union[Task, None]:
        try:
            r: TaskQueuePopResponse = taskqueue_stub.task_queue_pop(
                TaskQueuePopRequest(stub_id=stub_id, container_id=container_id)
            )

            if not r.ok or not r.task_msg:
                return None

            task = json.loads(r.task_msg)
            return Task(
                id=task["task_id"],
                args=task["args"],
                kwargs=task["kwargs"],
            )
        except (grpc.RpcError, OSError):
            print("Failed to retrieve task due to unexpected error", traceback.format_exc())
            return None

    def _monitor_task(
        self,
        *,
        context: FunctionContext,
        stub_id: str,
        container_id: str,
        task: Task,
        taskqueue_stub: TaskQueueServiceStub,
        gateway_stub: GatewayServiceStub,
    ) -> None:
        def _monitor_stream() -> bool:
            """
            Returns True if the stream ended with no errors (and should be restarted),
            or False if a exit event occurred (cancellation, completion, timeout,
            or a connection issue that caused us to kill the parent process)
            """
            initial_backoff = 5
            max_retries = 5
            backoff = initial_backoff
            retry = 0

            while retry <= max_retries:
                try:
                    for response in taskqueue_stub.task_queue_monitor(
                        TaskQueueMonitorRequest(
                            task_id=task.id,
                            stub_id=stub_id,
                            container_id=container_id,
                        )
                    ):
                        response: TaskQueueMonitorResponse
                        if response.cancelled:
                            print(f"Task cancelled: {task.id}")

                            send_callback(
                                gateway_stub=gateway_stub,
                                context=context,
                                payload={},
                                task_status=TaskStatus.Cancelled,
                            )
                            os._exit(TaskExitCode.Cancelled)

                        if response.complete:
                            return False

                        if response.timed_out:
                            print(f"Task timed out: {task.id}")

                            send_callback(
                                gateway_stub=gateway_stub,
                                context=context,
                                payload={},
                                task_status=TaskStatus.Timeout,
                            )
                            os._exit(TaskExitCode.Timeout)

                        retry = 0
                        backoff = initial_backoff

                    # Reaching here means that the stream ended with no errors,
                    # which can occur during a rollout restart of the gateway
                    # returning True here tells the outer loop to restart the stream
                    return True

                except (
                    grpc.RpcError,
                    ConnectionRefusedError,
                ):
                    if retry == max_retries:
                        print("Lost connection to task monitor, exiting")
                        os._exit(0)

                    print(f"Lost connection to task monitor, retrying... {retry}")
                    time.sleep(backoff)
                    backoff *= 2
                    retry += 1

                except BaseException:
                    print(f"Unexpected error occurred in task monitor: {traceback.format_exc()}")
                    os._exit(0)

        # Outer loop: restart only if the stream ended with no errors
        while True:
            should_restart = _monitor_stream()
            if not should_restart:
                # Exit condition encountered; exit the monitor task completely
                return

            # If we reached here, the stream ended with no errors;
            # so we should restart the monitoring stream

    @with_runner_context
    def process_tasks(self, channel: Channel) -> None:
        self.worker_startup_event.set()
        taskqueue_stub = TaskQueueServiceStub(channel)
        gateway_stub = GatewayServiceStub(channel)

        # Load handler and execute on_start method
        handler = FunctionHandler()
        on_start_value = execute_lifecycle_method(name=LifeCycleMethod.OnStart)

        print(f"Worker[{self.worker_index}] ready")

        # If checkpointing is enabled, wait for all workers to be ready before creating a checkpoint
        if cfg.checkpoint_enabled:
            wait_for_checkpoint()

        with ThreadPoolExecutorOverride() as thread_pool:
            while True:
                task = self._get_next_task(taskqueue_stub, config.stub_id, config.container_id)
                if not task:
                    time.sleep(TASK_POLLING_INTERVAL)
                    continue

                with StdoutJsonInterceptor(task_id=task.id):
                    print(f"Running task <{task.id}>")

                    context = FunctionContext.new(
                        config=config,
                        task_id=task.id,
                        on_start_value=on_start_value,
                    )

                    monitor_task = thread_pool.submit(
                        self._monitor_task,
                        context=context,
                        stub_id=config.stub_id,
                        container_id=config.container_id,
                        task=task,
                        taskqueue_stub=taskqueue_stub,
                        gateway_stub=gateway_stub,
                    )
                    futures.thread._threads_queues.clear()

                    start_time = time.time()
                    task_status = TaskStatus.Complete
                    result = None
                    duration = None

                    caught_exception = ""
                    args = task.args or []
                    kwargs = task.kwargs or {}

                    try:
                        result = handler(context, *args, **kwargs)
                    except BaseException as e:
                        print(traceback.format_exc())

                        task_status = TaskStatus.Error
                        if retry_on_errors(handler.parent_abstraction.retry_for, e):
                            print(f"retry_for error caught: {e!r}")
                            caught_exception = e.__class__.__name__
                            task_status = TaskStatus.Retry

                    finally:
                        duration = time.time() - start_time

                        try:
                            # TODO: add retries / the ability to recreate the channel dynamically if connection to gateway fails
                            complete_task_response: TaskQueueCompleteResponse = (
                                taskqueue_stub.task_queue_complete(
                                    TaskQueueCompleteRequest(
                                        task_id=task.id,
                                        stub_id=config.stub_id,
                                        task_duration=duration,
                                        task_status=task_status,
                                        container_id=config.container_id,
                                        container_hostname=config.container_hostname,
                                        keep_warm_seconds=config.keep_warm_seconds,
                                        result=serialize_result(result) if result else None,
                                    )
                                )
                            )
                            if not complete_task_response.ok:
                                raise RunnerException("Unable to end task")

                            if task_status == TaskStatus.Retry:
                                print(
                                    complete_task_response.message
                                    or f"Retrying task <{task.id}> after {caught_exception} exception"
                                )
                                continue

                            print(f"Task completed <{task.id}>, took {duration}s")
                            send_callback(
                                gateway_stub=gateway_stub,
                                context=context,
                                payload=result or {},
                                task_status=task_status,
                                override_callback_url=kwargs.get("callback_url"),
                            )

                        except BaseException:
                            print(traceback.format_exc())
                        finally:
                            monitor_task.cancel()


def retry_on_errors(errors: List[Type[Exception]], e: BaseException) -> bool:
    return any([err for err in errors if type(e) is err])


if __name__ == "__main__":
    tq = TaskQueueManager()
    tq.run()

    if tq.exit_code != 0 and tq.exit_code != TaskExitCode.SigTerm:
        sys.exit(tq.exit_code)

================================================================================
# File: schema.py
# Path: src/beta9/schema.py
================================================================================

import ast
import base64
import binascii
import inspect
import json
import os
import tempfile
import urllib.request
from io import BytesIO
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse


class ValidationError(Exception):
    def __init__(self, message, field=None):
        super().__init__(message)
        self.message = message
        self.field = field

    def to_dict(self):
        error = {"error": "ValidationError", "message": self.message}

        if self.field:
            error["field"] = self.field

        return error


class SchemaField:
    """Base class for all schema fields."""

    def validate(self, value: Any) -> Any:
        raise NotImplementedError()

    def dump(self, value: Any) -> Any:
        return value

    def to_dict(self) -> Dict[str, Any]:
        d = {"type": self.__class__.__name__}
        if isinstance(self, Object):
            d["fields"] = self.schema_cls.to_dict()
        return d

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "SchemaField":
        """Create a SchemaField from a dictionary."""
        field_type = data["type"]
        field_classes = {
            "String": String,
            "Integer": Integer,
            "File": File,
            "Image": Image,
            "Object": Object,
        }

        if field_type in ("Object", "object"):
            # Recursively build nested schema
            schema_cls = Schema.from_dict(data["fields"])
            return Object(schema_cls)

        if field_type not in field_classes:
            raise ValidationError(f"Unknown field type '{field_type}'")

        return field_classes[field_type]()


class String(SchemaField):
    def validate(self, value: Any) -> str:
        if not isinstance(value, str):
            raise ValidationError(f"Expected string, got {type(value).__name__}")
        return value


class Integer(SchemaField):
    def validate(self, value: Any) -> int:
        if not isinstance(value, int):
            raise ValidationError(f"Expected integer, got {type(value).__name__}")
        return value


class File(SchemaField):
    """Schema field for file-like objects, URLs, or base64 data."""

    def dump(self, value: Any) -> str:
        """Serialize the file and return a public URL."""
        from .abstractions.output import Output

        o = Output.from_file(value)
        o.save()
        return o.public_url()

    def _is_url(self, value: str) -> bool:
        try:
            result = urlparse(value)
            return all([result.scheme, result.netloc])
        except (ValueError, AttributeError):
            return False

    def _download_url(self, url: str) -> Any:
        temp_file = tempfile.NamedTemporaryFile(delete=False)
        try:
            with urllib.request.urlopen(url) as response:
                while True:
                    chunk = response.read(8192)
                    if not chunk:
                        break
                    temp_file.write(chunk)

            temp_file.close()

            return open(temp_file.name, "rb")
        except (urllib.error.URLError, urllib.error.HTTPError, IOError) as e:
            temp_file.close()
            os.unlink(temp_file.name)
            raise ValidationError(f"Failed to download file from URL: {e}")

    def _decode_base64(self, value: str) -> Any:
        try:
            temp_file = tempfile.NamedTemporaryFile(delete=False)

            try:
                decoded_data = base64.b64decode(value)
                temp_file.write(decoded_data)
                temp_file.close()
                return open(temp_file.name, "rb")
            except (IOError, OSError) as e:
                temp_file.close()
                os.unlink(temp_file.name)
                raise ValidationError(f"Failed to decode base64 data: {e}")
        except (ValueError, binascii.Error) as e:
            raise ValidationError(f"Invalid base64 data: {e}")

    def validate(self, value: Any) -> Any:
        if hasattr(value, "read"):
            return value

        if isinstance(value, str):
            # Check if it's a URL first
            if self._is_url(value):
                return self._download_url(value)

            # If not a URL, try to decode as base64
            try:
                return self._decode_base64(value)
            except ValidationError:
                raise ValidationError(
                    "String input must be either a valid URL or base64 encoded data"
                )

        raise ValidationError(
            "Input must be a file-like object, URL string, or base64 encoded string"
        )

    def __del__(self):
        if hasattr(self, "_temp_file") and self._temp_file:
            try:
                self._temp_file.close()
                os.unlink(self._temp_file.name)
            except (OSError, IOError):
                pass


class Image(SchemaField):
    """Schema field for validating and serializing images."""

    def __init__(
        self,
        max_size: Optional[Tuple[int, int]] = None,
        min_size: Optional[Tuple[int, int]] = None,
        allowed_formats: Optional[List[str]] = None,
        quality: int = 85,
        preserve_metadata: bool = False,
    ):
        """
        Initialize an Image schema field.

        Args:
            max_size: Optional (width, height) maximum.
            min_size: Optional (width, height) minimum.
            allowed_formats: Allowed image formats.
            quality: JPEG/WEBP quality (1-100).
            preserve_metadata: Preserve EXIF metadata if True.
        """
        self.max_size = max_size
        self.min_size = min_size
        self.allowed_formats = allowed_formats or ["PNG", "JPEG", "WEBP"]
        self.quality = max(1, min(100, quality))
        self.preserve_metadata = preserve_metadata

    def _import_pil(self):
        try:
            from PIL import Image as PILImage
            from PIL.ExifTags import TAGS

            return PILImage, TAGS
        except ImportError:
            raise ValidationError("Pillow library is not installed. Image schema is unavailable.")

    def _validate_dimensions(self, img):
        """Validate image dimensions against constraints."""
        width, height = img.size

        if self.max_size:
            max_w, max_h = self.max_size
            if width > max_w or height > max_h:
                raise ValidationError(
                    f"Image dimensions {width}x{height} exceed maximum allowed {max_w}x{max_h}"
                )

        if self.min_size:
            min_w, min_h = self.min_size
            if width < min_w or height < min_h:
                raise ValidationError(
                    f"Image dimensions {width}x{height} below minimum required {min_w}x{min_h}"
                )

    def _get_image_format(self, img):
        """Determine the best format for the image."""
        if img.format and img.format.upper() in self.allowed_formats:
            return img.format.upper()
        return "PNG"  # Default to PNG if format not in allowed list

    def _extract_metadata(self, img):
        """Extract image metadata if preservation is enabled."""
        if not self.preserve_metadata:
            return None

        metadata = {}
        if hasattr(img, "_getexif") and img._getexif():
            _, TAGS = self._import_pil()
            for tag_id, value in img._getexif().items():
                tag = TAGS.get(tag_id, tag_id)
                metadata[tag] = value
        return metadata

    def _is_url(self, value: str) -> bool:
        """Check if string is a valid URL."""
        try:
            result = urlparse(value)
            return all([result.scheme, result.netloc])
        except (ValueError, AttributeError):
            return False

    def _download_url(self, url: str) -> Any:
        """Download image from URL."""
        temp_file = tempfile.NamedTemporaryFile(delete=False)

        try:
            with urllib.request.urlopen(url) as response:
                while True:
                    chunk = response.read(8192)
                    if not chunk:
                        break
                    temp_file.write(chunk)

            temp_file.close()
            PILImage, _ = self._import_pil()
            return PILImage.open(temp_file.name)
        except Exception as e:
            temp_file.close()
            os.unlink(temp_file.name)
            raise ValidationError(f"Failed to download image from URL: {e}")

    def validate(self, value: Any) -> Any:
        """Validate and process image input."""
        PILImage, _ = self._import_pil()

        # Handle PIL Image directly
        if isinstance(value, PILImage.Image):
            img = value
        # Handle file path
        elif isinstance(value, str) and os.path.isfile(value):
            try:
                img = PILImage.open(value)
            except Exception as e:
                raise ValidationError(f"Failed to open image file: {e}")
        # Handle URL
        elif isinstance(value, str) and self._is_url(value):
            img = self._download_url(value)
        # Handle base64
        elif isinstance(value, str):
            try:
                image_data = base64.b64decode(value)
                img = PILImage.open(BytesIO(image_data))
            except Exception as e:
                raise ValidationError(f"Invalid base64 image data: {e}")
        else:
            raise ValidationError(
                f"Expected PIL.Image, file path, URL, or base64 string, got {type(value).__name__}"
            )

        # Validate image format
        if img.format and img.format.upper() not in self.allowed_formats:
            raise ValidationError(
                f"Image format {img.format} not in allowed formats: {self.allowed_formats}"
            )

        # Validate dimensions
        self._validate_dimensions(img)

        # Store metadata if needed
        if self.preserve_metadata:
            img._metadata = self._extract_metadata(img)

        return img

    def dump(self, value: Any) -> str:
        """Serialize image to a file and return a public URL."""
        PILImage, _ = self._import_pil()
        if not isinstance(value, PILImage.Image):
            raise ValidationError(
                f"Expected PIL.Image for serialization, got {type(value).__name__}"
            )

        # Determine output format
        output_format = self._get_image_format(value)

        # Prepare save parameters
        save_params = {}
        if output_format == "JPEG":
            save_params["quality"] = self.quality
            save_params["optimize"] = True
        elif output_format == "WEBP":
            save_params["quality"] = self.quality
            save_params["lossless"] = False

        # Convert to RGB if needed (for JPEG)
        if output_format == "JPEG" and value.mode in ("RGBA", "LA"):
            value = value.convert("RGB")

        # Save to a temporary file
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=f".{output_format.lower()}")
        try:
            value.save(temp_file, format=output_format, **save_params)
            temp_file.close()
            from .abstractions.output import Output

            with open(temp_file.name, "rb") as f:
                o = Output.from_file(f)
                o.save()
                return o.public_url()
        finally:
            try:
                os.unlink(temp_file.name)
            except Exception:
                pass

    def __del__(self):
        if hasattr(self, "_temp_file") and self._temp_file:
            try:
                self._temp_file.close()
                os.unlink(self._temp_file.name)
            except (OSError, IOError):
                pass


class Object(SchemaField):
    """Schema field for nested objects."""

    def __init__(self, schema_cls):
        self.schema_cls = schema_cls

    def validate(self, value: Any) -> Any:
        """Validate and load a nested object."""
        if isinstance(value, dict):
            return self.schema_cls(**value)
        elif isinstance(value, self.schema_cls):
            return value
        else:
            raise ValidationError(
                f"Expected dict or {self.schema_cls.__name__}, got {type(value).__name__}"
            )

    def dump(self, value: Any) -> Dict[str, Any]:
        """Serialize a nested object."""
        if isinstance(value, self.schema_cls):
            return value.dump()
        raise ValidationError(f"Expected {self.schema_cls.__name__} for serialization")


class SchemaMeta(type):
    """Metaclass for collecting schema fields."""

    def __new__(cls, name, bases, attrs):
        fields = {k: v for k, v in attrs.items() if isinstance(v, SchemaField)}
        attrs["_fields"] = fields
        return super().__new__(cls, name, bases, attrs)


class Schema(metaclass=SchemaMeta):
    """Base class for  input/output schemas."""

    def __init__(self, **kwargs):
        validated = self.validate(kwargs)

        for key, value in validated.items():
            setattr(self, key, value)

        self._data = validated

    @classmethod
    def validate(cls, data: Dict[str, Any]) -> Dict[str, Any]:
        validated = {}
        for key, field in cls._fields.items():
            if key not in data:
                raise ValidationError(f"Missing required field '{key}'")
            validated[key] = field.validate(data[key])
        return validated

    @classmethod
    def new(cls, data: Dict[str, Any]) -> "Schema":
        """Create a new schema instance from a dictionary."""
        return cls(**data)

    def dump(self) -> Dict[str, Any]:
        """Serialize the schema to a dictionary."""
        result = {}

        for key in self._fields:
            value = getattr(self, key)
            if isinstance(value, Schema):
                result[key] = value.dump()
            else:
                field = self._fields[key]
                result[key] = field.dump(value)

        return result

    def dict(self) -> Dict[str, Any]:
        """Return the schema definition as a plain dictionary."""
        result = {}

        for key in self._fields:
            value = getattr(self, key)
            if isinstance(value, Schema):
                result[key] = value.dict()
            else:
                result[key] = value

        return result

    def json(self) -> str:
        """Return the schema definition as a JSON string."""
        return json.dumps(self.dict())

    @classmethod
    def from_json(cls, json_str: str) -> "Schema":
        """Create a schema class from a JSON schema definition."""
        schema_config = json.loads(json_str)
        fields_config = schema_config.get("fields", {})
        attrs = {
            name: SchemaField.from_dict(field_dict) for name, field_dict in fields_config.items()
        }
        return SchemaMeta("DynamicSchema", (Schema,), attrs)

    @classmethod
    def to_json(cls) -> str:
        """Return the schema definition as a JSON string."""
        schema_config = {"fields": {name: field.to_dict() for name, field in cls._fields.items()}}
        return json.dumps(schema_config, indent=2)

    @classmethod
    def to_dict(cls) -> Dict[str, Any]:
        """Return the schema definition as a dictionary."""
        return {"fields": {name: field.to_dict() for name, field in cls._fields.items()}}

    @classmethod
    def from_dict(cls, schema_dict: Dict[str, Any]) -> "Schema":
        """Create a schema class from a dictionary definition."""
        fields_config = schema_dict.get("fields", {})
        attrs = {
            name: SchemaField.from_dict(field_dict) for name, field_dict in fields_config.items()
        }
        return SchemaMeta("DynamicSchema", (Schema,), attrs)

    @classmethod
    def object(cls, fields: dict) -> "Schema":
        """Dynamically create a schema class from a fields dictionary."""

        # Try to infer the name from the calling context
        try:
            frame = inspect.currentframe().f_back
            line = inspect.getframeinfo(frame).code_context[0]
            tree = ast.parse(line)
            assign = tree.body[0]
            if isinstance(assign, ast.Assign):
                name = assign.targets[0].id
            else:
                name = "DynamicSchema"
        except Exception:
            name = "DynamicSchema"

        processed_fields = {}
        for k, v in fields.items():
            if isinstance(v, dict):
                # Recursively create a nested schema and wrap in Object
                nested_schema = Schema.object(v)
                processed_fields[k] = Object(nested_schema)
            elif isinstance(v, type) and issubclass(v, Schema):
                processed_fields[k] = Object(v)
            else:
                processed_fields[k] = v

        # Build __annotations__ for editor support
        annotations = {}
        for k, v in processed_fields.items():
            if isinstance(v, String):
                annotations[k] = str
            elif isinstance(v, Integer):
                annotations[k] = int
            elif isinstance(v, File):
                annotations[k] = bytes
            elif isinstance(v, Image):
                try:
                    from PIL import Image as PILImage

                    annotations[k] = PILImage.Image
                except ImportError:
                    annotations[k] = object
            elif isinstance(v, Object):
                annotations[k] = v.schema_cls
            else:
                annotations[k] = object

        attrs = dict(processed_fields)
        attrs["__annotations__"] = annotations
        return type(name, (cls,), attrs)

================================================================================
# File: sync.py
# Path: src/beta9/sync.py
================================================================================

import hashlib
import os
import tempfile
import threading
import zipfile
from http import HTTPStatus
from pathlib import Path
from queue import Queue
from typing import Generator, NamedTuple

import requests
from watchdog.events import FileSystemEvent, FileSystemEventHandler

from beta9.vendor.pathspec import PathSpec

from . import terminal
from .clients.gateway import (
    CreateObjectRequest,
    CreateObjectResponse,
    GatewayServiceStub,
    HeadObjectRequest,
    HeadObjectResponse,
    ObjectMetadata,
    PutObjectRequest,
    SyncContainerWorkspaceOperation,
)
from .config import get_settings
from .env import is_local, is_notebook_env

_sync_lock = threading.Lock()

# Global workspace object id to signal to any other threads that the workspace has already been synced
_workspace_object_id = ""


def set_workspace_object_id(object_id: str) -> None:
    global _workspace_object_id
    _workspace_object_id = object_id


def get_workspace_object_id() -> str:
    global _workspace_object_id
    if not _workspace_object_id:
        _workspace_object_id = ""
    return _workspace_object_id


CHUNK_SIZE = 1024 * 1024 * 4


def ignore_file_name() -> str:
    return f".{get_settings().name}ignore".lower()


def ignore_file_contents() -> str:
    return f"""# Generated by {get_settings().name} SDK
.{get_settings().name.lower()}ignore
pyproject.toml
.git
.idea
.python-version
.vscode
.venv
venv
__pycache__
.DS_Store
.config
drive/MyDrive
.coverage
.pytest_cache
.ipynb
.ruff_cache
.dockerignore
.ipynb_checkpoints
.env.local
.envrc
**/__pycache__/
**/.pytest_cache/
**/node_modules/
**/.venv/
*.pyc
.next/
.circleci
"""


class FileSyncResult(NamedTuple):
    success: bool = False
    object_id: str = ""


class FileSyncer:
    def __init__(
        self,
        gateway_stub: GatewayServiceStub,
        root_dir=".",
    ):
        self.root_dir = Path(root_dir).absolute()
        self.gateway_stub: GatewayServiceStub = gateway_stub
        self.is_workspace_dir = root_dir == "."

    @property
    def ignore_file_path(self) -> Path:
        return self.root_dir / ignore_file_name()

    def _init_ignore_file(self) -> None:
        if not is_local():
            return

        if self.ignore_file_path.exists():
            return

        terminal.detail(f"Writing {ignore_file_name()} file")
        with self.ignore_file_path.open(mode="w") as f:
            f.writelines(ignore_file_contents())

    def _read_ignore_file(self) -> list:
        if not is_local():
            return []

        terminal.detail(f"Reading {ignore_file_name()} file")

        patterns = []

        if self.ignore_file_path.is_file():
            with self.ignore_file_path.open() as file:
                patterns = [line.strip() for line in file.readlines() if line.strip()]

        return patterns

    def _should_ignore(self, path: str) -> bool:
        relative_path = os.path.relpath(path, self.root_dir)
        spec = PathSpec.from_lines("gitwildmatch", self.ignore_patterns)
        return spec.match_file(relative_path)

    def _collect_files(self) -> Generator[str, None, None]:
        terminal.detail(f"Collecting files from {self.root_dir}")

        for root, dirs, files in os.walk(self.root_dir):
            dirs[:] = [d for d in dirs if not self._should_ignore(os.path.join(root, d))]

            for file in files:
                file_path = os.path.join(root, file)

                if not self._should_ignore(file_path):
                    yield file_path

    @staticmethod
    def _calculate_sha256(file_path: str, chunk_size: int = CHUNK_SIZE) -> str:
        hasher = hashlib.sha256()
        with open(file_path, "rb") as file:
            while chunk := file.read(chunk_size):
                hasher.update(chunk)
        return hasher.hexdigest()

    def sync(self) -> FileSyncResult:
        with _sync_lock:
            if self.is_workspace_dir and get_workspace_object_id() != "" and not is_notebook_env():
                terminal.header("Files already synced")
                return FileSyncResult(success=True, object_id=get_workspace_object_id())
            return self._sync()

    def _sync(self) -> FileSyncResult:
        terminal.header("Syncing files")

        self._init_ignore_file()
        self.ignore_patterns = self._read_ignore_file()
        temp_zip_name = tempfile.NamedTemporaryFile(delete=False).name

        with zipfile.ZipFile(temp_zip_name, "w") as zipf:
            for file in self._collect_files():
                try:
                    zipf.write(file, os.path.relpath(file, self.root_dir))
                    terminal.detail(f"Added {file}")
                except OSError as e:
                    terminal.warn(f"Failed to add {file}: {e}")

        size = os.path.getsize(temp_zip_name)
        hash = self._calculate_sha256(temp_zip_name)

        terminal.detail(f"Collected object is {terminal.humanize_memory(size, base=10)}")

        object_id = None
        head_response: HeadObjectResponse = self.gateway_stub.head_object(
            HeadObjectRequest(hash=hash)
        )
        if not head_response.exists:
            metadata = ObjectMetadata(name=hash, size=size)

            def _upload_object() -> requests.Response:
                with terminal.progress_open(temp_zip_name, "rb", description=None) as file:
                    response = requests.put(presigned_url, data=file)
                return response

            # TODO: remove this once all workspaces are migrated to use workspace storage
            def stream_requests():
                with terminal.progress_open(temp_zip_name, "rb", description=None) as file:
                    while chunk := file.read(CHUNK_SIZE):
                        yield PutObjectRequest(chunk, metadata, hash, False)

            terminal.header("Uploading")
            if head_response.use_workspace_storage:
                create_object_response: CreateObjectResponse = self.gateway_stub.create_object(
                    CreateObjectRequest(
                        object_metadata=metadata, hash=hash, size=size, overwrite=True
                    )
                )
                if create_object_response.ok:
                    presigned_url = create_object_response.presigned_url
                    response = _upload_object()
                    if response.status_code == HTTPStatus.OK:
                        if self.is_workspace_dir:
                            set_workspace_object_id(create_object_response.object_id)
                        object_id = create_object_response.object_id
                    else:
                        terminal.error("File sync failed ☠️")
            else:
                put_response = self.gateway_stub.put_object_stream(stream_requests())
                if put_response.ok and self.is_workspace_dir:
                    set_workspace_object_id(put_response.object_id)
                    object_id = put_response.object_id
                else:
                    terminal.error("File sync failed ☠️")

        elif head_response.exists and head_response.ok:
            terminal.header("Files already synced")

            if self.is_workspace_dir:
                set_workspace_object_id(head_response.object_id)
                object_id = head_response.object_id

            return FileSyncResult(success=True, object_id=head_response.object_id)

        os.remove(temp_zip_name)

        if object_id is None:
            terminal.error("File sync failed ☠️")
            return FileSyncResult(success=False, object_id="")

        terminal.header("Files synced")
        return FileSyncResult(success=True, object_id=object_id)


class SyncEventHandler(FileSystemEventHandler):
    def __init__(self, queue: Queue):
        super().__init__()
        self.queue = queue

    def on_any_event(self, event: FileSystemEvent) -> None:
        if not event.is_directory and event.src_path.endswith(".py"):
            terminal.warn(f"Detected changes in '{event.src_path}'. Reloading...")

    def on_created(self, event) -> None:
        self.queue.put((SyncContainerWorkspaceOperation.WRITE, event.src_path, None))

    def on_modified(self, event: FileSystemEvent) -> None:
        self.on_created(event)

    def on_deleted(self, event: FileSystemEvent) -> None:
        self.queue.put((SyncContainerWorkspaceOperation.DELETE, event.src_path, None))

    def on_moved(self, event: FileSystemEvent) -> None:
        self.queue.put((SyncContainerWorkspaceOperation.MOVED, event.src_path, event.dest_path))

================================================================================
# File: terminal.py
# Path: src/beta9/terminal.py
================================================================================

import datetime
import sys
import threading
from contextlib import contextmanager
from io import BytesIO
from os import PathLike
from typing import Any, Generator, Literal, Optional, Sequence, Tuple, Union

import rich
import rich.columns
import rich.control
import rich.status
import rich.traceback
from rich.console import Console
from rich.control import STRIP_CONTROL_CODES as _STRIP_CONTROL_CODES
from rich.markup import escape
from rich.progress import (
    BarColumn,
    DownloadColumn,
    Progress,
    ProgressColumn,
    Task,
    TextColumn,
    TimeRemainingColumn,
)
from rich.progress import open as _progress_open
from rich.text import Text

from . import env

# Fixes printing carriage returns and backspaces
# https://github.com/Textualize/rich/issues/3260
for i in (8, 13):
    if i in _STRIP_CONTROL_CODES:
        _STRIP_CONTROL_CODES.remove(i)
        rich.control.strip_control_codes.__defaults__ = ({c: None for c in _STRIP_CONTROL_CODES},)


if env.is_local():
    rich.traceback.install()

_console = Console()
_current_status = None
_status_lock = threading.Lock()
_status_count = 0


def header(text: str, subtext: str = "") -> None:
    header_text = f"[bold #4CCACC]=> {text}[/bold #4CCACC]"
    _console.print(header_text, subtext)


def print(*objects: Any, **kwargs: Any) -> None:
    _console.print(*objects, **kwargs)


def print_json(data: Any, **kwargs: Any) -> None:
    _console.print_json(data=data, indent=2, default=lambda o: str(o), **kwargs)


def prompt(
    *, text: str, default: Optional[Any] = None, markup: bool = False, password: bool = False
) -> Any:
    prompt_text = f"{text} [{default}]: " if default is not None else f"{text}: "
    user_input = _console.input(prompt_text, markup=markup, password=password).strip()
    return user_input if user_input else default


def detail(text: str, dim: bool = True, **kwargs) -> None:
    style = "dim" if dim else ""
    _console.print(Text(text, style=style), **kwargs)


def success(text: str) -> None:
    _console.print(Text(text, style="bold green"))


def warn(text: str) -> None:
    _console.print(Text(text, style="bold yellow"))


def error(text: str, exit: bool = True) -> None:
    _console.print(Text(text, style="bold red"))

    if exit:
        reset_terminal()
        sys.exit(1)


def url(text: str) -> None:
    _console.print(Text(text, style="underline blue"))


@contextmanager
def progress(task_name: str) -> Generator[rich.status.Status, None, None]:
    global _current_status, _status_count

    with _status_lock:
        if _current_status is None:
            _current_status = _console.status(task_name, spinner="dots", spinner_style="white")
            _current_status.start()
        _status_count += 1

    try:
        yield _current_status
    finally:
        with _status_lock:
            _status_count -= 1
            if _status_count == 0:
                _current_status.stop()
                _current_status = None


def progress_open(file: Union[str, PathLike, bytes], mode: str, **kwargs: Any) -> BytesIO:
    options = dict(
        complete_style="green",
        finished_style="slate_blue1",
        refresh_per_second=60,
        **kwargs,
    )

    if "description" in options and options["description"]:
        options["description"] = escape(f"[{options['description']}]")

    return _progress_open(file, mode, **options)  # type:ignore


def humanize_date(d: datetime.datetime) -> str:
    # Check if datetime is "zero" time
    if d == datetime.datetime(1, 1, 1, tzinfo=datetime.timezone.utc):
        return ""

    # Generate relative datetime
    diff = datetime.datetime.now(datetime.timezone.utc) - d
    s = diff.seconds
    if diff.days > 7 or diff.days < 0:
        return d.strftime("%b %d %Y")
    elif diff.days == 1:
        return "1 day ago"
    elif diff.days > 1:
        return f"{diff.days} days ago"
    elif s <= 1:
        return "just now"
    elif s < 60:
        return f"{s} seconds ago"
    elif s < 120:
        return "1 minute ago"
    elif s < 3600:
        return f"{s // 60} minutes ago"
    elif s < 7200:
        return "1 hour ago"
    else:
        return f"{s // 3600} hours ago"


def humanize_duration(delta: datetime.timedelta) -> str:
    total_seconds = int(delta.total_seconds())
    if total_seconds < 60:
        return f"{total_seconds} seconds"
    elif total_seconds < 3600:
        minutes = total_seconds // 60
        return f"{minutes} minutes"
    elif total_seconds < 86400:
        hours = total_seconds // 3600
        return f"{hours} hours"
    else:
        days = total_seconds // 86400
        return f"{days} days"


def humanize_memory(m: float, base: Literal[2, 10] = 2) -> str:
    if base not in [2, 10]:
        raise ValueError("Base must be 2 (binary) or 10 (decimal)")

    factor = 1024 if base == 2 else 1000
    units = (
        ["B", "KiB", "MiB", "GiB", "TiB", "PiB"]
        if base == 2
        else ["B", "KB", "MB", "GB", "TB", "PB"]
    )
    index = 0
    while m >= factor and index < len(units) - 1:
        m /= factor
        index += 1
    return f"{m:.2f} {units[index]}"


def pluralize(seq: Sequence, suffix: str = "s") -> Tuple[int, str]:
    n = len(seq)
    return n, "s" if n != 1 else ""


def reset_terminal() -> None:
    _console.show_cursor()


def progress_description(name: str, max_width: Optional[int] = None):
    max_desc_width = max_width or len(name)
    if len(name) > max_desc_width:
        text = f"...{name[-(max_desc_width - 3) :]}"
    else:
        text = name.ljust(max_desc_width)

    return escape(f"[{text}]")


class CustomProgress(Progress):
    def add_task(self, description: Any, *args, **kwargs):
        return super().add_task(progress_description(str(description)), *args, **kwargs)


class AverageTransferSpeedColumn(ProgressColumn):
    """
    Renders the average data transfer speed over the entire lifetime of the transfer.
    """

    def render(self, task: Task) -> Text:
        task.fields.setdefault("average_mib_s", 0.0)

        # If the task hasn't started or there's no elapsed time yet, we can't compute an average
        if not task.started or task.elapsed == 0:
            return Text("?", style="progress.data.speed")

        if task.completed == task.total:
            return Text(f"{task.fields['average_mib_s']:.2f} MiB/s", style="progress.data.speed")

        # Calculate average speed in bytes per second
        average_bps = task.completed / (task.elapsed or 1)

        # Convert bytes per second to MiB/s (1 MiB = 1024 * 1024 bytes)
        task.fields["average_mib_s"] = average_bps / (1024**2)

        # Format to a reasonable precision (e.g., 2 decimal places)
        return Text(f"{task.fields['average_mib_s']:.2f} MiB/s", style="progress.data.speed")


def StyledProgress() -> CustomProgress:
    """
    Return a styled progress bar with custom columns.
    """
    return CustomProgress(
        *[
            TextColumn("[progress.description]{task.description}"),
            BarColumn(
                complete_style="green",
                finished_style="slate_blue1",
            ),
            DownloadColumn(binary_units=True),
            AverageTransferSpeedColumn(),
            TimeRemainingColumn(elapsed_when_finished=True),
        ],
        auto_refresh=True,
        refresh_per_second=60,
        disable=False,
    )


@contextmanager
def redirect_terminal_to_buffer(buffer):
    global _console
    original_console = _console
    _console = Console(file=buffer, force_terminal=False)
    try:
        yield
    finally:
        _console = original_console

================================================================================
# File: type.py
# Path: src/beta9/type.py
================================================================================

from dataclasses import dataclass
from enum import Enum
from typing import Dict, Literal, Type, Union


class LifeCycleMethod(str, Enum):
    OnStart = "on_start"


class TaskStatus(str, Enum):
    Complete = "COMPLETE"
    Error = "ERROR"
    Pending = "PENDING"
    Running = "RUNNING"
    Cancelled = "CANCELLED"
    Retry = "RETRY"
    Timeout = "TIMEOUT"

    def __str__(self) -> str:
        return self.value

    def is_complete(self) -> bool:
        return self.value in [self.Complete, self.Error, self.Cancelled, self.Timeout]


class TaskExitCode:
    SigTerm = -15
    SigKill = -9
    Success = 0
    Error = 1
    ErrorLoadingApp = 2
    Cancelled = 3
    Timeout = 4
    Disconnect = 5


class PythonVersion(str, Enum):
    """
    An enum that defines versions of Python.

    The default version is python3. This defaults to python3 already in the image. If python3 does not exist in the image, then the default version of Python will be determined by the server (e.g. Python 3.10).

    Example:
        ```python
        from beta9 import Image, PythonVersion

        # with an enum
        image = Image(python_version=PythonVersion.Python310)

        # with a string
        image = Image(python_version="python3.10")
        ```
    """

    Python3 = "python3"
    Python38 = "python3.8"
    Python39 = "python3.9"
    Python310 = "python3.10"
    Python311 = "python3.11"
    Python312 = "python3.12"


PythonVersionLiteral = Literal[
    "python3.8",
    "python3.9",
    "python3.10",
    "python3.11",
    "python3.12",
]

PythonVersionAlias = Union[PythonVersion, PythonVersionLiteral]


class GpuType(str, Enum):
    """
    An enum that defines types of GPUs.

    Example:
        ```python
        from beta9 import GpuType, function

        @function(gpu=GpuType.T4)
        def some_func()
            print("I will run on a T4 gpu!")

        # This is equivalent to the above ^
        @function(gpu="T4")
        def some_other_func()
            print("I will run on a T4 gpu!")
        ```
    """

    NoGPU = ""
    Any = "any"
    T4 = "T4"
    L4 = "L4"
    A10G = "A10G"
    A100_40 = "A100-40"
    A100_80 = "A100-80"
    H100 = "H100"
    A6000 = "A6000"
    RTX4090 = "RTX4090"
    L40S = "L40S"


# Add GpuType str literals. Must copy/paste for now.
# https://github.com/python/typing/issues/781
GpuTypeLiteral = Literal[
    "",
    "any",
    "T4",
    "L4",
    "A10G",
    "A100-40",
    "A100-80",
    "H100",
    "A6000",
    "RTX4090",
    "L40S",
]

GpuTypeAlias = Union[GpuType, GpuTypeLiteral]


QUEUE_DEPTH_AUTOSCALER_TYPE = "queue_depth"
DEFAULT_AUTOSCALER_MAX_CONTAINERS = 1
DEFAULT_AUTOSCALER_TASKS_PER_CONTAINER = 1
DEFAULT_AUTOSCALER_MIN_CONTAINERS = 0


@dataclass
class Autoscaler:
    max_containers: int = DEFAULT_AUTOSCALER_MAX_CONTAINERS
    tasks_per_container: int = DEFAULT_AUTOSCALER_TASKS_PER_CONTAINER
    min_containers: int = DEFAULT_AUTOSCALER_MIN_CONTAINERS


@dataclass
class QueueDepthAutoscaler(Autoscaler):
    pass


@dataclass
class TaskPolicy:
    """
    Task policy for a function. This helps manages lifecycle of an individual task.

    Parameters:
        max_retries (int):
            The maximum number of times a task will be retried if the container crashes. Default is 3.
        timeout (int):
            The maximum number of seconds a task can run before it times out.
            Default depends on the abstraction that you are using.
            Set it to -1 to disable the timeout (this does not disable timeout for endpoints).
        ttl (int):
            The expiration time for a task in seconds. Must be greater than 0 and less than 24 hours (86400 seconds).
    """

    max_retries: int = 0
    timeout: int = 0
    ttl: int = 0


class PricingPolicyCostModel(str, Enum):
    Task = "task"
    Duration = "duration"


@dataclass
class PricingPolicy:
    max_in_flight: int = 10
    cost_model: PricingPolicyCostModel = PricingPolicyCostModel.Task
    cost_per_task: float = 0.000000000000000000
    cost_per_task_duration_ms: float = 0.000000000000000000


_AUTOSCALER_TYPES: Dict[Type[Autoscaler], str] = {
    QueueDepthAutoscaler: QUEUE_DEPTH_AUTOSCALER_TYPE,
}

================================================================================
# File: utils.py
# Path: src/beta9/utils.py
================================================================================

import importlib
import inspect
import os
import sys
from pathlib import Path

from . import terminal


class TempFile:
    """
    A temporary file that is automatically deleted when closed. This class exists
    because the `tempfile.NamedTemporaryFile` class does not allow for the filename
    to be explicitly set.
    """

    def __init__(self, name: str, mode: str = "wb", dir: str = "."):
        self.name = name
        self._file = open(os.path.join(dir, name), mode)

    def __getattr__(self, attr):
        return getattr(self._file, attr)

    def close(self):
        if not self._file.closed:
            self._file.close()
            os.remove(self.name)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()


def get_init_args_kwargs(cls):
    sig = inspect.signature(cls.__init__)

    # Separate args and kwargs
    args = []
    kwargs = {}

    for k, v in sig.parameters.items():
        # Skip 'self' since it's implicit for instance methods
        if k == "self":
            continue

        # Check if the argument is a required positional argument
        if v.default is inspect.Parameter.empty and v.kind in (
            inspect.Parameter.POSITIONAL_OR_KEYWORD,
            inspect.Parameter.POSITIONAL_ONLY,
        ):
            args.append(k)

        # Check if the argument is a keyword argument (with default)
        elif v.default is not inspect.Parameter.empty and v.kind in (
            inspect.Parameter.POSITIONAL_OR_KEYWORD,
            inspect.Parameter.KEYWORD_ONLY,
        ):
            kwargs[k] = v.default

    all_args_set = args + list(kwargs.keys())
    return set(all_args_set)


def get_class_name(cls):
    return cls.__class__.__name__


def load_module_spec(specfile: str, command: str):
    current_dir = os.getcwd()
    if current_dir not in sys.path:
        sys.path.insert(0, current_dir)

    module_path, obj_name, *_ = specfile.split(":") if ":" in specfile else (specfile, "")
    module_name = module_path.replace(".py", "").replace(os.path.sep, ".")

    if not Path(module_path).exists():
        terminal.error(f"Unable to find file: '{module_path}'")

    if not obj_name:
        terminal.error(
            f"Invalid handler function specified. Expected format: beam {command} [file.py]:[function]"
        )

    module = importlib.import_module(module_name)

    module_spec = getattr(module, obj_name, None)
    if module_spec is None:
        terminal.error(
            f"Invalid handler function specified. Make sure '{module_path}' contains the function: '{obj_name}'"
        )

    return module_spec, module_name, obj_name

================================================================================
# File: vendor/pathspec/__init__.py
# Path: src/beta9/vendor/pathspec/__init__.py
================================================================================

"""
The *pathspec* package provides pattern matching for file paths. So far
this only includes Git's wildmatch pattern matching (the style used for
".gitignore" files).

The following classes are imported and made available from the root of
the `pathspec` package:

-	:class:`pathspec.gitignore.GitIgnoreSpec`

-	:class:`pathspec.pathspec.PathSpec`

-	:class:`pathspec.pattern.Pattern`

-	:class:`pathspec.pattern.RegexPattern`

-	:class:`pathspec.util.RecursionError`

The following functions are also imported:

-	:func:`pathspec.util.lookup_pattern`

The following deprecated functions are also imported to maintain
backward compatibility:

-	:func:`pathspec.util.iter_tree` which is an alias for
	:func:`pathspec.util.iter_tree_files`.

-	:func:`pathspec.util.match_files`
"""

from .gitignore import (
	GitIgnoreSpec)
from .pathspec import (
	PathSpec)
from .pattern import (
	Pattern,
	RegexPattern)
from .util import (
	RecursionError,
	iter_tree,
	lookup_pattern,
	match_files)

from ._meta import (
	__author__,
	__copyright__,
	__credits__,
	__license__,
	__version__,
)

# Load pattern implementations.
from . import patterns

# DEPRECATED: Expose the `GitIgnorePattern` class in the root module for
# backward compatibility with v0.4.
from .patterns.gitwildmatch import GitIgnorePattern

# Declare private imports as part of the public interface. Deprecated
# imports are deliberately excluded.
__all__ = [
	'GitIgnoreSpec',
	'PathSpec',
	'Pattern',
	'RecursionError',
	'RegexPattern',
	'__author__',
	'__copyright__',
	'__credits__',
	'__license__',
	'__version__',
	'iter_tree',
	'lookup_pattern',
	'match_files',
]

================================================================================
# File: vendor/pathspec/_meta.py
# Path: src/beta9/vendor/pathspec/_meta.py
================================================================================

"""
This module contains the project meta-data.
"""

__author__ = "Caleb P. Burns"
__copyright__ = "Copyright © 2013-2023 Caleb P. Burns"
__credits__ = [
	"dahlia <https://github.com/dahlia>",
	"highb <https://github.com/highb>",
	"029xue <https://github.com/029xue>",
	"mikexstudios <https://github.com/mikexstudios>",
	"nhumrich <https://github.com/nhumrich>",
	"davidfraser <https://github.com/davidfraser>",
	"demurgos <https://github.com/demurgos>",
	"ghickman <https://github.com/ghickman>",
	"nvie <https://github.com/nvie>",
	"adrienverge <https://github.com/adrienverge>",
	"AndersBlomdell <https://github.com/AndersBlomdell>",
	"thmxv <https://github.com/thmxv>",
	"wimglenn <https://github.com/wimglenn>",
	"hugovk <https://github.com/hugovk>",
	"dcecile <https://github.com/dcecile>",
	"mroutis <https://github.com/mroutis>",
	"jdufresne <https://github.com/jdufresne>",
	"groodt <https://github.com/groodt>",
	"ftrofin <https://github.com/ftrofin>",
	"pykong <https://github.com/pykong>",
	"nhhollander <https://github.com/nhhollander>",
	"KOLANICH <https://github.com/KOLANICH>",
	"JonjonHays <https://github.com/JonjonHays>",
	"Isaac0616 <https://github.com/Isaac0616>",
	"SebastiaanZ <https://github.com/SebastiaanZ>",
	"RoelAdriaans <https://github.com/RoelAdriaans>",
	"raviselker <https://github.com/raviselker>",
	"johanvergeer <https://github.com/johanvergeer>",
	"danjer <https://github.com/danjer>",
	"jhbuhrman <https://github.com/jhbuhrman>",
	"WPDOrdina <https://github.com/WPDOrdina>",
	"tirkarthi <https://github.com/tirkarthi>",
	"jayvdb <https://github.com/jayvdb>",
	"jwodder <https://github.com/jwodder>",
	"kloczek <https://github.com/kloczek>",
	"orens <https://github.com/orens>",
	"spMohanty <https://github.com/spMohanty>",
	"ichard26 <https://github.com/ichard26>",
	"jack1142 <https://github.com/jack1142>",
	"mgorny <https://github.com/mgorny>",
	"bzakdd <https://github.com/bzakdd>",
	"haimat <https://github.com/haimat>",
	"Avasam <https://github.com/Avasam>",
	"yschroeder <https://github.com/yschroeder>",
	"axesider <https://github.com/axesider>",
	"tomruk <https://github.com/tomruk>",
	"oprypin <https://github.com/oprypin>",
	"kurtmckee <https://github.com/kurtmckee>",
]
__license__ = "MPL 2.0"
__version__ = "0.12.1"

================================================================================
# File: vendor/pathspec/gitignore.py
# Path: src/beta9/vendor/pathspec/gitignore.py
================================================================================

"""
This module provides :class:`.GitIgnoreSpec` which replicates
*.gitignore* behavior.
"""

from typing import (
	AnyStr,
	Callable,  # Replaced by `collections.abc.Callable` in 3.9.
	Iterable,  # Replaced by `collections.abc.Iterable` in 3.9.
	Optional,  # Replaced by `X | None` in 3.10.
	Tuple,  # Replaced by `tuple` in 3.9.
	Type,  # Replaced by `type` in 3.9.
	TypeVar,
	Union,  # Replaced by `X | Y` in 3.10.
	cast,
	overload)

from .pathspec import (
	PathSpec)
from .pattern import (
	Pattern)
from .patterns.gitwildmatch import (
	GitWildMatchPattern,
	_DIR_MARK)
from .util import (
	_is_iterable)

Self = TypeVar("Self", bound="GitIgnoreSpec")
"""
:class:`GitIgnoreSpec` self type hint to support Python v<3.11 using PEP
673 recommendation.
"""


class GitIgnoreSpec(PathSpec):
	"""
	The :class:`GitIgnoreSpec` class extends :class:`pathspec.pathspec.PathSpec` to
	replicate *.gitignore* behavior.
	"""

	def __eq__(self, other: object) -> bool:
		"""
		Tests the equality of this gitignore-spec with *other* (:class:`GitIgnoreSpec`)
		by comparing their :attr:`~pathspec.pattern.Pattern`
		attributes. A non-:class:`GitIgnoreSpec` will not compare equal.
		"""
		if isinstance(other, GitIgnoreSpec):
			return super().__eq__(other)
		elif isinstance(other, PathSpec):
			return False
		else:
			return NotImplemented

	# Support reversed order of arguments from PathSpec.
	@overload
	@classmethod
	def from_lines(
		cls: Type[Self],
		pattern_factory: Union[str, Callable[[AnyStr], Pattern]],
		lines: Iterable[AnyStr],
	) -> Self:
		...

	@overload
	@classmethod
	def from_lines(
		cls: Type[Self],
		lines: Iterable[AnyStr],
		pattern_factory: Union[str, Callable[[AnyStr], Pattern], None] = None,
	) -> Self:
		...

	@classmethod
	def from_lines(
		cls: Type[Self],
		lines: Iterable[AnyStr],
		pattern_factory: Union[str, Callable[[AnyStr], Pattern], None] = None,
	) -> Self:
		"""
		Compiles the pattern lines.

		*lines* (:class:`~collections.abc.Iterable`) yields each uncompiled
		pattern (:class:`str`). This simply has to yield each line so it can
		be a :class:`io.TextIOBase` (e.g., from :func:`open` or
		:class:`io.StringIO`) or the result from :meth:`str.splitlines`.

		*pattern_factory* can be :data:`None`, the name of a registered
		pattern factory (:class:`str`), or a :class:`~collections.abc.Callable`
		used to compile patterns. The callable must accept an uncompiled
		pattern (:class:`str`) and return the compiled pattern
		(:class:`pathspec.pattern.Pattern`).
		Default is :data:`None` for :class:`.GitWildMatchPattern`).

		Returns the :class:`GitIgnoreSpec` instance.
		"""
		if pattern_factory is None:
			pattern_factory = GitWildMatchPattern

		elif (isinstance(lines, (str, bytes)) or callable(lines)) and _is_iterable(pattern_factory):
			# Support reversed order of arguments from PathSpec.
			pattern_factory, lines = lines, pattern_factory

		self = super().from_lines(pattern_factory, lines)
		return cast(Self, self)

	@staticmethod
	def _match_file(
		patterns: Iterable[Tuple[int, GitWildMatchPattern]],
		file: str,
	) -> Tuple[Optional[bool], Optional[int]]:
		"""
		Check the file against the patterns.

		.. NOTE:: Subclasses of :class:`~pathspec.pathspec.PathSpec` may override
		   this method as an instance method. It does not have to be a static
		   method. The signature for this method is subject to change.

		*patterns* (:class:`~collections.abc.Iterable`) yields each indexed pattern
		(:class:`tuple`) which contains the pattern index (:class:`int`) and actual
		pattern (:class:`~pathspec.pattern.Pattern`).

		*file* (:class:`str`) is the normalized file path to be matched against
		*patterns*.

		Returns a :class:`tuple` containing whether to include *file* (:class:`bool`
		or :data:`None`), and the index of the last matched pattern (:class:`int` or
		:data:`None`).
		"""
		out_include: Optional[bool] = None
		out_index: Optional[int] = None
		out_priority = 0
		for index, pattern in patterns:
			if pattern.include is not None:
				match = pattern.match_file(file)
				if match is not None:
					# Pattern matched.

					# Check for directory marker.
					dir_mark = match.match.groupdict().get(_DIR_MARK)

					if dir_mark:
						# Pattern matched by a directory pattern.
						priority = 1
					else:
						# Pattern matched by a file pattern.
						priority = 2

					if pattern.include and dir_mark:
						out_include = pattern.include
						out_index = index
						out_priority = priority
					elif priority >= out_priority:
						out_include = pattern.include
						out_index = index
						out_priority = priority

		return out_include, out_index

================================================================================
# File: vendor/pathspec/pathspec.py
# Path: src/beta9/vendor/pathspec/pathspec.py
================================================================================

"""
This module provides an object oriented interface for pattern matching of files.
"""

from collections.abc import (
	Collection as CollectionType)
from itertools import (
	zip_longest)
from typing import (
	AnyStr,
	Callable,  # Replaced by `collections.abc.Callable` in 3.9.
	Collection,  # Replaced by `collections.abc.Collection` in 3.9.
	Iterable,  # Replaced by `collections.abc.Iterable` in 3.9.
	Iterator,  # Replaced by `collections.abc.Iterator` in 3.9.
	Optional,  # Replaced by `X | None` in 3.10.
	Type,  # Replaced by `type` in 3.9.
	TypeVar,
	Union)  # Replaced by `X | Y` in 3.10.

from . import util
from .pattern import (
	Pattern)
from .util import (
	CheckResult,
	StrPath,
	TStrPath,
	TreeEntry,
	_filter_check_patterns,
	_is_iterable,
	normalize_file)

Self = TypeVar("Self", bound="PathSpec")
"""
:class:`PathSpec` self type hint to support Python v<3.11 using PEP 673
recommendation.
"""


class PathSpec(object):
	"""
	The :class:`PathSpec` class is a wrapper around a list of compiled
	:class:`.Pattern` instances.
	"""

	def __init__(self, patterns: Iterable[Pattern]) -> None:
		"""
		Initializes the :class:`PathSpec` instance.

		*patterns* (:class:`~collections.abc.Collection` or :class:`~collections.abc.Iterable`)
		yields each compiled pattern (:class:`.Pattern`).
		"""
		if not isinstance(patterns, CollectionType):
			patterns = list(patterns)

		self.patterns: Collection[Pattern] = patterns
		"""
		*patterns* (:class:`~collections.abc.Collection` of :class:`.Pattern`)
		contains the compiled patterns.
		"""

	def __eq__(self, other: object) -> bool:
		"""
		Tests the equality of this path-spec with *other* (:class:`PathSpec`)
		by comparing their :attr:`~PathSpec.patterns` attributes.
		"""
		if isinstance(other, PathSpec):
			paired_patterns = zip_longest(self.patterns, other.patterns)
			return all(a == b for a, b in paired_patterns)
		else:
			return NotImplemented

	def __len__(self) -> int:
		"""
		Returns the number of compiled patterns this path-spec contains
		(:class:`int`).
		"""
		return len(self.patterns)

	def __add__(self: Self, other: "PathSpec") -> Self:
		"""
		Combines the :attr:`Pathspec.patterns` patterns from two
		:class:`PathSpec` instances.
		"""
		if isinstance(other, PathSpec):
			return self.__class__(self.patterns + other.patterns)
		else:
			return NotImplemented

	def __iadd__(self: Self, other: "PathSpec") -> Self:
		"""
		Adds the :attr:`Pathspec.patterns` patterns from one :class:`PathSpec`
		instance to this instance.
		"""
		if isinstance(other, PathSpec):
			self.patterns += other.patterns
			return self
		else:
			return NotImplemented

	def check_file(
		self,
		file: TStrPath,
		separators: Optional[Collection[str]] = None,
	) -> CheckResult[TStrPath]:
		"""
		Check the files against this path-spec.

		*file* (:class:`str` or :class:`os.PathLike`) is the file path to be
		matched against :attr:`self.patterns <PathSpec.patterns>`.

		*separators* (:class:`~collections.abc.Collection` of :class:`str`; or
		:data:`None`) optionally contains the path separators to normalize. See
		:func:`~pathspec.util.normalize_file` for more information.

		Returns the file check result (:class:`~pathspec.util.CheckResult`).
		"""
		norm_file = normalize_file(file, separators)
		include, index = self._match_file(enumerate(self.patterns), norm_file)
		return CheckResult(file, include, index)

	def check_files(
		self,
		files: Iterable[TStrPath],
		separators: Optional[Collection[str]] = None,
	) -> Iterator[CheckResult[TStrPath]]:
		"""
		Check the files against this path-spec.

		*files* (:class:`~collections.abc.Iterable` of :class:`str` or
		:class:`os.PathLike`) contains the file paths to be checked against
		:attr:`self.patterns <PathSpec.patterns>`.

		*separators* (:class:`~collections.abc.Collection` of :class:`str`; or
		:data:`None`) optionally contains the path separators to normalize. See
		:func:`~pathspec.util.normalize_file` for more information.

		Returns an :class:`~collections.abc.Iterator` yielding each file check
		result (:class:`~pathspec.util.CheckResult`).
		"""
		if not _is_iterable(files):
			raise TypeError(f"files:{files!r} is not an iterable.")

		use_patterns = _filter_check_patterns(self.patterns)
		for orig_file in files:
			norm_file = normalize_file(orig_file, separators)
			include, index = self._match_file(use_patterns, norm_file)
			yield CheckResult(orig_file, include, index)

	def check_tree_files(
		self,
		root: StrPath,
		on_error: Optional[Callable[[OSError], None]] = None,
		follow_links: Optional[bool] = None,
	) -> Iterator[CheckResult[str]]:
		"""
		Walks the specified root path for all files and checks them against this
		path-spec.

		*root* (:class:`str` or :class:`os.PathLike`) is the root directory to
		search for files.

		*on_error* (:class:`~collections.abc.Callable` or :data:`None`) optionally
		is the error handler for file-system exceptions. It will be called with the
		exception (:exc:`OSError`). Reraise the exception to abort the walk. Default
		is :data:`None` to ignore file-system exceptions.

		*follow_links* (:class:`bool` or :data:`None`) optionally is whether to walk
		symbolic links that resolve to directories. Default is :data:`None` for
		:data:`True`.

		*negate* (:class:`bool` or :data:`None`) is whether to negate the match
		results of the patterns. If :data:`True`, a pattern matching a file will
		exclude the file rather than include it. Default is :data:`None` for
		:data:`False`.

		Returns an :class:`~collections.abc.Iterator` yielding each file check
		result (:class:`~pathspec.util.CheckResult`).
		"""
		files = util.iter_tree_files(root, on_error=on_error, follow_links=follow_links)
		yield from self.check_files(files)

	@classmethod
	def from_lines(
		cls: Type[Self],
		pattern_factory: Union[str, Callable[[AnyStr], Pattern]],
		lines: Iterable[AnyStr],
	) -> Self:
		"""
		Compiles the pattern lines.

		*pattern_factory* can be either the name of a registered pattern factory
		(:class:`str`), or a :class:`~collections.abc.Callable` used to compile
		patterns. It must accept an uncompiled pattern (:class:`str`) and return the
		compiled pattern (:class:`.Pattern`).

		*lines* (:class:`~collections.abc.Iterable`) yields each uncompiled pattern
		(:class:`str`). This simply has to yield each line so that it can be a
		:class:`io.TextIOBase` (e.g., from :func:`open` or :class:`io.StringIO`) or
		the result from :meth:`str.splitlines`.

		Returns the :class:`PathSpec` instance.
		"""
		if isinstance(pattern_factory, str):
			pattern_factory = util.lookup_pattern(pattern_factory)

		if not callable(pattern_factory):
			raise TypeError(f"pattern_factory:{pattern_factory!r} is not callable.")

		if not _is_iterable(lines):
			raise TypeError(f"lines:{lines!r} is not an iterable.")

		patterns = [pattern_factory(line) for line in lines if line]
		return cls(patterns)

	def match_entries(
		self,
		entries: Iterable[TreeEntry],
		separators: Optional[Collection[str]] = None,
		*,
		negate: Optional[bool] = None,
	) -> Iterator[TreeEntry]:
		"""
		Matches the entries to this path-spec.

		*entries* (:class:`~collections.abc.Iterable` of :class:`~pathspec.util.TreeEntry`)
		contains the entries to be matched against :attr:`self.patterns <PathSpec.patterns>`.

		*separators* (:class:`~collections.abc.Collection` of :class:`str`; or
		:data:`None`) optionally contains the path separators to normalize. See
		:func:`~pathspec.util.normalize_file` for more information.

		*negate* (:class:`bool` or :data:`None`) is whether to negate the match
		results of the patterns. If :data:`True`, a pattern matching a file will
		exclude the file rather than include it. Default is :data:`None` for
		:data:`False`.

		Returns the matched entries (:class:`~collections.abc.Iterator` of
		:class:`~pathspec.util.TreeEntry`).
		"""
		if not _is_iterable(entries):
			raise TypeError(f"entries:{entries!r} is not an iterable.")

		use_patterns = _filter_check_patterns(self.patterns)
		for entry in entries:
			norm_file = normalize_file(entry.path, separators)
			include, _index = self._match_file(use_patterns, norm_file)

			if negate:
				include = not include

			if include:
				yield entry

	_match_file = staticmethod(util.check_match_file)
	"""
	Match files using the `check_match_file()` utility function. Subclasses may
	override this method as an instance method. It does not have to be a static
	method. The signature for this method is subject to change.
	"""

	def match_file(
		self,
		file: StrPath,
		separators: Optional[Collection[str]] = None,
	) -> bool:
		"""
		Matches the file to this path-spec.

		*file* (:class:`str` or :class:`os.PathLike`) is the file path to be
		matched against :attr:`self.patterns <PathSpec.patterns>`.

		*separators* (:class:`~collections.abc.Collection` of :class:`str`)
		optionally contains the path separators to normalize. See
		:func:`~pathspec.util.normalize_file` for more information.

		Returns :data:`True` if *file* matched; otherwise, :data:`False`.
		"""
		norm_file = normalize_file(file, separators)
		include, _index = self._match_file(enumerate(self.patterns), norm_file)
		return bool(include)

	def match_files(
		self,
		files: Iterable[StrPath],
		separators: Optional[Collection[str]] = None,
		*,
		negate: Optional[bool] = None,
	) -> Iterator[StrPath]:
		"""
		Matches the files to this path-spec.

		*files* (:class:`~collections.abc.Iterable` of :class:`str` or
		:class:`os.PathLike`) contains the file paths to be matched against
		:attr:`self.patterns <PathSpec.patterns>`.

		*separators* (:class:`~collections.abc.Collection` of :class:`str`; or
		:data:`None`) optionally contains the path separators to normalize. See
		:func:`~pathspec.util.normalize_file` for more information.

		*negate* (:class:`bool` or :data:`None`) is whether to negate the match
		results of the patterns. If :data:`True`, a pattern matching a file will
		exclude the file rather than include it. Default is :data:`None` for
		:data:`False`.

		Returns the matched files (:class:`~collections.abc.Iterator` of
		:class:`str` or :class:`os.PathLike`).
		"""
		if not _is_iterable(files):
			raise TypeError(f"files:{files!r} is not an iterable.")

		use_patterns = _filter_check_patterns(self.patterns)
		for orig_file in files:
			norm_file = normalize_file(orig_file, separators)
			include, _index = self._match_file(use_patterns, norm_file)

			if negate:
				include = not include

			if include:
				yield orig_file

	def match_tree_entries(
		self,
		root: StrPath,
		on_error: Optional[Callable[[OSError], None]] = None,
		follow_links: Optional[bool] = None,
		*,
		negate: Optional[bool] = None,
	) -> Iterator[TreeEntry]:
		"""
		Walks the specified root path for all files and matches them to this
		path-spec.

		*root* (:class:`str` or :class:`os.PathLike`) is the root directory to
		search.

		*on_error* (:class:`~collections.abc.Callable` or :data:`None`) optionally
		is the error handler for file-system exceptions. It will be called with the
		exception (:exc:`OSError`). Reraise the exception to abort the walk. Default
		is :data:`None` to ignore file-system exceptions.

		*follow_links* (:class:`bool` or :data:`None`) optionally is whether to walk
		symbolic links that resolve to directories. Default is :data:`None` for
		:data:`True`.

		*negate* (:class:`bool` or :data:`None`) is whether to negate the match
		results of the patterns. If :data:`True`, a pattern matching a file will
		exclude the file rather than include it. Default is :data:`None` for
		:data:`False`.

		Returns the matched files (:class:`~collections.abc.Iterator` of
		:class:`.TreeEntry`).
		"""
		entries = util.iter_tree_entries(root, on_error=on_error, follow_links=follow_links)
		yield from self.match_entries(entries, negate=negate)

	def match_tree_files(
		self,
		root: StrPath,
		on_error: Optional[Callable[[OSError], None]] = None,
		follow_links: Optional[bool] = None,
		*,
		negate: Optional[bool] = None,
	) -> Iterator[str]:
		"""
		Walks the specified root path for all files and matches them to this
		path-spec.

		*root* (:class:`str` or :class:`os.PathLike`) is the root directory to
		search for files.

		*on_error* (:class:`~collections.abc.Callable` or :data:`None`) optionally
		is the error handler for file-system exceptions. It will be called with the
		exception (:exc:`OSError`). Reraise the exception to abort the walk. Default
		is :data:`None` to ignore file-system exceptions.

		*follow_links* (:class:`bool` or :data:`None`) optionally is whether to walk
		symbolic links that resolve to directories. Default is :data:`None` for
		:data:`True`.

		*negate* (:class:`bool` or :data:`None`) is whether to negate the match
		results of the patterns. If :data:`True`, a pattern matching a file will
		exclude the file rather than include it. Default is :data:`None` for
		:data:`False`.

		Returns the matched files (:class:`~collections.abc.Iterable` of
		:class:`str`).
		"""
		files = util.iter_tree_files(root, on_error=on_error, follow_links=follow_links)
		yield from self.match_files(files, negate=negate)

	# Alias `match_tree_files()` as `match_tree()` for backward compatibility
	# before v0.3.2.
	match_tree = match_tree_files

================================================================================
# File: vendor/pathspec/pattern.py
# Path: src/beta9/vendor/pathspec/pattern.py
================================================================================

"""
This module provides the base definition for patterns.
"""

import dataclasses
import re
import warnings
from typing import (
	Any,
	AnyStr,
	Iterable,  # Replaced by `collections.abc.Iterable` in 3.9.
	Iterator,  # Replaced by `collections.abc.Iterator` in 3.9.
	Match as MatchHint,  # Replaced by `re.Match` in 3.9.
	Optional,  # Replaced by `X | None` in 3.10.
	Pattern as PatternHint,  # Replaced by `re.Pattern` in 3.9.
	Tuple,  # Replaced by `tuple` in 3.9.
	Union)  # Replaced by `X | Y` in 3.10.


class Pattern(object):
	"""
	The :class:`Pattern` class is the abstract definition of a pattern.
	"""

	# Make the class dict-less.
	__slots__ = (
		'include',
	)

	def __init__(self, include: Optional[bool]) -> None:
		"""
		Initializes the :class:`Pattern` instance.

		*include* (:class:`bool` or :data:`None`) is whether the matched files
		should be included (:data:`True`), excluded (:data:`False`), or is a
		null-operation (:data:`None`).
		"""

		self.include = include
		"""
		*include* (:class:`bool` or :data:`None`) is whether the matched files
		should be included (:data:`True`), excluded (:data:`False`), or is a
		null-operation (:data:`None`).
		"""

	def match(self, files: Iterable[str]) -> Iterator[str]:
		"""
		DEPRECATED: This method is no longer used and has been replaced by
		:meth:`.match_file`. Use the :meth:`.match_file` method with a loop for
		similar results.

		Matches this pattern against the specified files.

		*files* (:class:`~collections.abc.Iterable` of :class:`str`) contains each
		file relative to the root directory (e.g., ``"relative/path/to/file"``).

		Returns an :class:`~collections.abc.Iterable` yielding each matched file
		path (:class:`str`).
		"""
		warnings.warn((
			"{cls.__module__}.{cls.__qualname__}.match() is deprecated. Use "
			"{cls.__module__}.{cls.__qualname__}.match_file() with a loop for "
			"similar results."
		).format(cls=self.__class__), DeprecationWarning, stacklevel=2)

		for file in files:
			if self.match_file(file) is not None:
				yield file

	def match_file(self, file: str) -> Optional[Any]:
		"""
		Matches this pattern against the specified file.

		*file* (:class:`str`) is the normalized file path to match against.

		Returns the match result if *file* matched; otherwise, :data:`None`.
		"""
		raise NotImplementedError((
			"{cls.__module__}.{cls.__qualname__} must override match_file()."
		).format(cls=self.__class__))


class RegexPattern(Pattern):
	"""
	The :class:`RegexPattern` class is an implementation of a pattern using
	regular expressions.
	"""

	# Keep the class dict-less.
	__slots__ = (
		'pattern',
		'regex',
	)

	def __init__(
		self,
		pattern: Union[AnyStr, PatternHint, None],
		include: Optional[bool] = None,
	) -> None:
		"""
		Initializes the :class:`RegexPattern` instance.

		*pattern* (:class:`str`, :class:`bytes`, :class:`re.Pattern`, or
		:data:`None`) is the pattern to compile into a regular expression.

		*include* (:class:`bool` or :data:`None`) must be :data:`None` unless
		*pattern* is a precompiled regular expression (:class:`re.Pattern`) in which
		case it is whether matched files should be included (:data:`True`), excluded
		(:data:`False`), or is a null operation (:data:`None`).

			.. NOTE:: Subclasses do not need to support the *include* parameter.
		"""

		if isinstance(pattern, (str, bytes)):
			assert include is None, (
				f"include:{include!r} must be null when pattern:{pattern!r} is a string."
			)
			regex, include = self.pattern_to_regex(pattern)
			# NOTE: Make sure to allow a null regular expression to be
			# returned for a null-operation.
			if include is not None:
				regex = re.compile(regex)

		elif pattern is not None and hasattr(pattern, 'match'):
			# Assume pattern is a precompiled regular expression.
			# - NOTE: Used specified *include*.
			regex = pattern

		elif pattern is None:
			# NOTE: Make sure to allow a null pattern to be passed for a
			# null-operation.
			assert include is None, (
				f"include:{include!r} must be null when pattern:{pattern!r} is null."
			)

		else:
			raise TypeError(f"pattern:{pattern!r} is not a string, re.Pattern, or None.")

		super(RegexPattern, self).__init__(include)

		self.pattern: Union[AnyStr, PatternHint, None] = pattern
		"""
		*pattern* (:class:`str`, :class:`bytes`, :class:`re.Pattern`, or
		:data:`None`) is the uncompiled, input pattern. This is for reference.
		"""

		self.regex: PatternHint = regex
		"""
		*regex* (:class:`re.Pattern`) is the regular expression for the pattern.
		"""

	def __eq__(self, other: 'RegexPattern') -> bool:
		"""
		Tests the equality of this regex pattern with *other* (:class:`RegexPattern`)
		by comparing their :attr:`~Pattern.include` and :attr:`~RegexPattern.regex`
		attributes.
		"""
		if isinstance(other, RegexPattern):
			return self.include == other.include and self.regex == other.regex
		else:
			return NotImplemented

	def match_file(self, file: str) -> Optional['RegexMatchResult']:
		"""
		Matches this pattern against the specified file.

		*file* (:class:`str`) contains each file relative to the root directory
		(e.g., "relative/path/to/file").

		Returns the match result (:class:`.RegexMatchResult`) if *file* matched;
		otherwise, :data:`None`.
		"""
		if self.include is not None:
			match = self.regex.match(file)
			if match is not None:
				return RegexMatchResult(match)

		return None

	@classmethod
	def pattern_to_regex(cls, pattern: str) -> Tuple[str, bool]:
		"""
		Convert the pattern into an uncompiled regular expression.

		*pattern* (:class:`str`) is the pattern to convert into a regular
		expression.

		Returns the uncompiled regular expression (:class:`str` or :data:`None`),
		and whether matched files should be included (:data:`True`), excluded
		(:data:`False`), or is a null-operation (:data:`None`).

			.. NOTE:: The default implementation simply returns *pattern* and
			   :data:`True`.
		"""
		return pattern, True


@dataclasses.dataclass()
class RegexMatchResult(object):
	"""
	The :class:`RegexMatchResult` data class is used to return information about
	the matched regular expression.
	"""

	# Keep the class dict-less.
	__slots__ = (
		'match',
	)

	match: MatchHint
	"""
	*match* (:class:`re.Match`) is the regex match result.
	"""

================================================================================
# File: vendor/pathspec/patterns/__init__.py
# Path: src/beta9/vendor/pathspec/patterns/__init__.py
================================================================================

"""
The *pathspec.patterns* package contains the pattern matching
implementations.
"""

# Load pattern implementations.
from . import gitwildmatch

# DEPRECATED: Expose the `GitWildMatchPattern` class in this module for
# backward compatibility with v0.5.
from .gitwildmatch import GitWildMatchPattern

================================================================================
# File: vendor/pathspec/patterns/gitwildmatch.py
# Path: src/beta9/vendor/pathspec/patterns/gitwildmatch.py
================================================================================

"""
This module implements Git's wildmatch pattern matching which itself is derived
from Rsync's wildmatch. Git uses wildmatch for its ".gitignore" files.
"""

import re
import warnings
from typing import (
	AnyStr,
	Optional,  # Replaced by `X | None` in 3.10.
	Tuple)  # Replaced by `tuple` in 3.9.

from .. import util
from ..pattern import RegexPattern

_BYTES_ENCODING = 'latin1'
"""
The encoding to use when parsing a byte string pattern.
"""

_DIR_MARK = 'ps_d'
"""
The regex group name for the directory marker. This is only used by
:class:`GitIgnoreSpec`.
"""


class GitWildMatchPatternError(ValueError):
	"""
	The :class:`GitWildMatchPatternError` indicates an invalid git wild match
	pattern.
	"""
	pass


class GitWildMatchPattern(RegexPattern):
	"""
	The :class:`GitWildMatchPattern` class represents a compiled Git wildmatch
	pattern.
	"""

	# Keep the dict-less class hierarchy.
	__slots__ = ()

	@classmethod
	def pattern_to_regex(
		cls,
		pattern: AnyStr,
	) -> Tuple[Optional[AnyStr], Optional[bool]]:
		"""
		Convert the pattern into a regular expression.

		*pattern* (:class:`str` or :class:`bytes`) is the pattern to convert into a
		regular expression.

		Returns the uncompiled regular expression (:class:`str`, :class:`bytes`, or
		:data:`None`); and whether matched files should be included (:data:`True`),
		excluded (:data:`False`), or if it is a null-operation (:data:`None`).
		"""
		if isinstance(pattern, str):
			return_type = str
		elif isinstance(pattern, bytes):
			return_type = bytes
			pattern = pattern.decode(_BYTES_ENCODING)
		else:
			raise TypeError(f"pattern:{pattern!r} is not a unicode or byte string.")

		original_pattern = pattern

		if pattern.endswith('\\ '):
			# EDGE CASE: Spaces can be escaped with backslash. If a pattern that ends
			# with backslash followed by a space, only strip from left.
			pattern = pattern.lstrip()
		else:
			pattern = pattern.strip()

		if pattern.startswith('#'):
			# A pattern starting with a hash ('#') serves as a comment (neither
			# includes nor excludes files). Escape the hash with a back-slash to match
			# a literal hash (i.e., '\#').
			regex = None
			include = None

		elif pattern == '/':
			# EDGE CASE: According to `git check-ignore` (v2.4.1), a single '/' does
			# not match any file.
			regex = None
			include = None

		elif pattern:
			if pattern.startswith('!'):
				# A pattern starting with an exclamation mark ('!') negates the pattern
				# (exclude instead of include). Escape the exclamation mark with a
				# back-slash to match a literal exclamation mark (i.e., '\!').
				include = False
				# Remove leading exclamation mark.
				pattern = pattern[1:]
			else:
				include = True

			# Allow a regex override for edge cases that cannot be handled through
			# normalization.
			override_regex = None

			# Split pattern into segments.
			pattern_segs = pattern.split('/')

			# Check whether the pattern is specifically a directory pattern before
			# normalization.
			is_dir_pattern = not pattern_segs[-1]

			# Normalize pattern to make processing easier.

			# EDGE CASE: Deal with duplicate double-asterisk sequences. Collapse each
			# sequence down to one double-asterisk. Iterate over the segments in
			# reverse and remove the duplicate double asterisks as we go.
			for i in range(len(pattern_segs) - 1, 0, -1):
				prev = pattern_segs[i-1]
				seg = pattern_segs[i]
				if prev == '**' and seg == '**':
					del pattern_segs[i]

			if len(pattern_segs) == 2 and pattern_segs[0] == '**' and not pattern_segs[1]:
				# EDGE CASE: The '**/' pattern should match everything except individual
				# files in the root directory. This case cannot be adequately handled
				# through normalization. Use the override.
				override_regex = f'^.+(?P<{_DIR_MARK}>/).*$'

			if not pattern_segs[0]:
				# A pattern beginning with a slash ('/') will only match paths directly
				# on the root directory instead of any descendant paths. So, remove
				# empty first segment to make pattern relative to root.
				del pattern_segs[0]

			elif len(pattern_segs) == 1 or (len(pattern_segs) == 2 and not pattern_segs[1]):
				# A single pattern without a beginning slash ('/') will match any
				# descendant path. This is equivalent to "**/{pattern}". So, prepend
				# with double-asterisks to make pattern relative to root.
				# - EDGE CASE: This also holds for a single pattern with a trailing
				#   slash (e.g. dir/).
				if pattern_segs[0] != '**':
					pattern_segs.insert(0, '**')

			else:
				# EDGE CASE: A pattern without a beginning slash ('/') but contains at
				# least one prepended directory (e.g. "dir/{pattern}") should not match
				# "**/dir/{pattern}", according to `git check-ignore` (v2.4.1).
				pass

			if not pattern_segs:
				# After resolving the edge cases, we end up with no pattern at all. This
				# must be because the pattern is invalid.
				raise GitWildMatchPatternError(f"Invalid git pattern: {original_pattern!r}")

			if not pattern_segs[-1] and len(pattern_segs) > 1:
				# A pattern ending with a slash ('/') will match all descendant paths if
				# it is a directory but not if it is a regular file. This is equivalent
				# to "{pattern}/**". So, set last segment to a double-asterisk to
				# include all descendants.
				pattern_segs[-1] = '**'

			if override_regex is None:
				# Build regular expression from pattern.
				output = ['^']
				need_slash = False
				end = len(pattern_segs) - 1
				for i, seg in enumerate(pattern_segs):
					if seg == '**':
						if i == 0 and i == end:
							# A pattern consisting solely of double-asterisks ('**') will
							# match every path.
							output.append(f'[^/]+(?:/.*)?')

						elif i == 0:
							# A normalized pattern beginning with double-asterisks
							# ('**') will match any leading path segments.
							output.append('(?:.+/)?')
							need_slash = False

						elif i == end:
							# A normalized pattern ending with double-asterisks ('**') will
							# match any trailing path segments.
							if is_dir_pattern:
								output.append(f'(?P<{_DIR_MARK}>/).*')
							else:
								output.append(f'/.*')

						else:
							# A pattern with inner double-asterisks ('**') will match multiple
							# (or zero) inner path segments.
							output.append('(?:/.+)?')
							need_slash = True

					elif seg == '*':
						# Match single path segment.
						if need_slash:
							output.append('/')

						output.append('[^/]+')

						if i == end:
							# A pattern ending without a slash ('/') will match a file or a
							# directory (with paths underneath it). E.g., "foo" matches "foo",
							# "foo/bar", "foo/bar/baz", etc.
							output.append(f'(?:(?P<{_DIR_MARK}>/).*)?')

						need_slash = True

					else:
						# Match segment glob pattern.
						if need_slash:
							output.append('/')

						try:
							output.append(cls._translate_segment_glob(seg))
						except ValueError as e:
							raise GitWildMatchPatternError(f"Invalid git pattern: {original_pattern!r}") from e

						if i == end:
							# A pattern ending without a slash ('/') will match a file or a
							# directory (with paths underneath it). E.g., "foo" matches "foo",
							# "foo/bar", "foo/bar/baz", etc.
							output.append(f'(?:(?P<{_DIR_MARK}>/).*)?')

						need_slash = True

				output.append('$')
				regex = ''.join(output)

			else:
				# Use regex override.
				regex = override_regex

		else:
			# A blank pattern is a null-operation (neither includes nor excludes
			# files).
			regex = None
			include = None

		if regex is not None and return_type is bytes:
			regex = regex.encode(_BYTES_ENCODING)

		return regex, include

	@staticmethod
	def _translate_segment_glob(pattern: str) -> str:
		"""
		Translates the glob pattern to a regular expression. This is used in the
		constructor to translate a path segment glob pattern to its corresponding
		regular expression.

		*pattern* (:class:`str`) is the glob pattern.

		Returns the regular expression (:class:`str`).
		"""
		# NOTE: This is derived from `fnmatch.translate()` and is similar to the
		# POSIX function `fnmatch()` with the `FNM_PATHNAME` flag set.

		escape = False
		regex = ''
		i, end = 0, len(pattern)
		while i < end:
			# Get next character.
			char = pattern[i]
			i += 1

			if escape:
				# Escape the character.
				escape = False
				regex += re.escape(char)

			elif char == '\\':
				# Escape character, escape next character.
				escape = True

			elif char == '*':
				# Multi-character wildcard. Match any string (except slashes), including
				# an empty string.
				regex += '[^/]*'

			elif char == '?':
				# Single-character wildcard. Match any single character (except a
				# slash).
				regex += '[^/]'

			elif char == '[':
				# Bracket expression wildcard. Except for the beginning exclamation
				# mark, the whole bracket expression can be used directly as regex, but
				# we have to find where the expression ends.
				# - "[][!]" matches ']', '[' and '!'.
				# - "[]-]" matches ']' and '-'.
				# - "[!]a-]" matches any character except ']', 'a' and '-'.
				j = i

				# Pass bracket expression negation.
				if j < end and (pattern[j] == '!' or pattern[j] == '^'):
					j += 1

				# Pass first closing bracket if it is at the beginning of the
				# expression.
				if j < end and pattern[j] == ']':
					j += 1

				# Find closing bracket. Stop once we reach the end or find it.
				while j < end and pattern[j] != ']':
					j += 1

				if j < end:
					# Found end of bracket expression. Increment j to be one past the
					# closing bracket:
					#
					#  [...]
					#   ^   ^
					#   i   j
					#
					j += 1
					expr = '['

					if pattern[i] == '!':
						# Bracket expression needs to be negated.
						expr += '^'
						i += 1
					elif pattern[i] == '^':
						# POSIX declares that the regex bracket expression negation "[^...]"
						# is undefined in a glob pattern. Python's `fnmatch.translate()`
						# escapes the caret ('^') as a literal. Git supports the using a
						# caret for negation. Maintain consistency with Git because that is
						# the expected behavior.
						expr += '^'
						i += 1

					# Build regex bracket expression. Escape slashes so they are treated
					# as literal slashes by regex as defined by POSIX.
					expr += pattern[i:j].replace('\\', '\\\\')

					# Add regex bracket expression to regex result.
					regex += expr

					# Set i to one past the closing bracket.
					i = j

				else:
					# Failed to find closing bracket, treat opening bracket as a bracket
					# literal instead of as an expression.
					regex += '\\['

			else:
				# Regular character, escape it for regex.
				regex += re.escape(char)

		if escape:
			raise ValueError(f"Escape character found with no next character to escape: {pattern!r}")

		return regex

	@staticmethod
	def escape(s: AnyStr) -> AnyStr:
		"""
		Escape special characters in the given string.

		*s* (:class:`str` or :class:`bytes`) a filename or a string that you want to
		escape, usually before adding it to a ".gitignore".

		Returns the escaped string (:class:`str` or :class:`bytes`).
		"""
		if isinstance(s, str):
			return_type = str
			string = s
		elif isinstance(s, bytes):
			return_type = bytes
			string = s.decode(_BYTES_ENCODING)
		else:
			raise TypeError(f"s:{s!r} is not a unicode or byte string.")

		# Reference: https://git-scm.com/docs/gitignore#_pattern_format
		meta_characters = r"[]!*#?"

		out_string = "".join("\\" + x if x in meta_characters else x for x in string)

		if return_type is bytes:
			return out_string.encode(_BYTES_ENCODING)
		else:
			return out_string

util.register_pattern('gitwildmatch', GitWildMatchPattern)


class GitIgnorePattern(GitWildMatchPattern):
	"""
	The :class:`GitIgnorePattern` class is deprecated by :class:`GitWildMatchPattern`.
	This class only exists to maintain compatibility with v0.4.
	"""

	def __init__(self, *args, **kw) -> None:
		"""
		Warn about deprecation.
		"""
		self._deprecated()
		super(GitIgnorePattern, self).__init__(*args, **kw)

	@staticmethod
	def _deprecated() -> None:
		"""
		Warn about deprecation.
		"""
		warnings.warn((
			"GitIgnorePattern ('gitignore') is deprecated. Use GitWildMatchPattern "
			"('gitwildmatch') instead."
		), DeprecationWarning, stacklevel=3)

	@classmethod
	def pattern_to_regex(cls, *args, **kw):
		"""
		Warn about deprecation.
		"""
		cls._deprecated()
		return super(GitIgnorePattern, cls).pattern_to_regex(*args, **kw)

# Register `GitIgnorePattern` as "gitignore" for backward compatibility with
# v0.4.
util.register_pattern('gitignore', GitIgnorePattern)

================================================================================
# File: vendor/pathspec/util.py
# Path: src/beta9/vendor/pathspec/util.py
================================================================================

"""
This module provides utility methods for dealing with path-specs.
"""

import os
import os.path
import pathlib
import posixpath
import stat
import sys
import warnings
from collections.abc import (
	Collection as CollectionType,
	Iterable as IterableType)
from dataclasses import (
	dataclass)
from os import (
	PathLike)
from typing import (
	Any,
	AnyStr,
	Callable,  # Replaced by `collections.abc.Callable` in 3.9.
	Collection,  # Replaced by `collections.abc.Collection` in 3.9.
	Dict,  # Replaced by `dict` in 3.9.
	Generic,
	Iterable,  # Replaced by `collections.abc.Iterable` in 3.9.
	Iterator,  # Replaced by `collections.abc.Iterator` in 3.9.
	List,  # Replaced by `list` in 3.9.
	Optional,  # Replaced by `X | None` in 3.10.
	Sequence,  # Replaced by `collections.abc.Sequence` in 3.9.
	Set,  # Replaced by `set` in 3.9.
	Tuple,  # Replaced by `tuple` in 3.9.
	TypeVar,
	Union)  # Replaced by `X | Y` in 3.10.

from .pattern import (
	Pattern)

if sys.version_info >= (3, 9):
	StrPath = Union[str, PathLike[str]]
else:
	StrPath = Union[str, PathLike]

TStrPath = TypeVar("TStrPath", bound=StrPath)
"""
Type variable for :class:`str` or :class:`os.PathLike`.
"""

NORMALIZE_PATH_SEPS = [
	__sep
	for __sep in [os.sep, os.altsep]
	if __sep and __sep != posixpath.sep
]
"""
*NORMALIZE_PATH_SEPS* (:class:`list` of :class:`str`) contains the path
separators that need to be normalized to the POSIX separator for the
current operating system. The separators are determined by examining
:data:`os.sep` and :data:`os.altsep`.
"""

_registered_patterns = {}
"""
*_registered_patterns* (:class:`dict`) maps a name (:class:`str`) to the
registered pattern factory (:class:`~collections.abc.Callable`).
"""


def append_dir_sep(path: pathlib.Path) -> str:
	"""
	Appends the path separator to the path if the path is a directory.
	This can be used to aid in distinguishing between directories and
	files on the file-system by relying on the presence of a trailing path
	separator.

	*path* (:class:`pathlib.Path`) is the path to use.

	Returns the path (:class:`str`).
	"""
	str_path = str(path)
	if path.is_dir():
		str_path += os.sep

	return str_path


def check_match_file(
	patterns: Iterable[Tuple[int, Pattern]],
	file: str,
) -> Tuple[Optional[bool], Optional[int]]:
	"""
	Check the file against the patterns.

	*patterns* (:class:`~collections.abc.Iterable`) yields each indexed pattern
	(:class:`tuple`) which contains the pattern index (:class:`int`) and actual
	pattern (:class:`~pathspec.pattern.Pattern`).

	*file* (:class:`str`) is the normalized file path to be matched
	against *patterns*.

	Returns a :class:`tuple` containing whether to include *file* (:class:`bool`
	or :data:`None`), and the index of the last matched pattern (:class:`int` or
	:data:`None`).
	"""
	out_include: Optional[bool] = None
	out_index: Optional[int] = None
	for index, pattern in patterns:
		if pattern.include is not None and pattern.match_file(file) is not None:
			out_include = pattern.include
			out_index = index

	return out_include, out_index


def detailed_match_files(
	patterns: Iterable[Pattern],
	files: Iterable[str],
	all_matches: Optional[bool] = None,
) -> Dict[str, 'MatchDetail']:
	"""
	Matches the files to the patterns, and returns which patterns matched
	the files.

	*patterns* (:class:`~collections.abc.Iterable` of :class:`~pathspec.pattern.Pattern`)
	contains the patterns to use.

	*files* (:class:`~collections.abc.Iterable` of :class:`str`) contains
	the normalized file paths to be matched against *patterns*.

	*all_matches* (:class:`bool` or :data:`None`) is whether to return all
	matches patterns (:data:`True`), or only the last matched pattern
	(:data:`False`). Default is :data:`None` for :data:`False`.

	Returns the matched files (:class:`dict`) which maps each matched file
	(:class:`str`) to the patterns that matched in order (:class:`.MatchDetail`).
	"""
	all_files = files if isinstance(files, CollectionType) else list(files)
	return_files = {}
	for pattern in patterns:
		if pattern.include is not None:
			result_files = pattern.match(all_files)  # TODO: Replace with `.match_file()`.
			if pattern.include:
				# Add files and record pattern.
				for result_file in result_files:
					if result_file in return_files:
						if all_matches:
							return_files[result_file].patterns.append(pattern)
						else:
							return_files[result_file].patterns[0] = pattern
					else:
						return_files[result_file] = MatchDetail([pattern])

			else:
				# Remove files.
				for file in result_files:
					del return_files[file]

	return return_files


def _filter_check_patterns(
	patterns: Iterable[Pattern],
) -> List[Tuple[int, Pattern]]:
	"""
	Filters out null-patterns.

	*patterns* (:class:`Iterable` of :class:`.Pattern`) contains the
	patterns.

	Returns a :class:`list` containing each indexed pattern (:class:`tuple`) which
	contains the pattern index (:class:`int`) and the actual pattern
	(:class:`~pathspec.pattern.Pattern`).
	"""
	return [
		(__index, __pat)
		for __index, __pat in enumerate(patterns)
		if __pat.include is not None
	]


def _is_iterable(value: Any) -> bool:
	"""
	Check whether the value is an iterable (excludes strings).

	*value* is the value to check,

	Returns whether *value* is a iterable (:class:`bool`).
	"""
	return isinstance(value, IterableType) and not isinstance(value, (str, bytes))


def iter_tree_entries(
	root: StrPath,
	on_error: Optional[Callable[[OSError], None]] = None,
	follow_links: Optional[bool] = None,
) -> Iterator['TreeEntry']:
	"""
	Walks the specified directory for all files and directories.

	*root* (:class:`str` or :class:`os.PathLike`) is the root directory to
	search.

	*on_error* (:class:`~collections.abc.Callable` or :data:`None`)
	optionally is the error handler for file-system exceptions. It will be
	called with the exception (:exc:`OSError`). Reraise the exception to
	abort the walk. Default is :data:`None` to ignore file-system
	exceptions.

	*follow_links* (:class:`bool` or :data:`None`) optionally is whether
	to walk symbolic links that resolve to directories. Default is
	:data:`None` for :data:`True`.

	Raises :exc:`RecursionError` if recursion is detected.

	Returns an :class:`~collections.abc.Iterator` yielding each file or
	directory entry (:class:`.TreeEntry`) relative to *root*.
	"""
	if on_error is not None and not callable(on_error):
		raise TypeError(f"on_error:{on_error!r} is not callable.")

	if follow_links is None:
		follow_links = True

	yield from _iter_tree_entries_next(os.path.abspath(root), '', {}, on_error, follow_links)


def _iter_tree_entries_next(
	root_full: str,
	dir_rel: str,
	memo: Dict[str, str],
	on_error: Callable[[OSError], None],
	follow_links: bool,
) -> Iterator['TreeEntry']:
	"""
	Scan the directory for all descendant files.

	*root_full* (:class:`str`) the absolute path to the root directory.

	*dir_rel* (:class:`str`) the path to the directory to scan relative to
	*root_full*.

	*memo* (:class:`dict`) keeps track of ancestor directories
	encountered. Maps each ancestor real path (:class:`str`) to relative
	path (:class:`str`).

	*on_error* (:class:`~collections.abc.Callable` or :data:`None`)
	optionally is the error handler for file-system exceptions.

	*follow_links* (:class:`bool`) is whether to walk symbolic links that
	resolve to directories.

	Yields each entry (:class:`.TreeEntry`).
	"""
	dir_full = os.path.join(root_full, dir_rel)
	dir_real = os.path.realpath(dir_full)

	# Remember each encountered ancestor directory and its canonical
	# (real) path. If a canonical path is encountered more than once,
	# recursion has occurred.
	if dir_real not in memo:
		memo[dir_real] = dir_rel
	else:
		raise RecursionError(real_path=dir_real, first_path=memo[dir_real], second_path=dir_rel)

	with os.scandir(dir_full) as scan_iter:
		node_ent: os.DirEntry
		for node_ent in scan_iter:
			node_rel = os.path.join(dir_rel, node_ent.name)

			# Inspect child node.
			try:
				node_lstat = node_ent.stat(follow_symlinks=False)
			except OSError as e:
				if on_error is not None:
					on_error(e)
				continue

			if node_ent.is_symlink():
				# Child node is a link, inspect the target node.
				try:
					node_stat = node_ent.stat()
				except OSError as e:
					if on_error is not None:
						on_error(e)
					continue
			else:
				node_stat = node_lstat

			if node_ent.is_dir(follow_symlinks=follow_links):
				# Child node is a directory, recurse into it and yield its
				# descendant files.
				yield TreeEntry(node_ent.name, node_rel, node_lstat, node_stat)

				yield from _iter_tree_entries_next(root_full, node_rel, memo, on_error, follow_links)

			elif node_ent.is_file() or node_ent.is_symlink():
				# Child node is either a file or an unfollowed link, yield it.
				yield TreeEntry(node_ent.name, node_rel, node_lstat, node_stat)

	# NOTE: Make sure to remove the canonical (real) path of the directory
	# from the ancestors memo once we are done with it. This allows the
	# same directory to appear multiple times. If this is not done, the
	# second occurrence of the directory will be incorrectly interpreted
	# as a recursion. See <https://github.com/cpburnz/python-path-specification/pull/7>.
	del memo[dir_real]


def iter_tree_files(
	root: StrPath,
	on_error: Optional[Callable[[OSError], None]] = None,
	follow_links: Optional[bool] = None,
) -> Iterator[str]:
	"""
	Walks the specified directory for all files.

	*root* (:class:`str` or :class:`os.PathLike`) is the root directory to
	search for files.

	*on_error* (:class:`~collections.abc.Callable` or :data:`None`)
	optionally is the error handler for file-system exceptions. It will be
	called with the exception (:exc:`OSError`). Reraise the exception to
	abort the walk. Default is :data:`None` to ignore file-system
	exceptions.

	*follow_links* (:class:`bool` or :data:`None`) optionally is whether
	to walk symbolic links that resolve to directories. Default is
	:data:`None` for :data:`True`.

	Raises :exc:`RecursionError` if recursion is detected.

	Returns an :class:`~collections.abc.Iterator` yielding the path to
	each file (:class:`str`) relative to *root*.
	"""
	for entry in iter_tree_entries(root, on_error=on_error, follow_links=follow_links):
		if not entry.is_dir(follow_links):
			yield entry.path


def iter_tree(root, on_error=None, follow_links=None):
	"""
	DEPRECATED: The :func:`.iter_tree` function is an alias for the
	:func:`.iter_tree_files` function.
	"""
	warnings.warn((
		"util.iter_tree() is deprecated. Use util.iter_tree_files() instead."
	), DeprecationWarning, stacklevel=2)
	return iter_tree_files(root, on_error=on_error, follow_links=follow_links)


def lookup_pattern(name: str) -> Callable[[AnyStr], Pattern]:
	"""
	Lookups a registered pattern factory by name.

	*name* (:class:`str`) is the name of the pattern factory.

	Returns the registered pattern factory (:class:`~collections.abc.Callable`).
	If no pattern factory is registered, raises :exc:`KeyError`.
	"""
	return _registered_patterns[name]


def match_file(patterns: Iterable[Pattern], file: str) -> bool:
	"""
	Matches the file to the patterns.

	*patterns* (:class:`~collections.abc.Iterable` of :class:`~pathspec.pattern.Pattern`)
	contains the patterns to use.

	*file* (:class:`str`) is the normalized file path to be matched
	against *patterns*.

	Returns :data:`True` if *file* matched; otherwise, :data:`False`.
	"""
	matched = False
	for pattern in patterns:
		if pattern.include is not None and pattern.match_file(file) is not None:
			matched = pattern.include

	return matched


def match_files(
	patterns: Iterable[Pattern],
	files: Iterable[str],
) -> Set[str]:
	"""
	DEPRECATED: This is an old function no longer used. Use the
	:func:`~pathspec.util.match_file` function with a loop for better results.

	Matches the files to the patterns.

	*patterns* (:class:`~collections.abc.Iterable` of :class:`~pathspec.pattern.Pattern`)
	contains the patterns to use.

	*files* (:class:`~collections.abc.Iterable` of :class:`str`) contains
	the normalized file paths to be matched against *patterns*.

	Returns the matched files (:class:`set` of :class:`str`).
	"""
	warnings.warn((
		f"{__name__}.match_files() is deprecated. Use {__name__}.match_file() with "
		f"a loop for better results."
	), DeprecationWarning, stacklevel=2)

	use_patterns = [__pat for __pat in patterns if __pat.include is not None]

	return_files = set()
	for file in files:
		if match_file(use_patterns, file):
			return_files.add(file)

	return return_files


def normalize_file(
	file: StrPath,
	separators: Optional[Collection[str]] = None,
) -> str:
	"""
	Normalizes the file path to use the POSIX path separator (i.e.,
	``"/"``), and make the paths relative (remove leading ``"/"``).

	*file* (:class:`str` or :class:`os.PathLike`) is the file path.

	*separators* (:class:`~collections.abc.Collection` of :class:`str`; or
	``None``) optionally contains the path separators to normalize.
	This does not need to include the POSIX path separator (``"/"``),
	but including it will not affect the results. Default is ``None``
	for ``NORMALIZE_PATH_SEPS``. To prevent normalization, pass an
	empty container (e.g., an empty tuple ``()``).

	Returns the normalized file path (:class:`str`).
	"""
	# Normalize path separators.
	if separators is None:
		separators = NORMALIZE_PATH_SEPS

	# Convert path object to string.
	norm_file: str = os.fspath(file)

	for sep in separators:
		norm_file = norm_file.replace(sep, posixpath.sep)

	if norm_file.startswith('/'):
		# Make path relative.
		norm_file = norm_file[1:]

	elif norm_file.startswith('./'):
		# Remove current directory prefix.
		norm_file = norm_file[2:]

	return norm_file


def normalize_files(
	files: Iterable[StrPath],
	separators: Optional[Collection[str]] = None,
) -> Dict[str, List[StrPath]]:
	"""
	DEPRECATED: This function is no longer used. Use the :func:`.normalize_file`
	function with a loop for better results.

	Normalizes the file paths to use the POSIX path separator.

	*files* (:class:`~collections.abc.Iterable` of :class:`str` or
	:class:`os.PathLike`) contains the file paths to be normalized.

	*separators* (:class:`~collections.abc.Collection` of :class:`str`; or
	:data:`None`) optionally contains the path separators to normalize.
	See :func:`normalize_file` for more information.

	Returns a :class:`dict` mapping each normalized file path (:class:`str`)
	to the original file paths (:class:`list` of :class:`str` or
	:class:`os.PathLike`).
	"""
	warnings.warn((
		"util.normalize_files() is deprecated. Use util.normalize_file() "
		"with a loop for better results."
	), DeprecationWarning, stacklevel=2)

	norm_files = {}
	for path in files:
		norm_file = normalize_file(path, separators=separators)
		if norm_file in norm_files:
			norm_files[norm_file].append(path)
		else:
			norm_files[norm_file] = [path]

	return norm_files


def register_pattern(
	name: str,
	pattern_factory: Callable[[AnyStr], Pattern],
	override: Optional[bool] = None,
) -> None:
	"""
	Registers the specified pattern factory.

	*name* (:class:`str`) is the name to register the pattern factory
	under.

	*pattern_factory* (:class:`~collections.abc.Callable`) is used to
	compile patterns. It must accept an uncompiled pattern (:class:`str`)
	and return the compiled pattern (:class:`.Pattern`).

	*override* (:class:`bool` or :data:`None`) optionally is whether to
	allow overriding an already registered pattern under the same name
	(:data:`True`), instead of raising an :exc:`AlreadyRegisteredError`
	(:data:`False`). Default is :data:`None` for :data:`False`.
	"""
	if not isinstance(name, str):
		raise TypeError(f"name:{name!r} is not a string.")

	if not callable(pattern_factory):
		raise TypeError(f"pattern_factory:{pattern_factory!r} is not callable.")

	if name in _registered_patterns and not override:
		raise AlreadyRegisteredError(name, _registered_patterns[name])

	_registered_patterns[name] = pattern_factory


class AlreadyRegisteredError(Exception):
	"""
	The :exc:`AlreadyRegisteredError` exception is raised when a pattern
	factory is registered under a name already in use.
	"""

	def __init__(
		self,
		name: str,
		pattern_factory: Callable[[AnyStr], Pattern],
	) -> None:
		"""
		Initializes the :exc:`AlreadyRegisteredError` instance.

		*name* (:class:`str`) is the name of the registered pattern.

		*pattern_factory* (:class:`~collections.abc.Callable`) is the
		registered pattern factory.
		"""
		super(AlreadyRegisteredError, self).__init__(name, pattern_factory)

	@property
	def message(self) -> str:
		"""
		*message* (:class:`str`) is the error message.
		"""
		return "{name!r} is already registered for pattern factory:{pattern_factory!r}.".format(
			name=self.name,
			pattern_factory=self.pattern_factory,
		)

	@property
	def name(self) -> str:
		"""
		*name* (:class:`str`) is the name of the registered pattern.
		"""
		return self.args[0]

	@property
	def pattern_factory(self) -> Callable[[AnyStr], Pattern]:
		"""
		*pattern_factory* (:class:`~collections.abc.Callable`) is the
		registered pattern factory.
		"""
		return self.args[1]


class RecursionError(Exception):
	"""
	The :exc:`RecursionError` exception is raised when recursion is
	detected.
	"""

	def __init__(
		self,
		real_path: str,
		first_path: str,
		second_path: str,
	) -> None:
		"""
		Initializes the :exc:`RecursionError` instance.

		*real_path* (:class:`str`) is the real path that recursion was
		encountered on.

		*first_path* (:class:`str`) is the first path encountered for
		*real_path*.

		*second_path* (:class:`str`) is the second path encountered for
		*real_path*.
		"""
		super(RecursionError, self).__init__(real_path, first_path, second_path)

	@property
	def first_path(self) -> str:
		"""
		*first_path* (:class:`str`) is the first path encountered for
		:attr:`self.real_path <RecursionError.real_path>`.
		"""
		return self.args[1]

	@property
	def message(self) -> str:
		"""
		*message* (:class:`str`) is the error message.
		"""
		return "Real path {real!r} was encountered at {first!r} and then {second!r}.".format(
			real=self.real_path,
			first=self.first_path,
			second=self.second_path,
		)

	@property
	def real_path(self) -> str:
		"""
		*real_path* (:class:`str`) is the real path that recursion was
		encountered on.
		"""
		return self.args[0]

	@property
	def second_path(self) -> str:
		"""
		*second_path* (:class:`str`) is the second path encountered for
		:attr:`self.real_path <RecursionError.real_path>`.
		"""
		return self.args[2]


@dataclass(frozen=True)
class CheckResult(Generic[TStrPath]):
	"""
	The :class:`CheckResult` class contains information about the file and which
	pattern matched it.
	"""

	# Make the class dict-less.
	__slots__ = (
		'file',
		'include',
		'index',
	)

	file: TStrPath
	"""
	*file* (:class:`str` or :class:`os.PathLike`) is the file path.
	"""

	include: Optional[bool]
	"""
	*include* (:class:`bool` or :data:`None`) is whether to include or exclude the
	file. If :data:`None`, no pattern matched.
	"""

	index: Optional[int]
	"""
	*index* (:class:`int` or :data:`None`) is the index of the last pattern that
	matched. If :data:`None`, no pattern matched.
	"""


class MatchDetail(object):
	"""
	The :class:`.MatchDetail` class contains information about
	"""

	# Make the class dict-less.
	__slots__ = ('patterns',)

	def __init__(self, patterns: Sequence[Pattern]) -> None:
		"""
		Initialize the :class:`.MatchDetail` instance.

		*patterns* (:class:`~collections.abc.Sequence` of :class:`~pathspec.pattern.Pattern`)
		contains the patterns that matched the file in the order they were
		encountered.
		"""

		self.patterns = patterns
		"""
		*patterns* (:class:`~collections.abc.Sequence` of :class:`~pathspec.pattern.Pattern`)
		contains the patterns that matched the file in the order they were
		encountered.
		"""


class TreeEntry(object):
	"""
	The :class:`.TreeEntry` class contains information about a file-system
	entry.
	"""

	# Make the class dict-less.
	__slots__ = ('_lstat', 'name', 'path', '_stat')

	def __init__(
		self,
		name: str,
		path: str,
		lstat: os.stat_result,
		stat: os.stat_result,
	) -> None:
		"""
		Initialize the :class:`.TreeEntry` instance.

		*name* (:class:`str`) is the base name of the entry.

		*path* (:class:`str`) is the relative path of the entry.

		*lstat* (:class:`os.stat_result`) is the stat result of the direct
		entry.

		*stat* (:class:`os.stat_result`) is the stat result of the entry,
		potentially linked.
		"""

		self._lstat: os.stat_result = lstat
		"""
		*_lstat* (:class:`os.stat_result`) is the stat result of the direct
		entry.
		"""

		self.name: str = name
		"""
		*name* (:class:`str`) is the base name of the entry.
		"""

		self.path: str = path
		"""
		*path* (:class:`str`) is the path of the entry.
		"""

		self._stat: os.stat_result = stat
		"""
		*_stat* (:class:`os.stat_result`) is the stat result of the linked
		entry.
		"""

	def is_dir(self, follow_links: Optional[bool] = None) -> bool:
		"""
		Get whether the entry is a directory.

		*follow_links* (:class:`bool` or :data:`None`) is whether to follow
		symbolic links. If this is :data:`True`, a symlink to a directory
		will result in :data:`True`. Default is :data:`None` for :data:`True`.

		Returns whether the entry is a directory (:class:`bool`).
		"""
		if follow_links is None:
			follow_links = True

		node_stat = self._stat if follow_links else self._lstat
		return stat.S_ISDIR(node_stat.st_mode)

	def is_file(self, follow_links: Optional[bool] = None) -> bool:
		"""
		Get whether the entry is a regular file.

		*follow_links* (:class:`bool` or :data:`None`) is whether to follow
		symbolic links. If this is :data:`True`, a symlink to a regular file
		will result in :data:`True`. Default is :data:`None` for :data:`True`.

		Returns whether the entry is a regular file (:class:`bool`).
		"""
		if follow_links is None:
			follow_links = True

		node_stat = self._stat if follow_links else self._lstat
		return stat.S_ISREG(node_stat.st_mode)

	def is_symlink(self) -> bool:
		"""
		Returns whether the entry is a symbolic link (:class:`bool`).
		"""
		return stat.S_ISLNK(self._lstat.st_mode)

	def stat(self, follow_links: Optional[bool] = None) -> os.stat_result:
		"""
		Get the cached stat result for the entry.

		*follow_links* (:class:`bool` or :data:`None`) is whether to follow
		symbolic links. If this is :data:`True`, the stat result of the
		linked file will be returned. Default is :data:`None` for :data:`True`.

		Returns that stat result (:class:`os.stat_result`).
		"""
		if follow_links is None:
			follow_links = True

		return self._stat if follow_links else self._lstat
